---
title: CS-443 Machine Learning
description: "My notes from the CS-443 Machine Learning course given at EPFL, in the 2018 autumn semester (MA1)"
edited: true
note: true
---

* TOC
{:toc}

⚠ *Work in progress*

<!-- More --> 

We'll always use subscript $n$ for data point, and $d$ for feature. $N$ is the data size and $D$ is the dimensionality.

Recommended website: <http://www.matrixcalculus.org/>

## Linear regression
A linear regression is a model that assumes a linear relationship between inputs and the output. We will study three types of methods:

1. Grid search
2. Iterative optimization algorithms
3. Least squares

### Simple linear regression

For a single input dimension ($D=1$), we can use a simple linear regression, which is given by:

$$
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\expectsub}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\cost}[1]{\mathcal{L}\left(#1\right)}
\newcommand{\Strain}{S_{\text{train}}}
\newcommand{\Stest}{S_{\text{test}}}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\argmin}{\arg\!\min}

y_n \approx f(x_n) := w_0 + w_1 x_{n1}
$$

$w = (w_0, w_1)$ are the parameters of the model.

### Multiple linear regression

If our data has multiple input dimensions, we obtain multivariate linear regression:

$$
y_n \approx 
    f(\pmb{x}_n) :=w_0 + w_1x_{n1}+\dots+w_D x_{wD} 
    = w_0 + \pmb{x}_n^t \begin{bmatrix}
        w_1 \\
        \vdots \\
        w_D \\
    \end{bmatrix}
    = \tilde{\pmb{x}}_n ^T \tilde{\pmb{w}}
$$

> 👉🏼 If we wanted to be a little more strict, we should write $f_{\pmb{w}}(\pmb{x}_n)$, as the model of course also depends on the weights.

The tilde notation means that we have included the offset term $w_0$, also known as the **bias**:

$$
\tilde{\pmb{x}}_n=\begin{bmatrix}1 \\ x_{n1} \\ \vdots \\ x_{nD} \end{bmatrix} \in \mathbb{R}^{D+1}, 
\quad
\tilde{\pmb{w}} = \begin{bmatrix}w_0 \\ w_1 \\ \vdots \\ w_D\end{bmatrix} \in \mathbb{R^{D+1}}
$$

### The $D > N$ problem

If the number of parameters exceeds the number of data examples, we say that the task is *under-determined*. This can be solved by regularization, which we’ll get to more precisely later

## Cost functions

$\pmb{x}_n$ is the data, which we can easily understand where comes from. But how does one find a good $\pmb{w}$ from the data? 

A **cost function** (also called loss function) is used to learn parameters that explain the data well. It quantifies how well our model does by giving errors a score, quantifying penalties for errors. Our goal is to find weights that minimize the loss functions.

### Properties

Desirable properties of cost functions are:

- **Symmetry around 0**: that is, being off by a positive or negative amount is equivalent; what matters is the amplitude of the error, not the sign.
- **Robustness**: penalizes large errors at about the same rate as very large errors. This is a way to make sure that outliers don’t completely dominate our regression.

### Good cost functions

#### MSE

Probably the most commonly used cost function is Mean Square Error (MSE): 

$$
\mathcal{L}_{\text{MSE}}(\pmb{w}) := \frac{1}{N} \sum_{n=1}^N \left(y_n - f(\pmb{x}_n)\right)^2
\label{def:mse}
$$

MSE is symmetrical around 0, but also tends to penalize outliers quite harshly (because it squares error): MSE is not robust. In practice, this is problematic, because outliers occur more often than we’d like to.

Note that we often use MSE with a factor $\frac{1}{2N}$ instead of $\frac{1}{N}$. This is because it makes for a cleaner derivative, but we'll get into that later. Just know that for all intents and purposes, it doesn't change really change anything about the behavior of the models we'll study.

#### MAE

When outliers are present, Mean Absolute Error (MAE) tends to fare better:

$$
\text{MAE}(\pmb{w}) := \frac{1}{N} \sum_{n=1}^N \left| y_n - f(\pmb{x}_n)\right|
$$

Instead of squaring, we take the absolute value. This is more robust. Note that MAE isn’t differentiable at 0, but we’ll talk about that later.

There are other cost functions that are even more robust; these are available as additional reading, but is not exam material.

### Convexity

A function is **convex** iff a line joining two points never intersects with the function anywhere else. More strictly defined, a function $f(\pmb{u})$ with $\pmb{u}\in\chi$ is *convex* if, for any $\pmb{u}, \pmb{v} \in\chi$, and for any $0 \le\lambda\le 1$, we have:

$$
f(\lambda\pmb{u}+(1-\lambda)\pmb{v})\le\lambda f(\pmb{u}) +(1-\lambda)f(\pmb{v})
$$

A function is **strictly convex** if the above inequality is strict ($<$).

A stritly convex function has a unique global minimum $\pmb{w}^*$. For convex functions, every local minimum is a global minimum. This makes it a desirable property for loss functions, since it means that cost function optimization is guaranteed to find the global minimum.

Sums of convex functions are also convex. Therefore, MSE and MAE are convex.

## Optimization

### Learning / Estimation / Fitting

Given a cost function (or loss function) $\mathcal{L}(\pmb{w})$, we wish to find $\pmb{w}^*$ which minimizes the cost:

$$
\min_{\pmb W}{\mathcal{L}(\pmb w)}, \quad\text{ subject to } \pmb w \in \mathbb R^D
$$

This is what we call **learning**: learning is simply an optimization problem, and as such, we’ll use an optimization algorithm to solve it – that is, find a good $\pmb w$.

### Grid search

This is one of the simplest optimization algorithms, although far from being the most efficient one. It can be described as “try all the values”, a kind of brute-force algorithm; you can think of it as nested for-loops over the individual $w_i$ weights.

For instance, if our weights are $\pmb{w} = \begin{bmatrix}w_1 \\ w_2\end{bmatrix}$, then we can try, say 4 values for $w_1$, 4 values for $w_2$, for a total of 16 values of $\mathcal{L}(\pmb{w})$.

But obviously, complexity is exponential $\mathcal{O}(a^D)$ (where $a$ is the number of values to try), which is really bad, especially when we can have $D\approx$ millions of parameters. Additionally, grid search has no guarantees that it’ll find an optimum; it’ll just find the best value we tried.

If grid search sounds bad for optimization, that’s because it is. In practice, it is not used for optimization of parameters, but it *is* used to tune hyperparameters.

### Optimization landscapes

#### Local minimum

A vector $\pmb{w}^\*$ is a *local minimum* of a function $\mathcal{L}$ (we’re interested in the minimum of cost functions $\mathcal{L}$, which we denote with $\pmb{w}^*$, as opposed to any other value $\pmb{w}$, but this obviously holds for any function) if $\exists \epsilon > 0$ such that

$$
\mathcal{L}(\pmb{w}^*) \le \mathcal{L(\pmb{w})}, \quad \forall\pmb w : \norm{\pmb{w} -\pmb{w}^*} < \epsilon
$$

In other words, the local minimum $\pmb{w}^*$ is better than all the neighbors in some non-zero radius.

#### Global minimum

The global minimum is defined by getting rid of the radius $\epsilon$ and comparing to all other values:

$$
\mathcal{L}(\pmb{w}^*) \le \mathcal{L(\pmb{w})}, \qquad \forall\pmb{w}\in\mathbb{R}^D
$$

#### Strict minimum

A minimum is said to be **strict** if the corresponding equality is strict for $\pmb{w} \ne \pmb{w}^*$, that is, there is only one minimum value.

### Smooth (differentiable) optimization

#### Gradient

A gradient at a given point is the slope of the tangent to the function at that point. It points to the direction of largest increase of the function. By following the gradient (in the opposite direction, because we’re searching for a minimum and not a maximum), we can find the minimum.

![Graphs of MSE and MAE](/images/ml/mse-mae.png)

Gradient is defined by:

$$
\nabla \mathcal{L}(\pmb{w}) := \begin{bmatrix}
	\frac{\partial\mathcal{L}(\pmb{w})}{\partial w_1} \\
	\vdots \\
	\frac{\partial\mathcal{L}(\pmb{w})}{\partial w_D} \\
\end{bmatrix}
$$

This is a vector, i.e. $\nabla\mathcal{L}(\pmb{w})\in\mathbb R^D$. Each dimension $i$ of the vector indicates how fast the cost $\mathcal{L}$ changes depending on the weight $w_i$.

#### Gradient descent

Gradient descent is an iterative algorithm. We start from a candidate $w^{(t)}$, and iterate.

$$
\pmb{w}^{(t+1)}:=\pmb{w}^{(t)} - \gamma \nabla\mathcal{L}\left(\pmb{w}^{(t)}\right)
$$

As stated previously, we’re adding the negative gradient to find the minimum, hence the subtraction.

$\gamma$ is known as the **step-size**, which is a small value (maybe 0.1). You don’t want to be too aggressive with it, or you might risk overshooting in your descent. In practice, the step-size that makes the learning as fast as possible is often found by trial and error 🤷🏼‍♂️.

As an example, we will take an analytical look at a gradient descent, in order to understand its behavior and components. We will do gradient descent on a 1-parameter model, in which we minimize the MSE, which is defined as follows:

$$
\mathcal{L}\left(w_0\right)=\frac{1}{2N}\sum_{n=1}^N{\left(y_n - w_0\right)^2}
$$

Note that we’re dividing by 2 on top of the regular MSE; it has no impact on finding the minimum, but when we will compute the gradient below, it will conveniently cancel out the $\frac{1}{2}$.

The gradient of $\mathcal{L}\left(w_0\right)$ is:

$$
\begin{align}
\nabla\mathcal{L}\left(\pmb{w}\right)
	& = \frac{\partial}{\partial w_0}\mathcal{L} \\
	& = \frac{1}{2N}\sum_{n=1}^N{-2(y_n - w_0)}  \\
	& = w_0 - \bar{y}
\end{align}
$$

And thus, our gradient descent is given by:

$$
\begin{align*}
w_0^{(t+1)}
	&:= w_0^{(t)} - \gamma\nabla\mathcal{L}\left(\pmb w\right) \\
	& = w_0^{(t)} - \gamma(w_0^{(t)} - \bar{y}) \\
	& = (1-\gamma)w_0^{(t)} + \gamma\bar{y}, 
	\qquad\text{where } \bar{y}:=\sum_{n}{\frac{y_n}{N}}
\end{align*}
$$

This sequence is guaranteed to converge for $\pmb{w}^* = \bar{y}$ (so the solution to this exact problem can be extracted analytically from gradient descent). This would set the cost function to 0, which is the minimum.

The choice of $\gamma$ has an influence on the algorithm’s outcome:

- If we pick $\gamma=1$, we would get to the optimum in one step
- If we pick $\gamma < 1$, we would get a little closer in every step, eventually converging to $\bar{y}$
- If we pick $\gamma > 1$, we are going to overshoot $\bar{y}$. Slightly bigger than 1 (say, 1.5) would still converge; $\gamma=2$ would loop infinitely between two points; $\gamma > 2$ diverges.

#### Gradient descent for linear MSE

Our linear regression is given by a line $\pmb{y}$ that is a regression for some data $\pmb X$:

$$
\pmb{y}=\begin{bmatrix}
	y_1 \\ y_2 \\ \vdots \\ y_N
\end{bmatrix}, 
\quad
\pmb{X}=\begin{bmatrix}
x_{11} & x_{12} & \dots & x_{1D} \\
x_{21} & x_{22} & \dots & x_{2D} \\
\vdots & \vdots & \ddots & \vdots \\
x_{N1} & x_{N2} & \dots & x_{ND} \\
\end{bmatrix}
$$

Our model is:

$$
f_w(x)=\pmb{x}_n^T \pmb{w}
$$

We define the error vector by:

$$
\pmb{e}=\pmb{y} - \pmb{Xw}, 
\quad \text{ or } \quad 
\pmb{e}_n = \pmb{x}_n^T\pmb{w}
$$

The MSE can then be restated as follows:

$$
\mathcal{L}\left(\pmb{w}\right)
	:= \frac{1}{2N}\sum_{n=1}^N{\left( y_n - \pmb{x}_n^T \pmb{w}\right)^2}
	=  \frac{1}{2N}\pmb{e}^T\pmb{e}
$$

And the gradient is, component-wise:

$$
\frac{\partial}{\partial\pmb{w}_d}\mathcal{L}
	= -\frac{1}{2N}\sum_{n=1}^N {2(y_n - \pmb{x}_n^T \pmb{w}) \pmb{x}_{nd}}
	= -\frac{1}{N} (\pmb{X}_{:d})^T \pmb{e}
$$

We’re using column notation $\pmb{X}_{:d}$ to signify column $d$ of the matrix $X$.

And thus, all in all, our gradient is:

$$
\nabla\mathcal{L}\left(\pmb{w}\right) = -\frac{1}{N}\pmb{X}^T\pmb{e}
$$

To compute this expression, we must compute:

- The error $\pmb e$, which takes $2N\cdot D - 1$ floating point operations (flops) for the matrix-vector multiplication, and $N$ for the subtraction, for a total of $2N\cdot D + N - 1$, which is $\mathcal{O}(N\cdot D)$
- The gradient $\nabla\mathcal{L}$, which costs $2N\cdot D + D - 1$, which is $\mathcal{O}(N\cdot D)$.

In total, this process is $\mathcal{O}(N\cdot D)$ at every step. This is not too bad, it’s equivalent to reading the data once.

#### Stochastic gradient descent (SGD)

In ML, most cost functions are formulated as a sum of:

$$
\mathcal{L}\left(\pmb{w}\right) = \frac{1}{N}\sum_{n=1}^N{\mathcal{L}_n(\pmb{w})}
$$

In practice, this can be expensive to compute, so the solution is to pick a random $n$ uniformly at random in $n\in\left[1, N\right]$ to be able to make the sum go away.

The stochastic gradient descent is thus:

$$
\pmb{w}^{(t+1)}:=\pmb{w}^{(t)} - \gamma \nabla\mathcal{L}_n\left({\pmb{w}^{(t)}}\right)
$$

Why is it allowed to pick just one $n$ instead of the full thing? We won’t give a full proof, but the intuition is that:

$$
\expect{\nabla\mathcal{L}_n(\pmb{w})}
	= \frac{1}{N} \sum_{n=1}^N{\nabla\mathcal{L}_n(\pmb{w})}
	= \nabla\left(\frac{1}{N} \sum_{n=1}^N{\mathcal{L}_n(\pmb{w})}\right)
	\equiv \nabla\mathcal{L}\left(\pmb{w}\right)
$$

The gradient of a single n is:

$$
\mathcal{L}_n(\pmb{w}) = \frac{1}{2} \left(y_n -\pmb{x}_n^T w\right)^2 \\
\nabla\mathcal{L}_n(\pmb{w}) = (-x_n^T) (y_n-\pmb{x}_n^T \pmb{w})
$$

Note that $x_n^T \in\mathbb{R}^D$, and $(y_n-\pmb{x}_n^T \pmb{w})\in\mathbb{R}$. Computational complexity for this is $\mathcal{O}(D)$. 

#### Mini-batch SGD

But perhaps just picking a **single** value is too extreme; there is an intermediate version in which we choose a subset $B\subseteq \left[N\right]$ instead of $\abs{B}$ points, instead of a single point.

$$
g := \frac{1}{|B|}\sum_{n\in B}{\nabla\mathcal{L}_n(\pmb{w}^{(t)})} \\
w^{(t+1)} := w^{(t)} - \gamma\pmb{g}
$$

Note that if $\abs{B} = N$ then we’re performing a full gradient descent.

The computation of $\pmb{g}$ can be parallelized easily over $\abs{B}$ GPU threads, which is quite common in practice; $\abs{B}$ is thus often dictated by the number of available threads.

Computational complexity is $\mathcal{O}(\abs{B}\cdot D)$.

### Non-smooth (non-differentiable) optimization

We’ve defined convexity previously, but we can also use the following alternative characterization of convexity:

$$
\mathcal{L}\left(\pmb u\right) \ge \mathcal{L}\left(\pmb w\right) + \nabla \mathcal{L}\left(\pmb w\right)^T(\pmb{u} - \pmb{w}) \quad \forall \pmb{u}, \pmb{w}
\iff \mathcal{L} \text{ convex}
$$

Meaning that the function must always lie above its linearization (which is the first-order Taylor expansion) to be convex.

![A convex function lies above its linearization](/images/ml/convex-above-linearization.png)

#### Subgradients

A vector $\pmb{g}\in\mathbb{R}^D$ such that:

$$
\mathcal{L}\left(\pmb u\right) \ge \mathcal{L}\left(\pmb w\right) + \pmb{g}^T(\pmb u - \pmb w) \quad \forall \pmb{u}, \pmb{w}
$$

is called a **subgradient** to the function $\mathcal{L}$ at $\pmb w$. The subgradient forms a line that is always below the curve, somewhat like the gradient of a convex function.

![The subgradient lies below the function](/images/ml/subgradient-below-function.png)

This definition is valid even for an arbitrary $\mathcal{L}$ that may not be differentiable, and not even necessarily convex.

If the function $\mathcal{L}$ is differentiable at $\pmb w$, then the *only subgradient* at $\pmb{w}$ is $\pmb{g} = \nabla\mathcal{L}\left(\pmb{w}\right)$.

#### Subgradient descent

This is exactly like gradient descent, except for the fact that we use the *subgradient* $\pmb{g}$ at the current iterate $\pmb{w}^{(t)}$ instead of the *gradient*:

$$
w^{(t+1)} := w^{(t)} - \gamma\pmb{g}
$$

For instance, MAE is not differentiable at 0, so we must use the subgradient.

$$
\text{Let }h: \mathbb{R} \rightarrow \mathbb{R}, \quad h(e) := |e| \\
\text{At } e, \text{the subgradient }
g \in \partial h = \begin{cases}
-1 & \text{if } e < 0 \\
[-1, 1] & \text{if } e = 0 \\
1 & \text{if } e > 0 \\
\end{cases}
$$

Here, $\partial h$ is somewhat confusing notation for the set of all possible subgradients at our position.

For linear regressions, the (sub)gradient is easy to compute using the *chain rule*.

Let $h$ be non-differentiable, $q$ differentiable, and $\mathcal{L}\left(\pmb{w}\right) = h(q(w))$. The chain rule tells us that, at $\pmb w$, our subgradient is:

$$
g \in \partial h(q(\pmb{w})) \cdot \nabla q(\pmb{w})
$$

#### Stochastic subgradient descent

This is still commonly abbreviated SGD.

It’s exactly the same, except that $\pmb g$ is a subgradient to the randomly selected $\mathcal{L}_n$ at the current iterate $\pmb{w}^{(t)}$.



### Comparison

|                             | Smooth                                                       | Non-smooth                                                   |
| --------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Full gradient descent       | Gradient of $$\mathcal{L}$$ <br />Complexity is $\mathcal{O}(N\cdot D)$ | Subgradient of $\mathcal{L}$<br />Complexity is $\mathcal{O}(N\cdot D)$ |
| Stochastic gradient descent | Gradient of $\mathcal{L}_n$                                | Subgradient of $\mathcal{L}_n$                             |



### Constrained optimization

Sometimes, optimization problems come posed with an additional constraint.

#### Convex sets

We’ve seen convexity for functions, but we can also define it for sets. A set $\mathcal{C}$ is convex iff the line segment between any two points of $\mathcal{C}$ lies in $\mathcal{C}$. That is, $\forall \pmb{u}, \pmb{v} \in \mathcal{C}, \quad \forall 0 \le \theta \le 1$, we have:

$$
\theta \pmb{u} + (1 - \theta)\pmb{v} \in \mathcal{C}
$$

This means that the line between any two points in the set $\mathcal{C}$ must also be fully contained within the set.

![Examples of convex and non-convex sets](/images/ml/convex-sets.png)

A couple of properties of convex sets:

- Intersection of convex sets is also convex.
- Projections onto convex sets are **unique** (and often efficient to compute).


#### Projected gradient descent

When dealing with constrained problems, we have two options. The first one is to add a projection onto $\mathcal{C}$ in every step:

$$
P_\mathcal{C}(\pmb{w}') := \arg{\min_{\pmb{v}\in\mathcal{C}}}\norm{\pmb{v-w'}}
$$

The rule for gradient descent can thus be updated to become:

$$
w^{(t+1)} := P_\mathcal{C}\left(w^{(t)} - \gamma \nabla \mathcal{L}(w^{(t)}) \right)
$$

This means that at every step, we compute the new $w^{(t+1)}$ normally, but apply a projection on top of that. In other words, if the regular gradient descent sets our weights outside of the constrained space, we project them back.

<figure>
    <img alt="Steps of projected SGD" src="/images/ml/projected-sgd.png" />
    <figcaption>Here, $w'$ is the result of regular SGD, i.e. $w' = w^{(t)} - \gamma \nabla \mathcal{L}(w^{(t)})$</figcaption>
</figure>

This is the same for stochastic gradient descent, and we have the same convergence properties.

Note that the computational cost of the projection is very important here, since it is performed at every step.

#### Turning constrained problems into unconstrained problems

If projection as described above is approach A, this is approach B.

We use a **penalty function**, such as the “brick wall” indicator function below:

$$
I_\mathcal{C}(\pmb w) = \begin{cases}
0 & \pmb{w} \in \mathcal{C} \\
+\infty & \pmb{w} \notin \mathcal{C}
\end{cases}
$$

We could also perhaps use something with a less drastic error value than $+\infty$, if we don’t care about the constraint quite as extreme.

Note that this is similar to regularization, which we’ll talk about later. 

Now, instead of directly solving $min_{\pmb{w}\in\mathcal{C}}{\mathcal{L}(\pmb{w})}$, we solve for:

$$
\min_{\pmb{w}\in \mathbb{R}^D} {
    \mathcal{L}(\pmb{w}) + I_\mathcal{C}(\pmb{w})
}
$$

### Implementation issues in gradient methods

#### Stopping criteria

When $\norm{\mathcal{L}(\pmb{w})}$ is zero (or close to zero), we are often close to the optimum.

#### Optimality

If the second order derivative is positive (or positive semi-definite for the general case $D\ge1$), then it is a (possibly local) minimum. If the function is also convex, then this is necessarily a global minimum. That is:

$$
\nabla \mathcal{L(\pmb{w})} = 0, \quad \mathcal{L} \text{ convex}
\implies
\text{optimum at }\pmb{w}
$$

#### Step size

If $\gamma$ is too big, we might diverge ([as seen previously](#gradient-descent)). But if it is too small, we might be very slow! Convergence is only guaranteed for $\gamma < \gamma_{min}$, which is a value that depends on the problem. 

## Least squares

### Normal equations

In some rare cases, we can take an analytical approach to computing the optimum of the cost function, rather than a computational one; for instance, for linear regression with MSE, as we've done previously. These types of equations are sometimes called **normal equations**. This is one of the most popular methods for data fitting, called **least squares**.

How do we get these normal equations?

First, we show that the problem is convex. If that is the case, then according to the [optimality conditions](#optimality) for convex functions, the point at which the derivative is zero is the optimum:

$$
\mathcal{L}(\pmb{w}^*)=\pmb{0}
$$

This gives us a system of $D$ equations known as the normal equations.

### Single parameter linear regression
Let's try this for a single parameter linear regression, with MSE as the cost function. 

First, we will just accept that the cost function is convex in the $w_0$ parameter. 

As [proven previously](#gradient-descent), we know that for the single parameter model, the derivative is:

$$
\begin{align}
\nabla\mathcal{L}\left(\pmb{w}\right)
    & = \frac{\partial}{\partial w_0}\mathcal{L} \\
    & = \frac{1}{2N}\sum_{n=1}^N{-2(y_n - w_0)}  \\
    & = w_0 - \bar{y}
\end{align}
$$

This means that the derivative is 0 for $w_0 = \bar{y}$. This allows us to define our optimum parameter $\pmb{w}^\*$ as $\pmb{w}^* = \begin{bmatrix}\bar{y}\end{bmatrix}$.

### Multiple parameter linear regression

As we know by now, the cost function for linear regression with MSE is:

$$
\mathcal{L}\left(\pmb{w}\right)
	:= \frac{1}{2N}\sum_{n=1}^N{\left( y_n - \pmb{x}_n^T \pmb{w}\right)^2}
	=  \frac{1}{2N}(\pmb{y-Xw})^T(\pmb{y-Xw})
$$


Where the matrices are defined as:

$$
\pmb{y}=\begin{bmatrix}
	y_1 \\ y_2 \\ \vdots \\ y_N
\end{bmatrix}, 
\quad
\pmb{X}=\begin{bmatrix}
x_{11} & x_{12} & \dots & x_{1D} \\
x_{21} & x_{22} & \dots & x_{2D} \\
\vdots & \vdots & \ddots & \vdots \\
x_{N1} & x_{N2} & \dots & x_{ND} \\
\end{bmatrix}
$$


We denote the $i^\text{th}$ row of $X$ by $x_i^T$. Each $x_i^T$ represents a different data point.

We claim that this cost function is *convex* in $\pmb{w}$. We can prove that in any of the following ways:

***

#### Simplest way
The cost function is the sum of many convex functions, and is thus also convex.

#### Directly verify the definition

$$
\forall \lambda\in [0,1], 
\quad \forall \pmb{w}, \pmb{w}',
\qquad
\mathcal{L}\left(\lambda\pmb{w} + \left(1-\lambda\right)\pmb{w}'\right) 
- \left(\lambda\mathcal{L}(\pmb{w}) + \left( 1-\lambda \right) \mathcal{L}(\pmb{w}')\right) \le 0
$$
  
The left-hand side of the inequality reduces to:
  
$$
-\frac{1}{2N}\lambda(1-\lambda)\norm{\pmb{X}(\pmb{w}-\pmb{w}')}_2^2
$$
   
which is indeed non-positive.

#### Compute the Hessian

The Hessian is the matrix of second derivatives, defined as follows:

$$
H_{ij} = \left( \frac{\partial\mathcal{L}}{\partial w_i \partial w_j} \right)_{ij}
$$

If the Hessian is positive semidefinite (i.e. all its eigenvalues are non-negative), then the function is convex.
  
For our case, the Hessian is given by:
  
$$
\frac{1}{N}\pmb{X}^T\pmb{X}
$$
  
This is indeed positive semi-definite, as its eigenvalues are the squares of the eigenvalues of $\pmb{X}$, and must therefore be positive.

***

Knowing that the function is convex, we can find the minimum. If we take the gradient of this expression, we get:

$$
\nabla\mathcal{L}(\pmb{w}) = -\frac{1}{N}\pmb{X}^T(\pmb{y-Xw})
$$

We can set this to 0 to get the normal equations for linear regression, which are:

$$
\pmb{X}^T(\pmb{y-Xw}) =: \pmb{X}^T\pmb{e} = \pmb{0}
$$

This proves that the normal equations for linear regression are given by $\pmb{X}^T\pmb{e} = \pmb{0}$.

### Geometric interpretation

The above definition of normal equations are given by $\pmb{X}^T\pmb{e} = \pmb{0}$. How can visualize that?

The error is given by:

$$
\pmb{e} := \pmb{y} - \pmb{Xw}
$$

By definition, this error vector is orthogonal to all columns of $\pmb{X}$. Indeed, it tells us how far above or below the span our prediction $\pmb{y}$ is. 

The **span** of $\pmb{X}$ is the space spanned by the columns of $\pmb{X}$. Every element of the span can be written as $\pmb{u} = \pmb{Xw}$ for some choice of $\pmb{w}$. 

For the normal equations, we must pick an optimal $\pmb{w}^\*$ for which the gradient is 0. Picking an $\pmb{w}^\*$ is equivalent to picking an optimal $\pmb{u}^* = \pmb{Xw}^\*$ from the span of $\pmb{X}$.

But which element of $\text{span}(\pmb{X})$ shall we take, which one is the optimal one? The normal equations tell us that the optimum choice for $\pmb{u}$, called $$\pmb{u}^*$$ is the element such that $$\pmb{y} - \pmb{u}^*$$ is orthogonal to $\text{span}(X)$.

In other words, we should pick $\pmb{u}^*$ to be the projection of $\pmb{y}$ onto $\text{span}(\pmb{X})$.

![Geometric interpretation of the normal equations](/images/ml/geometric-interpretation-normal-equations.png)


### Least squares
All we've done so far is to solve the same old problem of a matrix equation:

$$
Ax = b
$$

But we've always done so with a bit of a twist; there may not be an exact value of $x$ satisfying exact equality, but we could find one that gets us as close as possible:

$$
Ax \approx b
$$

This is also what least squares does. It attempts to minimize the MSE to get as $Ax$ close as possible to $b$.

In this course, we often denote the data matrix $A$ as $\pmb{X}$, the weights $x$ as $\pmb{w}$, and $b$ as $y$; in other words, we're trying to solve:

$$
\pmb{X}\pmb{w} \approx \pmb{y}
$$

In least squares, we multiply this whole equation by $\pmb{X}^T$ on the left. We attempt to find $\pmb{w}^*$, the minimal weight that gets us as minimally wrong as possible. In other we're trying to solve:

$$
\left( \pmb{X}^T\pmb{X} \right) \pmb{w} \approx \pmb{X}^T\pmb{y}
$$

One way to solve this problem would simply be to invert the $A$ matrix, which in our case is $\pmb{X}^T\pmb{X}$:

$$
\pmb{w}^* = (\pmb{X}^T\pmb{X})^{-1} \pmb{X}^T y
$$

As such, we can use this model to predict values for unseen data points:

$$
\hat{y}_m := \pmb{x}_m^T \pmb{w}^* = \pmb{x}_m^T (\pmb{X}^T\pmb{X})^{-1} \pmb{X}^T y
$$

### Invertibility and uniqueness
Note that the Gram matrix, defined as $\pmb{X}^T\pmb{X} \in \mathbb{R}^{D\times D}$, is invertible **if and only if** $\pmb{X}$ has **full column rank**, or in other words, $\text{rank}(\pmb{X}) = D$.

$$
\pmb{X}^T\pmb{X} \in \mathbb{R}^{D\times D} \text{ invertible}
\iff
\text{rank}(\pmb{X}) = D
$$

Unfortunately, in practice, our data matrix $\pmb{X}\in\mathbb{R}^{N\times D}$ is often **rank-deficient**.

- If $D>N$, we always have $\text{rank}(\pmb{X}) < D$ (since column and row rank are the same).
- If $D \le N$, but some of the columns $\pmb{x}_{:d}$ are collinear (or in practice, nearly collinear), then the matrix is **ill-conditioned**. This leads to numerical issues when solving the linear system.
  
  To know how bad things are, we can compute the condition number, which is the maximum eigenvalue of the Gram matrix, divided by the minimum See course contents of Numerical Methods.

If our data matrix is rank-deficient or ill-conditioned (which is practically always the case), we certainly shouldn't be inverting it directly! We'll introduce high numerical errors that falsify our output.

That doesn't mean we can't do least squares in practice. We can still use a linear solver. In Python, that means you should use [`np.linalg.solve`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html), which uses a LU decomposition internally and thus avoids the worst numerical errors. In any case, do not directly invert the matrix as we have done above! 


## Maximum likelihood
Maximum likelihood offers a second interpretation of least squares, but starting with a probabilistic approach.

### Gaussian distribution
A Gaussian random variable in $\mathbb{R}$ has mean $\mu$ and variance $\sigma^2$. 

$$
\mathcal{N}(y \mid \mu, \sigma^2) = 
    \frac{1}{\sqrt{2\pi\sigma^2}} 
    \exp{\left[ -\frac{(y-\mu)^2}{2\sigma^2} \right]}
$$

For a Gaussian random *vector* (instead of a single random variable), with mean $\pmb{\mu}$ and covariance $\pmb{\Sigma}$ (which is positive semi-definite) is:

$$
\pmb{\mathcal{N}}(\pmb{y} \mid \pmb{\mu}, \pmb{\Sigma}) = 
    \frac{1}
         {\sqrt{(2\pi)^D \text{ det}(\pmb{\Sigma})}} 

    \exp{\left[ -\frac{1}{2} (\pmb{y} - \pmb{\mu})^T \pmb{\Sigma}^{-1} (\pmb{y} - \pmb{u}) \right]}
$$

Remember that $\pmb{y} \in \mathbb{R}^N$. 

As another reminder, two variables $x$ and $y$ are said to be **independent** when $p(x, y) = p(x)p(y)$.

### A probabilistic model for least squares
We assume that our data is generated by a linear model $\pmb{x}_n^T\pmb{w}$, with added Gaussian noise $\epsilon_n$:

$$
y_n = \pmb{x}_n^T\pmb{w} + \epsilon_n
$$

This is often a realistic assumption in practice.

![Noise generated by a Gaussian source](/images/ml/gaussian-noise.png)

The noise is $\epsilon_n \overset{\text{i.i.d.}}{\sim} \mathcal{N}(y_n \mid \mu = 0, \sigma^2)$ for each dimension $n$. In other words, it is centered at 0, has a certain variance, and the error in each dimension is independent of that in other dimensions. The model $\pmb{w}$ is, as always, unknown.

Given $N$ samples, the **likelihood** of the data vector $\pmb{y} = (y_1, \dots, y_n)$ given the model $\pmb{w}$ and the input $\pmb{X}$ (where each row is one input) is:

$$
p(\pmb{y} \mid \pmb{X}, \pmb{w}) 
    = \prod_{n=1}^N {p(y_n \mid \pmb{x}_n, \pmb{w})}
    = \prod_{n=1}^N {\mathcal{N}(y_n \mid \pmb{x}_n^T\pmb{w}, \sigma^2)}
$$

Intuitively, we'd like to maximize this likelihood over the choice of the best model $\pmb{w}$. The best model is the one that maximizes this likelihood.

### Defining cost with log-likelihood
The log-likelihood (LL) is given by:

$$
\mathcal{L}_{LL} := \log{p(\pmb{y} \mid \pmb{X}, \pmb{w})}
    = - \frac{1}{2\sigma^2} \sum_{n=1}^N{\left(y_n - \pmb{x}_n^T\pmb{w}\right)^2} + \text{ cnst}
$$

Taking the log allows us to get away from the nasty product, and get a nice sum instead.

Notice that this definition looks pretty similar to MSE:

$$
\mathcal{L}_{\text{MSE}}(\pmb{w}) := \frac{1}{N} \sum_{n=1}^N \left(y_n - f(\pmb{x}_n)\right)^2
$$

Note that we would like to minimize MSE, but we want LL to be as high as possible (intuitively, we can look at the sign to understand that).

### Maximum likelihood estimator (MLE)
Maximizing the log-likelihood (and thus the likelihood) will be equivalent to minimizing the MSE; this gives us another way to design cost functions. We can describe the whole process as:

$$
\argmin_{\pmb{w}}{\mathcal{L}_\text{MSE}(\pmb{w})} =
\argmax_{\pmb{w}}{\mathcal{L}_\text{LL}(\pmb{w})}
$$

The maximum likelihood estimator (MLE) can be understood as finding the model under which the observed data is most likely to have been generated from (probabilistically). This interpretation has some advantages that we discuss below. 

#### Properties of MLE
MLE is a *sample* approximation to the *expected log-likelihood*. In other words, if we had an infinite amount of data, MLE would perfectly be equal to the expected value of the log-likelihood.

$$
\mathcal{L}_{LL}(\pmb{w}) 
    \approx \expectsub{p(y, \pmb{x})}{\log{p(x \mid \pmb{x}, \pmb{w})}}
$$

This means that MLE is **consistent**, i.e. it gives us the correct model assuming we have enough data. In probability, we can write this as:

$$
\pmb{w}_\text{MLE} \longrightarrow^p \pmb{w}_\text{true}
$$

This sounds amazing, but the catch is that this all is under the assumption that the noise $\epsilon$ indeed was generated under a Gaussian model.

## Overfitting and underfitting

### Underfitting with linear models
Linear models can very easily underfit; as soon as the data itself is given by anything more complex than a line, fitting a linear model will underfit: the model is too simple for the data, and we'll have huge errors.

But we can also easily overfit, where our model learns the specificities of the data too intimately. And this happens quite easily with linear combination of high-degree polynomials.

### Extended feature vectors
We can actually get high-degree linear combinations of polynomials, but still keep our linear model. Instead of making the model more complex, we simply "augment" the input to become degree $M$. If the input is one-dimensional, we can add a polynomial basis to the input:

$$
\pmb{\phi}(x_n) =
\begin{bmatrix}
1 & x_n & x_n^2 & x_n^3 & \dots & x_n^M
\end{bmatrix}
$$

Note that this is basically a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix).

We then fit a linear model to this extended feature vector $\pmb{\phi}(x_n)$:

$$
y_n \approx w_0 + w_1 x_n + w_2 x_n^2 + \dots + w_m x_n^M =: \pmb{\phi}(x_n)^T\pmb{w}
$$

Here, $\pmb{w}\in\mathbb{R}^{M+1}$. In other words, there are $M+1$ parameters in a degree $M$ extended feature vector. One should be careful with this degree; too high may overfit, too low may underfit.

If it is important to distinguish the original input $\pmb{x}$ from the augmented input $\pmb{\phi}(\pmb{x})$ then we will use the $\pmb{\phi}(\pmb{x})$ notation. But often, we can just consider this as a part of the pre-processing, and simply write $\pmb{x}$ as the input, which will save us a lot of notation.

### Reducing overfitting
To reduce overfitting, we can chose a less complex model (in the above, we can pick a lower degree $M$), but we could also just add more data:

![An overfitted model acts more reasonably when we add a bunch of data](/images/ml/reduce-overfit-add-data.png)

## Regularization
To prevent overfitting, we can introduce **regularization** to penalize complex models. This can be applied to any model.

The idea is to not only minimize cost, but also minimize a regularizer:

$$
\min_{\pmb{w}} {\mathcal{L}(\pmb{w}) + \Omega(\pmb{w})}
$$

The $\Omega$ function is the regularizer, measuring the complexity of the model. We'll see some good candidates for the regularizer below.

### $L_2$-Regularization: Ridge Regression
The most frequently used regularizer is the standard Euclidean norm ($L_2$-norm):

$$
\Omega(\pmb{w}) = \lambda \norm{\pmb{w}}^2_2
$$

Where $\lambda \in \mathbb{R}$. The value of $\lambda$ will affect the fit; $\lambda \rightarrow 0$ can have overfitting, while $\lambda \rightarrow \infty$ can have underfitting.

The norm is given by:

$$
\norm{\pmb{w}}_2^2 = \sum_i{w_i^2}
$$

The main effect of this is that large model weights $w_i$ will be penalized, while small ones won't affect our minimization too much.

#### Ridge regression
Depending on the values we choose for $\mathcal{L}$ and $\Omega$, we get into some special cases. For instance, choosing MSE for $\mathcal{L}$ is called **ridge regression**, in which we optimize the following:

$$
\min_{\pmb{w}} {\left(\frac{1}{N} \sum_{n=1}^N \left[y_n - f(\pmb{x}_n)\right]^2 \quad + \quad \Omega(\pmb{w})\right)}
$$

Least squares is also a special case of ridge regression, where $\lambda = 0$

We can find an explicit solution for $\pmb{w}$ in ridge regression by differentiating the cost and regularizer, and setting them to zero:

$$
\begin{align}
\nabla \mathcal{L}(\pmb{w}) & = -\frac{1}{N} \pmb{X}^T (\pmb{y} - \pmb{Xw}) \\ \\
\nabla \Omega(\pmb{w}) & = 2\lambda \pmb{w} \\
\end{align}
$$

We can now set the full cost to zero, which gives us the result:

$$
\pmb{w}^*_\text{ridge} = (\pmb{X}^T\pmb{X} + \lambda' \pmb{I})^{-1}\pmb{X}^T\pmb{y}
$$

Where $\frac{\lambda'}{2N} = \lambda$. Note that for $\lambda = 0$, we indeed have the solution least squares. 

#### Ridge regression to fight ill-conditioning 
This formulation of $\pmb{w}^*$ is quite nice, because adding the identity matrix helps us get something that always is invertible; in cases where we have ill-conditioned matrices, it also means that we can invert with more stability.

We'll prove that the matrix indeed is invertible. The gist is that the eigenvalues of $(\pmb{X}^T\pmb{X} + \lambda' \pmb{I})$ are all at least $\lambda'$. 

To prove it, we'll write the singular value decomposition (SVD) of $\pmb{X}^T\pmb{X}$ as $\pmb{USU}^T$. We then have:

$$
\pmb{X}^T\pmb{X} + \lambda'\pmb{I} = \pmb{USU}^T + \lambda'\pmb{UIU}^T = \pmb{U}(\pmb{S} + \lambda'\pmb{I})\pmb{U}^T
$$

The singular value is "lifted" by an amount $\lambda'$. There's an alternative proof in the class notes, but we won't go into that.

### $L_1$-Regularization: The Lasso
We can use a different norm as an alternative measure of complexity. The combination of $L_1$-norm and MSE is known as **The Lasso**:

$$
\min_{\pmb{w}} {\frac{1}{2N} \sum_{n=1}^N \left[y_n - f(\pmb{x}_n)\right]^2 \quad + \quad \lambda \norm{w}_1}
$$

Where the $L_1$-norm is defined as

$$
\norm{w}_1 := \sum_i{\abs{w_i}}
$$

If we draw out a constant value of the $L_1$ norm, we get a sort of "ball". Below, we've graphed $\left\\{ \pmb{w} \mid \norm{\pmb{w}}_1 \le 5 \right\\}$.

![Graph of the lasso](/images/ml/lasso.png)

To keep things in the following, we'll just claim that $\pmb{X}^T\pmb{X}$ is invertible. We'll also claim that the following set is an ellipsoid which scales around the origin as we change $\alpha$:

$$
\left\{
    \pmb{w} \mid \norm{\pmb{y} - \pmb{Xw}}^2 = \alpha
\right\}
$$

The slides have a formal proof for this, but we won't get into it.

Note that the above definition of the set corresponds to the set of points with equal loss (which we can assume is MSE, for instance):

$$
\left\{
    \pmb{w} \mid \mathcal{L}(\pmb{w}) = \alpha
\right\}
$$

Under these assumptions, we claim that for $L_1$ regularization, the optimum solution will likely be sparse (many zero components) compared to $L_2$ regularization.

To prove this, suppose we know the $L_1$ norm of the optimum solution. Visualizing that ball, we know that our optimum solution $\pmb{w}^\*$ will be somewhere on the surface of that ball. We also know that there are ellipsoids, all with the same mean and rotation, describing the equal error surfaces. The optimum solution is where the "smallest" of these ellipsoids just touches the
$L_1$ ball.

![Intersection of the L1 ball and the cost ellipses](/images/ml/ball-ellipse.png)

Due to the geometry of this ball this point is more likely to be on one of the "corner" points. In turn, sparsity is desirable, since it leads to a "simple" model.

## Model selection
As we've seen in ridge regression, we have a *regularization parameter* $\lambda > 0$ that can be tuned to reduce overfitting by reducing model complexity. We say that the parameter $\lambda$  is a **hyperparameter**.

We've also seen ways to enrich model complexity, like [polynomial feature expansion](#extended-feature-vectors), in which the degree $M$ is also a hyperparameter.

We'll now see how best to choose these hyperparameters; this is called the **model selection** problem.

### Probabilistic setup
We assume that there is an (unknown) underlying distribution $\mathcal{D}$ producing the dataset, with range $\mathcal{X}\times\mathcal{Y}$. The dataset $\mathcal{S}$ we see is produced from samples from $\mathcal{D}$:

$$
S = \left\{(\pmb{x}_n, y_n) 
\overset{\text{i.i.d}}{\sim}
\mathcal{D}\right\}_{n=1}^N
$$

Based on this, the *learning algorithm* choses the "best" model, under the parameters of the algorithm.

We write $f_s = \mathcal{A}(S)$, where $\mathcal{A}$ denotes the learning algorithm. It depends on the data subset we're given, and $f_s$ is the resulting prediction of our model.

To indicate that $f_s$ sometimes depend on hyperparameters, we'll write $f_{s, \lambda}$.

### Training Error vs. Generalization Error
Given a model $f$, how can we asses if $f$ is any good? We already have the loss function, but its result is highly dependent on the error in the data, not to how good the model is. Instead, we can compute the *expected error* over all samples chosen according to $\mathcal{D}$.

$$
L_\mathcal{D}(f) = \expectsub{\mathcal{D}}{\mathcal{l}(y, f(\pmb{x}))}
$$

Where $\mathcal{l}(\cdot, \cdot)$ is our loss function; e.g. for ridge regression, $\mathcal{l}(y, f(\pmb{x})) = \frac{1}{2}(y-f(\pmb{x}))^2$.

The quantity $L_\mathcal{D}(f)$ has many names, including **generalization error** (or true/expected error/risk/loss). This is the quantity that we are fundamentally interested in, but we cannot compute it since $\mathcal{D}$ is unknown.

What we do know is the data subset $\mathcal{S}$. It's therefore natural to compute the equivalent *empirical* quantity, which is the average loss:

$$
L_S(f) = \frac{1}{\abs{S}} \sum_{(\pmb{x}_n, y_n)\in S} {\mathcal{l}(y_n, f(\pmb{x}_n))}
$$

But again, we run into trouble. The function $f$ is itself a function of the data $S$, so what we really do is to compute the quantity:

$$
L_S(f_S) = \frac{1}{\abs{S}} \sum_{(\pmb{x}_n, y_n)\in S} {\mathcal{l}(y_n, f_S(\pmb{x}_n))}
$$

$f_S$ is the trained model. This is called the **training error**. Usually, the training error is smaller than the generalization error, because overfitting can happen (even with regularization, because the hyperparameter may still be too low).

### Splitting the data
To avoid validating the model on the same data subset we trained it on (which is conducive to overfitting), we can split the data into a **training set** and a **test set** (aka *validation set*), which we call $\Strain$ and $\Stest$.

We apply the learning algorithm $\mathcal{A}$ on the training set $\Strain$, and compute the function $f_{\Strain}$. We then compute the error on the test set:

$$
L_{\Stest}(f_{\Strain}) = \frac{1}{\abs{\Stest}} \sum_{(\pmb{x}_n, y_n)\in \Stest} {\mathcal{l}(y_n, f_{\Strain}(\pmb{x}_n))}
$$

If we have duplicates in our data, then this could be a bit dangerous. Still, in general, this really helps us with the problem of overfitting since $\Stest$ is a "fresh" sample, which means that we can hope that $L_{\Stest}(f_{\Strain})$ defined above is close to the quantity $L_\mathcal{D}(f_{\Strain})$. Indeed, *in expectation* both are the same:

$$
L_\mathcal{D}(f_{\Strain}) 
= \expectsub{\Stest\sim\mathcal{D}}{
    L_{\Stest}(f_{\Strain})
}
$$

This is a quite nice property, but there are a few limits. First, we paid a price by splitting the data and thus reducing the size of our training data, though this can be mediated using cross-validation, which we'll see later. 

### Generalization error vs test error
Assume that we have a model $f$ and that our loss function $\mathcal{l}(\cdot, \cdot)$ is bounded in $[a, b]$. We are given a test set $\Stest$ chosen i.i.d. from the underlying distribution $\mathcal{D}$. 

How far apart is the test error (empirical) from the true generalization error? As we've seen above, they are the same in expectation. But we need to worry about the variation, about how far off from the true error we typically are:

We claim that:

$$
\mathbb{P}\left[
    \abs{L_\mathcal{D}(f) - L_{\Stest}(f)}
    \ge
    \sqrt{\frac{(b-a)^2 \ln{(2/\delta)}}{2\abs{\Stest}}}
\right]
\le \delta
\label{eq:loss-bound}
\tag{loss-bound}
$$

Where $\delta > 0$ is a quality parameter. This gives us an upper bound on how far away our empirical loss is from the true loss.

This bound gives us some nice insights. Error decreases in the size of the test set as $\mathcal{O}(1/\sqrt{\abs{\Stest}})$, so the more data points we have, the more confident we can be in the empirical loss being close to the true loss.

We'll prove $\ref{eq:loss-bound}$. We assumed that each sample in the test set is chosen independently. Therefore, given a model $f$, the associated losses $\mathcal{l}(y_n, f(\pmb{x}_n))$ are also i.i.d. random variables, taking values in $[a, b]$ by assumption. We can call each such loss $\Theta_n$:

$$
\Theta_n = \mathcal{l}(y_n, f(\pmb{x}_n))
$$

This is just a naming alias; since the underlying value is that of the loss function, the expected value of $\Theta_n$ is simply that of the loss function, which is the true loss:

$$
\expect{\Theta_n} = \expect{\mathcal{l}(y_n, f(\pmb{x}_n))} = L_\mathcal{D}(f)
$$

The empirical loss on the other hand is equal to the average of $\abs{\Stest}$ such i.i.d. values. 

The formula of $\ref{eq:loss-bound}$ gives us the probability that empirical loss $L_{\Stest}(f)$ diverges from the true loss by more than a given constant, which is a classical problem addressed in the following lemma (which we'll just assert, not prove).

{% comment %} lemma Chernoff Bound {% endcomment %}
**Chernoff Bound**: Let $\Theta_1, \dots, \Theta_n$ be a sequence of i.i.d random variables with mean $\expect{\Theta}$ and range $[a, b]$. Then, for any $\epsilon > 0$:

$$
\mathbb{P}\left[
    \abs{\frac{1}{N}\sum_{n=1}^N {\Theta_n - \expect{\Theta}}}
    \ge
    \epsilon
\right]
\le
2\exp{\left(\frac{-2N\epsilon^2}{(b-a)^2}\right)}
\label{eq:Chernoff}
\tag{Chernoff}
$$
{% comment %} endlemma {% endcomment %}

Using $\ref{eq:Chernoff}$ we can show $\ref{eq:loss-bound}$. By setting $\delta = 2\exp{\left(\frac{-2N\epsilon^2}{(b-a)^2}\right)}$, we find that $\epsilon = \sqrt{\frac{(b-a)^2 \ln{(2/\delta)}}{2\abs{\Stest}}}$ as claimed.

### Model selection
Our main goal was to look for a way to select the hyperparameters of our model. Given a finite set of values $\lambda_k$ for $k=1, \dots, K$ of a hyperparameter $\lambda$, we can run the learning algorithm $K$ times on the same training set $\Strain$, and compute the $K$ prediction functions $f_{\Strain, \lambda_k}$. For each such prediction function we compute the test error, and choose the $\lambda_k$ which minimizes the test error.

This is essentially a grid search on $\lambda$ using the test error function.

#### Model selection based on test error
How do we know that, for a fixed function $f$, $L_{\Stest}(f)$ is a good approximation to $f_\mathcal{D}$?

The answer to this follows the same idea as when we talked about [generalization vs test error](#generalization-error-vs-test-error), but we now assume that we have $K$ models $f_k$ for $k=1, \dots, K$. We assume again that the loss function is bounded in $[a, b]$, and that we're given a test set whose samples are chosen i.i.d. in $\mathcal{D}$.

How far is each of the $K$ (empirical) test errors $L_{\Stest}(f_k)$ from the true $L_\mathcal{D}(f_k)$? As before, we can bound the deviation for all $k$ candidates, by:

$$
\mathbb{P}\left[
    \max_k {\abs{L_\mathcal{D}(f_k) - L_{\Stest}(f_k)}}
    \ge
    \sqrt{\frac{(b-a)^2 \ln{(2K/\delta)}}{2\abs{\Stest}}}
\right]
\le \delta
$$

A bit of intuition of where this comes from: for a general $K$, if we check the deviations for $K$ independent samples and ask for the probability that for at least one such sample we get a deviation of at least $\epsilon$ (this is what the Chernoff bound answers). Then by the union bound this probability is at most $K$ times as large as in the case where we are only concerned with a single instance. Thus the upper bound in Chernoff becomes $2K\exp{\left(\frac{-2N\epsilon^2}{(b-a)^2}\right)}$, which gives us $\epsilon = \sqrt{\frac{(b-a)^2 \ln{(2K/\delta)}}{2\abs{\Stest}}}$ as above.

As before, this tells us that error decreases in $\mathcal{O}(1/\sqrt{\abs{\Stest}})$. Now that we test $K$ hyperparameters, our error only goes up by a tiny amount of $\sqrt{\ln{(K)}}$. This follows from $\ref{eq:loss-bound}$, which we proved for the special case of $K = 1$.

### Cross-validation
Splitting the data once into two parts (one for training and one for testing) is not the most efficient way to use the data. Cross-validation is a better way.

K-fold cross-validation is a popular variant. We randomly partition the data into $K$ groups, and train $K$ times. Each time, we use one of the $K$ groups as our test set, and the remaining $K−1$ groups for training. 

To get a common result, we average out the $K$ results. This means we'll use  the average weights to get the average test error over the $K$ folds.

Cross-validation returns an unbiased estimate of the generalization error and its variance.

### Bias-Variance decomposition
When we perform model selection, there is an inherent [bias&ndash;variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) trade-off.

{% comment %} Todo copy bias variance image {% endcomment %}

For now, we'll just look at "high-bias & low-variance" models, and "high-variance & low-bias" models.

- **High-bias & low-variance**: the model is too simple. It's underfit, has a large bias, and and the variance of $L_\mathcal{D}(f_S)$ is small.
- **High-variance & low-bias**: the model is too complex. It's overfit, has a small bias and large variance of $L_\mathcal{D}(f_S)$ (as a single addition of a data point is likely to change the prediction function $f_S$ considerably)

Consider a linear regression with one-dimensional input and [polynomial feature expansion](#extended-feature-vectors) of degree $d$. The former can be achieved by picking a too low value for $d$, while the latter by picking $d$ too high. The same principle applies for other parameters, such as ridge regression with hyperparameter $\lambda$.

#### Data generation model
Let's assume that our data is generated by some arbitrary, unknown function $f$, and a noise source with distribution $\mathcal{D}_\epsilon$ (i.i.d. from sample to sample, and independent from the data). We can think of $f$ representing the precise, hypothetical function that perfectly produced the data. We assume that the noise has mean zero (without loss of generality, as a non-zero mean could be encoded into $f$).

$$
y = f(\pmb{x}) + \epsilon
$$

We assume that $\pmb{x}$ is generated according to some fixed but unknown distribution $\mathcal{D}_{\pmb{x}}$. We'll be working with square loss as our loss function $\mathcal{l}(\cdot, \cdot)$. We will denote the joint distribution on pairs $(\pmb{x}, y)$ as $\mathcal{D}$.

#### Error Decomposition
As always, we have a training set $\Strain$, which consists of $N$ i.i.d. samples from $\mathcal{D}$. Given our learning algorithm $\mathcal{A}$, we compute the prediction function $f_{\Strain} = \mathcal{A}(\Strain)$. The square loss of a single prediction for a fixed element $\pmb{x}_0$ is given by the computation of:

$$
\bigl( y_0 - f_{\Strain}(\pmb{x}_0) \bigr)^2
= 
\bigl( f(\pmb{x}_0) + \epsilon - f_{\Strain}(\pmb{x}_0) \bigr)^2
$$

Our experiment was to create $\Strain$, learn $f_{\Strain}$, and then evaluate the performance by computing the square loss for a fixed element $\pmb{x}_0$. If we run this experiment many times, the expected value is written as:

$$
\expectsub{\Strain \sim \mathcal{D},\ \epsilon\sim\mathcal{D}_\epsilon}{
    \left( f(\pmb{x}_0) + \epsilon - f_{\Strain}(\pmb{x}_0) \right)^2
}
$$

We will now show that this expression can be rewritten as a sum of three non-negative terms, and that each of these:

$$
\newcommand{\otherconstantterm}{\mathbb{E}_{S'_\text{train}\sim\mathcal{D}}\left[f_{S'_\text{train}}(\pmb{x}_0)\right]}

\begin{align}
& \expectsub{\Strain \sim \mathcal{D},\ \epsilon\sim\mathcal{D}_\epsilon} {
    \left( f(\pmb{x}_0) + \epsilon - f_{\Strain}(\pmb{x}_0) \right)^2
} \\

\overset{(a)}{=}\  & 
    \expectsub{\epsilon\sim\mathcal{D}_\epsilon} {
        \epsilon^2
    }
    + \expectsub{\Strain \sim \mathcal{D}} {
        \bigl(f(\pmb{x}_0) - f_{\Strain}(\pmb{x}_0)\bigl)^2
    } \\

\overset{(b)}{=}\ & 
    \text{Var}_{\epsilon\sim\mathcal{D}_\epsilon}\left[\epsilon\right]
    + \expectsub{\Strain \sim \mathcal{D}}{
        \bigl(f(\pmb{x}_0) - f_{\Strain}(\pmb{x}_0)\bigl)^2
    } \\

\overset{(c)}{=}\ &
    \underbrace{
        \text{Var}_{\epsilon\sim\mathcal{D}_\epsilon}\left[\epsilon\right]
    }_\text{noise variance} \\
& + \underbrace{
    \left( f(\pmb{x}_0) - \otherconstantterm \right)^2
}_\text{bias} \\
& + \expectsub{\Strain\sim\mathcal{D}} {
        \underbrace{
            \left( \otherconstantterm - f_{\Strain(\pmb{x}_0)} \right)^2
        }_\text{variance}
    } \\
\end{align}
$$

Note that here, $S\'\_\text{train}$ is a second training set, also sampled from $\mathcal{D}$, that is independent of the training set $\Strain$. It has the same expectation, but it is different and thus produces a different trained model $f_{S'}$.

Step $(a)$ uses $(u+v)^2 = u^2 + 2uv + v^2$ as well as linearity of expectation produce $\expect{(u+v)^2} = \expect{u^2} + 2\expect{uv} + \expect{v^2}$. Note that the $2uv$ part is zero as the noise $\epsilon$ is independent from $\Strain$.

Step $(b)$ uses the definition of variance as:

$$
\text{Var}(X) = \expect{(X - \expect{X})^2} = \expect{X^2} - \expect{X}^2
$$

Seeing that our noise $\epsilon$ has mean zero, we have $\expect{\epsilon}^2 = 0$ and therefore $\text{Var}(\epsilon) = \expect{\epsilon^2}$. 

In step $(c)$, we add and subtract the constant term $\otherconstantterm$ to the expression like so:

$$
\expectsub{S\sim \mathcal{D}}{\left(
    \underbrace{A - \otherconstantterm}_u + \underbrace{\otherconstantterm + B}_v
\right)^2}
$$

We can then expand the square. The $2uv$ part of the expansion is zero, as we show below:

$$
\begin{align}
& \expectsub{S \sim \mathcal{D}} {
    \left( 
        f(\pmb{x}_0) - \otherconstantterm 
    \right) \cdot \left(
        \otherconstantterm - f_S(\pmb{x}_0)
    \right)
} \\
& = \left(
    f(\pmb{x}_0) - \otherconstantterm 
\right) \cdot \expectsub{S\sim\mathcal{D}} {
    \otherconstantterm - f_S(\pmb{x}_0)
} \\
& = \left(
    f(\pmb{x}_0) - \otherconstantterm 
\right) \cdot \left(
    \otherconstantterm - \expectsub{S\sim\mathcal{D}}{S(\pmb{x}_0)}
\right) \\
& = 0 \\
\end{align} 
$$

#### Interpretation of the decomposition
Each of the three terms in non-negative, so each of them is a lower bound on the expected loss when we predict the value for the input $\pmb{x}_0$.

- When the data contains **noise**, then that imposes a strict lower bound on the error we can achieve.
- The **bias term** is a non-negative term that tells us how far we are from the true value, in expectation. It's the square loss between the actual value $f(\pmb{x}_0)$ and the expected prediction, where the expectation is over the training sets. As [we discussed above](#bias-variance-decomposition), with a simple model we will not find a good fit in average, which means the bias will be large, which adds to the error we observe.
- The **variance term** is the variance of the prediction function. For complex models, small variations in the data set can produce vastly different models, and our prediction will vary widely, which also adds to our total error.

## Classification
When we did regression, our data was of the form:

$$
\mathcal{S}_\text{train} = \left\{(x_n, y_n)\right\}_{n=1}^N,
\qquad x_n \in \mathbb{R}^d,\ y_n \in\mathbb{R}
$$

With **classification**, our prediction is no longer discrete. Now, $y_n\in\left\\{\mathcal{C}\_0, \dots, \mathcal{C}_{K-1} \right\\}$. If it can only take two values (i.e. $K=2$), then it is called **binary classification**. If it can take more than two values, it is **multi-class classification**.

There is no ordering among these classes, so we may sometimes denote these labels as $y\in\left\\{0, 1, 2, \dots, K-1\right\\}$.

If we have an underlying distribution $\mathcal{D}$, then we can write:

$$
\expect{\mathbb{I}\left\{ y-f(x) \ne 0 \right\}} = \mathbb{P}(y-f(x) \ne 0)
$$

Where $\mathbb{I}$ is an indicator function that returns 1 when the condition is correct, and 0 otherwise.

### Linear classifier
A classifier will divide the input space into a collection of regions belonging to each class; the boundaries are called **decision boundaries**.

A linear classifier splits the input with a line in 2D, a plane in 3D, or more generally, a hyperplane. But a linear classifier can also classify more complex shapes if we allow for [feature augmentation](#extended-feature-vectors). For instance (in 2D), if we augment the input to degree $M=2$ and a constant factor, our linear classifier can also detect ellipsoids. So without loss of generality, we'll simply study linear classifiers and allow feature augmentation, without loss of generality.

### Is classification a special case of regression?
From the initial definition of classification, we see that it is a special case of regression, where the output $y$ is restricted to a small discrete set instead of a continuous spectrum.

We could construct classification from regression by simply rounding to the nearest $\mathcal{C}\_i$ value. For instance, if we have $y\in\left\\{0, 1\right\\}$, we can use (regularized) least-squares to learn a prediction function $f_{\Strain}$ for this regression problem. We can then convert the regression to a classification by rounding: we decide on $\mathcal{C}\_1=0$ if $f_{\Strain}(\pmb{x})<0.5$ and $\mathcal{C}\_2=1$ if $f_{\Strain}(\pmb{x})>0.5$.

But this is somewhat questionable as an approach. MSE penalizes points that are far away from the result **before rounding**, even though they would be correct **after rounding**. This means that the line will likely not be a good line. 

With MSE, the "position" of the line defined by $f_{\Strain}$ will depend crucially on how many points are in each class, and where the points lie. This is not desirable for classification: instead of minimizing the cost function, we'd like for the fraction of misclassified cases to be small. The mean-squared error turns out to be only loosely related to this.

So instead of building classification as a special case of regression, let's take a look at some basic alternative ideas to perform classification.

### Nearest neighbor
In some cases it is reasonable to postulate that there is some spatial correlations between points of the same class: inputs that are "close" are also likely to have the same label. Closeness may be measured by Euclidean distance, for instance.

This can be generalized easily: instead of taking the single nearest neighbor, a process very prone to being swayed by outliers, we can take the $k$ nearest neighbors ([k-NN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)), or a weighted linear combination of elements in the neighborhood ([smoothing kernels](https://en.wikipedia.org/wiki/Kernel_smoother)).

But this idea fails miserably in high dimensions, where the geometry renders the idea of "closeness" meaningless: in a high-dimensional space, if we grow the area around a point, we're likely to see no one for a very long time, and then 💥, everyone. This is known as the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). The idea also fails when we have too little data, especially in high dimensions, where the closest point may actually be far away and a very bad indicator of the local situation.

### Linear decision boundaries 
As a starting point, we can assume that decision boundaries are linear (hyperplanes). To keep things simple, we can assume that there is a separating hyperplane, i.e. a hyperplane so that no point in the training set is misclassified.

There may be many such lines, so which one do we pick? This may be a little hand-wavy, but the intuition is the most "robustness", or the one that offers the greatest "margin": we want to be able to "wiggle" the inputs as much as possible while keeping the numbers of misclassifications low. This idea will lead us to *support vector machines* (SVMs).

But the linear decision boundaries are limited, and in many cases too strong of an assumption. We can augment the feature vector with some non-linear functions, which is what we do with the kernel trick, which we will talk about later. Another option is to use neural networks to find an appropriate non-linear transform of the inputs.

### Optimal classification for a known generating model
To find a solution, we can gain some insights if we assume that we know the joint distribution $p(\pmb{x}, y)$ that created the data (where $y$ takes values in a discrete set $\mathcal{y}$). In practice, we don't know the model, but this is just a thought experiment. We can assume that the data was generated from a model $(\pmb{x}, y)\sim\mathcal{D}$, where $y=g(\pmb{x})+\epsilon$, where $\epsilon$ is noise.

Given the fact that there is noise, a perfect solution may not always be possible. But if we see an input $\pmb{x}$, how can we pick an optimal choice $\hat{y}(\pmb{x})$ for this distribution? We want to maximize the probability of guessing the correct label, so we should choose according to the rule:

$$
\hat{y}(\pmb{x}) = \argmax_{y\in\mathcal{Y}}{p(y\mid\pmb{x})}
$$

This is known as the maximum a-posteriori (MAP) criterion, since we maximize the posterior probability (the probability of a class label *after* having observed the input).

The probability of a correct guess is thus the average over all inputs of the MAP, i.e.:

$$
\mathbb{P}(\hat{y}(\pmb{x}) = y) = \int{p(\pmb{x})p(\hat{y}(\pmb{x})\mid \pmb{x})dx}
$$

In practice we of course do not know the joint distribution, but we could use this approach by using the data itself to learn the distribution (perhaps under the assumption that it is Gaussian, and just fitting the $\mu$ and $\sigma$ parameters).

## Logistic regression
Recall that [we discussed](#is-classification-a-special-case-of-regression) what happens if we look at binary classification as a regression. We also discussed that it is tempting to look at the predicted value as a probability (i.e. if the regression says 0.8, we could interpret it as 80% certainty of $\mathcal{C}\_1 = 0$ and 20% probability of $\mathcal{C}\_2 = 1$). But this leads to problems, as the predicted values may not be in $[0, 1]$, even largely surpassing these bounds, and this contributes to the error in MSE even though they indicate high certainty.

So the natural idea is to *transform* the prediction, which can take values in $(-\infty, \infty)$, into a true probability in $[0, 1]$. This is done by applying an appropriate function, one of which is the *logistic* function:

$$
\sigma(z) := \frac{e^z}{1+e^z}
$$

How do we use this? Let's consider binary classification, with labels 0 and 1. Given a training set, we learn a weight vector $\pmb{w}$, and a "shift" scalar $\pmb{w}_0$. 

Note that $w_0$ can, for the sake of simpler notation, be considered to be a constant feature in $\pmb{w}$. [As before](#multiple-linear-regression), we'll use this notation to keep things concise.

Given a new feature vector $\pmb{x}$, the *probability* of the class labels given $\pmb{x}$ are:

$$
\begin{align}
p(1 \mid \pmb{x}) & = \sigma(\pmb{x}^T\pmb{w}) \\
p(0 \mid \pmb{x}) & = 1 - \sigma(\pmb{x}^T\pmb{w}) \\
\end{align}
$$

This allows us to predict a certainty, which is a real value and not a label, which is why logistic regression is called regression, even though it is still part of a classification scheme. Indeed, we typically use logistic regression as the first step of a classifier.

### Training
To train the classifier, the intuition is that we'd like to maximize the likelihood of our weight vector explaining the data:

$$
\argmax_{\pmb{w}}{p(y, X \mid \pmb{w})}
$$

[As with MLE](#properties-of-mle), this is **consistent**, it gives us the correct model assuming we have enough data. Using the chain rule for probabilities, the probability becomes:

$$
p(y, X \mid \pmb{w}) = p(\pmb{X}\mid\pmb{w})p(\pmb{y} \mid \pmb{X}, \pmb{w}) = p(\pmb{X})p(\pmb{y} \mid \pmb{X}, \pmb{w})
$$

As we're trying to get the argmax over the weights, we can discard $p(X)$ as it doesn't depend on $\pmb{w}$. Therefore:

$$
\argmax_{\pmb{w}}{p(\pmb{y}, \pmb{X} \mid \pmb{w})} = \argmax_{\pmb{w}}{p(\pmb{y} \mid \pmb{X}, \pmb{w})}
$$

Using the fact that the samples in the dataset are independent, and given the above formulation of the prior, we can express the maximum likelihood criterion for the general case (in the previous section, we had only done it for the binary case $N=2$):

$$
\begin{align}
p(\pmb{y} \mid \pmb{X}, \pmb{w})
    & = p(y_1, \dots, y_N \mid \pmb{x}_1, \dots, \pmb{x}_N, \pmb{w}) \\
    & = \prod_{n=1}^N{p(y_n \mid \pmb{x}_n), \pmb{w}} \\
    & = \prod_{n=1}^N{\sigma(x_n^T \pmb{w})^{y_n} (1-\sigma(x_n^T \pmb{w})^{1-y_n})} \\
\end{align}
$$

But this product is nasty, so we'll remove it by taking the log. We also multiply by $-1$, which means we also need to be careful about taking the minimum instead of the maximum. The resulting cost function is thus:

$$
\begin{align}
\cost{\pmb{w}}
    & = -\sum_{n=1}^N{\left[
        y_n \log{(\sigma(x_n^T w))} + (1-y_n)\log{(1-\sigma(x_n^T w))}
    \right]} \\
    & = \sum_{n=1}^N{\log{(1+\exp{(x_n^T w)})} - x_n x_n^T w}
    \tag{Log-Likelihood}\label{eq:log-likelihood}
\end{align}
$$

### Conditions of optimality
As we discuss above, we'd like to minimize the cost $\cost{\pmb{w}}$. Let's look at the stationary points of our cost function by computing its gradient and setting it to zero.

It just turns out that taking the derivative of the logarithm in the inner part of the sum above gives us the logistic function:

$$
\frac{\partial \log{(1+\exp{(x_n^T w)})}}{\partial x} = \sigma(x)
$$

Therefore, the whole derivative is:

$$
\begin{align}
\nabla\cost{\pmb{w}} 
    & = \sum_{n=1}^N {x_n \sigma(\pmb{x}_n^T\pmb{w}) - y_n} \\
    & = \pmb{X}^T \left[ \sigma(\pmb{Xw}) - \pmb{y} \right]
\end{align}
$$

The matrix $\pmb{x}$ is $N\times N$; both $\pmb{y}$ and $\pmb{w}$ are column vectors of length $N$. Therefore, to simplify notation, we let $\sigma(\pmb{Xw})$ represent element-wise application of the sigmoid function on the size $N$ vector resulting from $\pmb{Xw}$.

There is no closed-form solution for this, so we'll discuss how to solve it in an iterative fashion by using gradient descent or the Newton method.

### Gradient descent
$\ref{eq:log-likelihood}$ is convex in the weight vector $\pmb{w}$. We can therefore do gradient descent on this cost function as we've always done:

$$
\pmb{w}^{(t+1)} := \pmb{w}^{(t)} - \gamma^{(t)}\nabla\cost(\pmb{w}^{(t)})
$$

### Newton's method
Gradient descent is a *first-order* method, using only the first derivative of the cost function. We can get a more powerful optimization algorithm using the second derivative. This is based on the idea of Taylor expansions. The order 2 Taylor expansion of the cost, around $\pmb{w}^*$, is:

$$
\cost{\pmb{w}} \approx \cost{\pmb{w}^*}^T(\pmb{w}-\pmb{w}^*) + \frac{1}{2}(\pmb{w}-\pmb{w}^*)^T H(\pmb{w}^*)(\pmb{w}-\pmb{w}^*)
$$

Where H denotes the Hessian, the $D\times D$ symmetric matrix with entries:

$$
H_{i, j} = \frac{\partial^2\cost{\pmb{w}}}{\partial w_i \partial w_j}
$$

#### Hessian of the log-likelihood
Let's compute this Hessian matrix. We've already computed the gradient of the cost function [in the section above](#conditions-of-optimality). Looking at the sum form (not the matrix form), we see that each term only depends on $\pmb{w}$ in the $\sigma(\pmb{x}_n^T w)$ term. Therefore, the Hessian associated to one term is:

$$
\pmb{x}_n(\nabla\sigma(\pmb{x}_n^T\pmb{w}))^T
$$

Given that the derivative of the sigmoid is $\sigma'(x) = \sigma(x)(1-\sigma(x))$, by the [chain rule](https://en.wikipedia.org/wiki/Chain_rule), each term of the sum gives rise to the Hessian:

$$
\pmb{x}_n\pmb{x}_n^T\sigma(\pmb{x}_n^T \pmb{w})(1 - \sigma(\pmb{x}_n^T \pmb{w}))
$$

#### Newton's method
In this model, we'll assume that the Taylor expansion above denotes the cost function exactly instead of approximately. In other words, we're assuming strict equality $=$ instead of approximation $\approx$ as above. This is only an assumption; it isn't strictly true, but it's a decent approximation. Where does this take minimum value? To know that, let's set the gradient of the Taylor expansion to zero. This yields:

$$
H(\pmb{w}^*)^{-1} \nabla\cost{\pmb{w}^*} = \pmb{w}^* - \pmb{w}
$$

If we solve for $\pmb{w}$, this gives us an iterative algorithm for finding the optimum:

$$
w^{(t+1)} = w^{(t)} - H(w^{(t)})^{-1} \nabla\cost{w^{(t)}} \gamma^{(t)}
$$

In this iterative algorithm, our starting point, $w^{(0)}$ corresponds to $w^*$.

The above skips a few steps, and is just meant to give the intuition of how we get to our result. In any way, the important thing to remember is the formula for the descent, and the fact that the Hessian can be computed as follows:

$$
\begin{align}
H(w) 
    & = \sum_{n=1}^N{\nabla^2\mathcal{L}_n(w)} \\
    & = \sum_{n=1}^N{
        \underbrace{\pmb{X}_n \pmb{X}_n^T}_{D\times D}
        \sigma(x_n^T w)
        \bigl(1 - \sigma(x_n^T w) \bigr)
    } \\
\end{align}
$$

This can also be written as:

$$
H(w) = 
    \underbrace{\  X^T \ }_{D\times N}\ 
    \underbrace{\ S\ }_{N\times N}\ 
    \underbrace{\  X \ }_{N\times D}
$$

The S matrix is diagonal, where:

$$
S_{n, n} = \sigma(x_n^T w)\bigl(1 - \sigma(x_n^T w) \bigr)
$$

The trade-off for the Newton method is that while we need fewer iterations, each of them is more costly. In practice, which one to use depends, but at least we have another option with the Newton method.

### Regularized logistic regression
If the data is linearly separable, there is no finite weight vector. Running the iterative algorithm will make the weights diverge to infinity. To avoid this, we can regularize with a penalty term.

$$
\argmin_w{-\sum_{n=1}^N{\log{p(y_n \mid \pmb{x}_n^T\pmb{w})}} + \frac{\lambda}{2}\norm{\pmb{w}}^2}
$$

## Generalized Linear Models
Previously, with [least squares](#least-squares), our data was of the form:

$$
y = x^T w + z, \quad \text{with } z\sim\mathcal{N}(0, \sigma^2)
$$

This is a D-linear model. When talking about generalized linear models, we're still talking about something linear, but we allow the noise $z$ to be something else than a Gaussian distribution.

### Motivation
The motivation for this is that while logistic regression allows for, say, binary outputs, we may want to have something equivalently computationally efficient for, say, $y\in\mathbb{N}$. To do so, we introduce different classes of distributions, called the *exponential family*, with which we can revisit logistic regression and get other properties.

This will be useful in adding a degree of freedom. Previously, we most often used linear models, in which we model the data as a line, plus zero-mean Gaussian noise. As we saw, this leads to least squares. When the data is more complex than a simple line, we saw that we could augment the features (e.g. with $x^2$, $x^3$), and still use a linear model. The idea was to augment the feature space $x$. This gave us an added degree of freedom, and allowed us to use linear models for higher-degree problems.

These linear models predicted the mean of the distribution from which we assumed the data to be sampled. When talking about mean here, we mean what we assume the data to be modeled after, without the noise. In this section, we'll see how we can use the linear model to predict a different quantity than the mean. This will allow us to add another degree of freedom, and use linear models to get other predictions than just the shape of the data.

We've actually already done this, without knowing it. In (binary) logistic regression, the probability of the classes was:

$$
\begin{align}
p(y = 1 \mid \eta) & = \sigma(\eta y) \\
p(y = 0 \mid \eta) & = 1 -  \sigma(\eta y) \\
\end{align}
$$

We're using $\eta$ as a shorthand for $\pmb{x}^T\pmb{w}$, and will do so in this section. More compactly, we can write this in a single formula:

$$
p(y\mid\eta) = \frac{e^{\eta y}}{1 + e^\eta} = \exp{\left[
    \eta y - \log{(1 + e^\eta)}
\right]}, \qquad y\in\left\{0, 1\right\}
$$

Note that this linear model does not predict the mean, which we'll denote $\mu$ (don't get confused by this notation; $\mu$ is in general not a scalar, it represents the "real values" that the data is modeled after, without the noise). Instead, our linear model predicts $\eta = \pmb{x}^T\pmb{w}$, which is transformed into the mean by using the $\sigma$ function:

$$
\mu = \sigma(\eta)
$$

This relation between $\mu$ and $\sigma$ is known as the **link function**. It is a nonlinear function that makes it possible to use a linear model to predict something else than the mean $\mu$.

### Exponential family
In general, the form of a distribution in the exponential family is:

$$
p(y\mid\pmb{\eta}) = h(y)\exp{\left[\pmb{\eta}^T\phi(y) - A(\pmb{\eta})\right]}
$$

Let's take a look at the various components of this distribution:

- $\pmb{\eta}$ is a shorthand for $\pmb{x}^T\pmb{w}$
- $\phi(y)$ is called a **sufficient statistics**. It's usually a vector. Its name stems from the fact that its empirical average is all we need to estimate $\pmb{\eta}$
- $A(\pmb{\eta})$ is the **log-partition function**, or the **cumulant**.

The domain of $y$ can be vary: we could choose $y\in\mathbb{R}$, $y\in\left\\{0, 1\right\\}$, $y\in\mathbb{N}$, etc. Depending on this, we may have to do sums or integrals in the following. 

We require that the probability be non-negative, so we need to ensure that $h(y) \ge 0$. Additionally, a probability distribution needs to integrate to 1, so we also require that that:

$$
\int_y{h(y)\exp{\left[\pmb{\eta}^T\phi(y) - A(\pmb{\eta})\right]}} dy = 1
$$

This can be rewritten to:

$$
\int_y{h(y)\exp{\left[\pmb{\eta}^T\phi(y)\right]}} dy = \exp{A(\eta)}
$$

The role of $A(\pmb{\eta})$ is thus only to ensure a proper normalization. To create a member of the exponential family, we can choose the factor $h(y)$, the vector $\phi(y)$ and the parameter $\pmb{\eta}$; the cumulant $A(\pmb{\eta})$ is then determined for each such choice, and ensures that the expression is properly normalized. From the above, it follows that $A(\pmb{\eta})$ is defined as:

$$
A(\eta) = \log{\left[\int_y{h(y)\exp{\left[\pmb{\eta}^T\phi(y) - A(\pmb{\eta})\right]}} dy\right]}
$$

We exclude the case where the integral is infinite, as we cannot compute a real $A(\eta)$ for that case.

#### Example: Bernoulli
The Bernoulli distribution is a member of the exponential family. Its probability density is given by:

$$
\begin{align}
p(y\mid\mu) 
    & = \mu^y(1-\mu)^{1-y}, \quad \text{where } \mu\in(0, 1) \\
    & = \exp{\left[
        \left( \log{\frac{\mu}{1-\mu}} \right) y +
        \log{(1 - \mu)}
     \right]} \\
    & = \exp{\left[\eta\phi(y) - A(\eta)\right]}
\end{align}
$$

The parameters are thus:

$$
\begin{align}
\phi(y) & = y \\
\eta    & = \log{\frac{\mu}{1-\mu}} \\
A(\eta) & = -\log{(1-\mu)}=\log{(1 + e^\eta)} \\
h(y)    & = 1
\end{align}
$$

Here, $\phi(y)$ is a scalar, which means that the family only depends on a single parameter. Note that $\eta$ and $\mu$ are linked:

$$
\eta = g(\mu) = \log{\frac{\mu}{1-\mu}} \iff \mu = g^{-1}(\eta) = \log{\frac{e^\eta}{1+e^\eta}} = \sigma(\eta)
$$

The link function is the same sigmoid function we encountered in logistic regression.

#### Example: Gaussian
The density of a Gaussian $\mathcal{N}(\mu, \sigma^2)$ is:

$$
p(y) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{-\frac{(y-\mu)^2}{2\sigma^2}},
\qquad \mu\in\mathbb{R}, 
\quad \sigma\in\mathbb{R}^+
$$

There are two parameters to choose in a Gaussian, $\mu$ and $\sigma$, so we can expect something of degree 2 in exponential form. Let's rewrite the above:

$$
\begin{align}
p(y) & = \exp{\left[
    -\frac{y^2}{2\sigma^2}
    + \frac{\mu y}{\sigma^2}
    - \underbrace{
        \frac{\mu^2}{2\sigma^2} - \frac{1}{2}\log{(2\pi\sigma^2)}
    }_{A(\eta)}
\right]} \\
& = \exp{\left[
    \eta^T\phi(y) - A(\eta)
\right]}
\end{align}
$$

Where:

$$
\begin{align}
h(y) & = 1 \\

\phi(y) & = \begin{bmatrix}
    y   \\
    y^2 \\
\end{bmatrix} \\

\eta & = \begin{bmatrix}
    \eta_1 \\
    \eta_2 \\
\end{bmatrix} = \begin{bmatrix}
    \frac{\mu}{\sigma^2} \\
    -\frac{1}{2\sigma^2} \\
\end{bmatrix} \\

A(\eta) & = \frac{\mu^2}{2\sigma^2} - \frac{1}{2}\log{(2\pi\sigma^2)}
    = \frac{\eta_1^2}{4\eta_2} - \frac{1}{2}\log{(-\eta_2/\pi)}
\end{align}
$$

Indeed, this time $\phi(y)$ is a vector of dimension 2, which reflects that the distribution depends on 2 parameters. As the formulation of $\eta$ shows, we have a 1-to-1 correspondence to $\pmb{\eta}=(\eta_1, \eta_2)$ and the $(\mu, \sigma^2)$ parameters:

$$
\eta_1 = \frac{\mu}{\sigma^2},\ \eta_2 = -\frac{1}{2\sigma^2}
\quad \iff \quad
\mu = -\frac{\eta_1}{2\eta_2},\ \sigma^2 = -\frac{1}{2\eta_2}
$$

#### Properties
1. $A(\eta)$ is convex
2. $\nabla_\eta A(\eta) = \expect{\phi(y)}$
3. $\nabla_\eta^2 A(\eta) = \expect{\phi(y)^T\phi(y)} - \expect{\phi(y)}^T\expect{\phi(y)}$

Proofs are in the lecture notes.

#### Link function
As we've seen above, there is a relationship between the mean $\pmb{\mu} := \expect{\phi(y)}$ and $\pmb{\eta}$ using the link function $g$:

$$
\pmb{\eta} = g(\pmb{\mu}) \iff \pmb{\mu} = g^{-1}(\pmb{\eta})
$$

For a list of such functions, consult the chapter on Generalized Linear Models in [the KPM book](https://www.cs.ubc.ca/~murphyk/MLbook/).

### Application in ML

#### Maximum Likelihood Parameter Estimation
Assume that we have samples composing our training set, $\Strain = \left\\{(x_n, y_n)\right\\}_{n=1}^N$ i.i.d. from some distribution, which we assume is some exponential distribution family. Assume we have picked a model, i.e. that we fixed $h(y)$ and $\phi(y)$, but that $\eta$ is unknown. How can we find an optimal $\eta$?

We said previously that $\phi(y)$ is a sufficient statistic, and that we could find $\eta$ from its empirical average; this is what we'll do here. We can use the maximum likelihood principle to find this parameter, meaning that we want to minimize log-likelihood:

$$
\begin{align}
\argmin{\mathcal{L}_{LL}(\pmb{\eta})} 
    & = -\log{(p(y \mid \pmb{\eta}))} \\
    & = \sum_{n=1}^N{\left(
        -\log{\left[h(y_n) - \pmb{\eta}^T\phi(y_n) + A(\pmb{\eta})\right]}
    \right)}
\end{align}
$$

This is a convex function in $\eta$: the $h(y)$ term does not depend on $\eta$, $\eta^T\phi(y_n)$ is linear, $A(\eta)$ has the [property of being convex](#properties-1).

If we assume that we have the link function already, we can get $\pmb{\eta}$ by setting the gradient of our exponential family to 0:

$$
\frac{1}{N} \nabla\cost{\pmb{\eta}}
    = -\left( \frac{1}{N} \sum_{n=1}^N{\phi(y)} \right)
      + \expect{\phi(y)} 
    = 0
$$

Since $\pmb{\mu} := \expect{\phi(y)}$, we get:

$$
\pmb{\eta} = g^{-1}\left( \frac{1}{N}\sum_{n=1}^N{\phi(y_n)} \right)
$$

#### Generalized linear models
If we assume that our samples follow the distribution of an exponential family, we can construct a *generalized linear model*. As we've explained previously, this is a generalization of the model we used for logistic regression.

For such a model, the maximum likelihood problem, as described above, is easy to solve. As we've noted above, the cost function is convex, so a greedy, iterative algorithm should work well. Let's look at the gradient of the cost:

$$
\nabla_{\pmb{w}}\cost{\pmb{w}} = - \sum_{n=1}^N {\pmb{x}_n \phi(y_n) - \pmb{x}_n A(\pmb{x}_n^T\pmb{w})}
$$

Let's recall that the derivative of the cumulant is:

$$
\frac{\partial A(\eta)}{\partial \eta} = \expect{\phi(y)} = g^{-1}(\eta)
$$

Hence the gradient of the cost function is:

$$
\nabla_{\pmb{w}}\cost{\pmb{w}} = - \sum_{n=1}^N {\pmb{x}_n \phi(y_n) - \pmb{x}_n g^{-1}(\pmb{x}_n^T\pmb{w})}
$$

Setting this to zero gives us the condition of optimality. Using matrix notation, we can rewrite this sum as follows:

$$
\nabla\cost{\pmb{w}} = \pmb{X}^T\left( g^{-1}(\pmb{Xw}) - \phi(\pmb{y}) \right) = 0
$$

Note that this is a more general form of the formula we had [for logistic regression](#conditions-of-optimality).

## Nearest neighbor classifiers and the curse of dimensionality
For simplicity, let's assume that we're operating in a d-dimensional box, that is, in the domain $\chi = [0, 1]^d$. As always, we have a training set $\Strain=\left\\{(x_n, y_n)\right\\}$. 

### K Nearest Neighbor (KNN)
Given a "fresh" input x, we can make a prediction using $\text{nbh}\_{S\_\text{train},\ k}(\pmb{x})$. This is a function returning the $k$ inputs in the training set that are closest to $\pmb{x}$.

For the regression problem, we can take the average of the k nearest neighbors:

$$
f(x) = \frac{1}{k}\sum_{n\in\text{nbh}_{\Strain,\ k}}{y_n}
$$

For binary classification, we take the majority element in the k-neighborhood. It's a good idea to pick k odd so that there is a clear winner.

$$
f(x) = \text{maj}\left\{y_n : n \in \text{nbh}_{\Strain,\ k}(x) \right\}
$$

If we pick a large value of k, then we are smoothing over a large area. Therefore, a large k gives us a simple model, with simpler boundaries, while a small k is a more complex model. In other words, complexity is inversely proportional to k.

If we pick k small we can expect a small bias but huge variance. If we pick a large k we can expect large bias but small variance.


### Analysis
We'll analyze the simplest setting, a binary KNN model (that is, there are only two output labels, 0 and 1). Let's start by simplifying our notation. We'll introduce the following function:

$$
\eta(\pmb{x}) = \mathbb{P}\left\{y=1\mid\pmb{x}\right\}
$$

This is the conditional probability that the label is 1, given that the input is $\pmb{x}$. If this probability is to be meaningful at all, we must have some correlation between the "position" x and the associated label; knowing the labels close by must give us some information. This means that we need an assumption on the distribution $\mathcal{D}$:

$$
\abs{\eta(\pmb{x}) - \eta(\pmb{x}')} \le \mathcal{c}\norm{\pmb{x} - \pmb{x}'}
\label{eq:lipschitz-bound}\tag{Lipschitz bound}
$$

On the right-hand side we have Euclidian distance. In other words, we ask that the conditional probability $\mathbb{P}\left\\{y=1\mid\pmb{x}\right\\}$, denoted by $\eta(x)$, be [Lipschitz continuous](https://en.wikipedia.org/wiki/Lipschitz_continuity) with Lipschitz constant $\mathcal{c}$. We will use this assumption later on to prove a performance bound for our KNN model.

Let's assume for a moment that we know the actual underlying distribution. This is not something that we actually know in practice, but is useful for deriving a formulation for the optimal model. Knowing the distribution probability distribution, our optimum decision rule is given by the classifier:

$$
f_*(\pmb{x}) = \mathbb{I}\left[ \eta(\pmb{x}) > \frac{1}{2} \right]
$$

The idea of this classifier is that with two labels, we'll pick the label that is likely to happen more than half of the time. The intuition is that if we were playing heads or tails and knew the probability in advance, we would always pick the option that has probability more than one half, and that is the best strategy we can use. This is known as the **Bayes classifier**, also called **maximum a posteriori (MAP) classifier**. It is optimal, in that it has the smallest probability of any classifier, namely:

$$
\cost{f_*} = \expectsub{\pmb{x}\sim\mathcal{D}}{
    \min{\left\{ \eta(\pmb{x}), 1-\eta(\pmb{x}) \right\}}
}
$$

Let's compare this to the probability of misclassification of the real model:

$$
\cost{f_{\Strain,\ k=1}} = \expect{\mathbb{I}\left[ f_{\Strain}(\pmb{x}) \ne y \right]}
$$

This tells us that the risk (that is, the error probability of our $k=1$ nearest neighbor classifier) is the above expectation. It's hard to find a closed form for that expectation, but we can place a bound on it by comparing the ideal, theoretical model to the actual model. We'll state the following lemma:

$$
\begin{align}
\cost{f_{\Strain}}
    & \le 2 \cost{f_*} + \mathcal{c} \expectsub{\Strain, \pmb{x}\sim\mathcal{D}}{\norm{\pmb{x} - \text{nbh}_{\Strain, 1}(\pmb{x})}} \\
    
    & \le 2 \cost{f_*} + 4\mathcal{c}\sqrt{d} N^{-\frac{1}{d+1}} \\
\end{align}
$$

Before we see where this comes from, let's just interpret it. The above gives us a bound on the real classifier, compared to the optimal one. The actual classifier is upper bounded by twice the risk of the optimal classifier (this is good), plus a geometric term reflecting dimensionality (it depends on $d$: this will cause us some trouble).

This second term of the sum is the average distance of a randomly chosen point to the nearest point in the training set, times the Lipschitz constant $\mathcal{c}$. It intuitively makes sense to incorporate this factor into our bound: if we are basing our prediction on a point that is very close, we're more likely to be right, and if it's far away, less so. If we're in a box of $[0, 1]^d$, then the distance between two corners would be $\sqrt{d}$ (by Pythagoras' theorem). The term $N^{-\frac{1}{d+1}}$ indicates that the closest data point may be closer than the opposite corner of the cube: if we have more data, we'll probably not have to go that far. However, for large dimensions, we need much more data to have something that'll probably be close.

Let's prove where this geometric term comes from. Let's consider the cube $[0, 1]^d$, the space of inputs containing $\pmb{x}$. If we cut this large cube into small cubes of side length $\epsilon$. Consider the small cube containing $\pmb{x}$. If we are lucky, this small cube also contains a neighboring data point at distance at most $\sqrt{d}\epsilon$ (as per Pythagoras' theorem, as stated above). However, if we're less lucky, the closest neighbor may be at the other corner of the big cube, at distance $\sqrt{d}$. So what is the probability of a point not having a neighbor in its small $\epsilon$ cube?

Let's denote the probability of $\pmb{x}$ landing in a particular box by $\mathbb{P}_i$. The chance that none of the N training points are in the box is $(1-\mathbb{P}_i)^N$. We don't know the distribution $\mathcal{D}$, so we can't really express $\mathbb{P}_i$ in a closed form, but that doesn't matter, this notation allows us to abstract over that. The rest of the proof is calculus, carefully choosing the right scaling for $\epsilon$ in order to get a good bound.

Now, let's understand where the term $2\cost{f_*}$ comes from. If we flip two coins, $y$ and $y'$, what is the probability of the outcome being different?

$$
\mathbb{P}\left\{y \ne y' \right\} = 2p(1-p)
$$

Now, let's consider two points $\pmb{x}$ and $\pmb{x}'$, both elements of $[0, 1]^d$. Their labels are $y$ and $y'$, respectively. The probability of these two labels being different is roughly the same as above (although the probabilities of the two events may not be the same in general):

$$
\begin{align}
\mathbb{P}\left\{ y \ne y'\right\}
    =   & \eta(\pmb{x})(1-\eta{\pmb{x'}}) + \eta(\pmb{x'})(1-\eta{\pmb{x}}) \\
    =   & 2\eta(\pmb{x})(1-\eta(\pmb{x})) + (2\eta(\pmb{x})-1)(\eta(\pmb{x})-\eta(\pmb{x}')) \\
    \le & 2\eta(\pmb{x})(1-\eta(\pmb{x})) + (\eta(\pmb{x}) - \eta(\pmb{x}')) \\
    \le & 2\eta(\pmb{x})(1-\eta(\pmb{x})) + \mathcal{c}\norm{\pmb{x}-\pmb{x}'}
\end{align}
$$

The last step uses $\ref{eq:lipschitz-bound}$.

Therefore, we can confirm the following bound:

$$
\mathcal{P}\left\{ y\ne y' \right\} \le  2\eta(\pmb{x})(1-\eta{\pmb{x}}) + \mathcal{c}\norm{\pmb{x} - \pmb{x}'}
$$

But we are still one step away from explaining how we can compare this to the optimal estimator. In the above, we derived a bound for two labels being different. How is this related to our KNN model? The probability of getting a wrong prediction from KNN with $k=1$ (which we denoted $\expectsub{\Strain}{\cost{f_{\Strain}}}$) is the probability of the predicted label being different from the solution label. 

We get to our lemma by the following reasoning:

$$
2\eta(\pmb{x})(1-\eta{\pmb{x}}) 
    \le 2\min{\left\{ \eta(\pmb{x}), 1-\eta(\pmb{x}) \right\}}
    = 2\cost{f_*}
$$

Additionally, the average of the term $\mathcal{c}\norm{\pmb{x} - \pmb{x}'}$ is $\mathcal{c}\expectsub{\Strain, \pmb{x}\sim\mathcal{D}}{\norm{\pmb{x} - \text{nbh}_{\Strain, 1}(\pmb{x})}}$

If we had assumed that it was a ball instead of a cube, we would've gotten slightly different results. But that's besides the point: the main insight from this is that it depends on the dimension, and that for low dimensions at least, we still have a fairly good classifier. But finding a closest neighbor in high dimension can quickly become meaningless.