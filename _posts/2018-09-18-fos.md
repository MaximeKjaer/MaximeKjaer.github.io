---
title: CS-452 Foundations of Software
description: "My notes from the CS-452 Foundations of Software course given at EPFL, in the 2018 autumn semester (MA1)"
edited: true
note: true
---

* TOC
{:toc}

âš  *Work in progress*

## Writing a parser with parser combinators
In Scala, you can (ab)use the operator overload to create an embedded DSL (EDSL) for grammars. While a grammar may look as follows in a grammar description language (Bison, Yak, ANTLR, ...):

{% highlight antlr linenos %}
Expr ::= Term {'+' Term | 'âˆ’' Term}
Term ::= Factor {'âˆ—' Factor | '/' Factor}
Factor ::= Number | '(' Expr ')'
{% endhighlight %}

In Scala, we can model it as follows:

{% highlight scala linenos %}
def expr: Parser[Any] = term ~ rep("+" ~ term | "âˆ’" ~ term)
def term: Parser[Any] = factor ~ rep("âˆ—" ~ factor | "/" ~ factor)
def factor: Parser[Any] = "(" ~ expr ~ ")" | numericLit
{% endhighlight %}

This is perhaps a little less elegant, but allows us to encode it directly into our language, which is often useful for interop.

The `~`, `|`, `rep` and `opt` are **parser combinators**. These are primitives with which we can construct a full parser for the grammar of our choice.

### Boilerplate

First, let's define a class `ParseResult[T]` as an ad-hoc monad; parsing can either succeed or fail:

{% highlight scala linenos %}
sealed trait ParseResult[T]
case class Success[T](result: T, in: Input) extends ParseResult[T]
case class Failure(msg : String, in: Input) extends ParseResult[Nothing]
{% endhighlight %}

> ðŸ‘‰ `Nothing` is the bottom type in Scala; it contains no members, and nothing can extend it

Let's also define the tokens produced by the lexer (which we won't define) as case classes extending `Token`:

{% highlight scala linenos %}
sealed trait Token
case class Keyword(chars: String) extends Token
case class NumericLit(chars: String) extends Token
case class StringLit(chars: String) extends Token
case class Identifier(chars: String) extends Token
{% endhighlight %}

Input into the parser is then a lazy stream of tokens (with positions for error diagnostics, which we'll omit here):

{% highlight scala linenos %}
type Input = Reader[Token]
{% endhighlight %}

We can then define a standard, sample parser which looks as follows on the type-level:

{% highlight scala linenos %}
class StandardTokenParsers {
    type Parser = Input => ParseResult
}
{% endhighlight %}

### The basic idea
For each language (defined by a grammar symbol `S`), define a function `f` that, given an input stream `i` (with tail `i'`):

- if a prefix of `i` is in `S`, return `Success(Pair(x, i'))`, where `x` is a result for `S`
- otherwise, return `Failure(msg, i)`, where `msg` is an error message string

The first is called *success*, the second is *failure*. We can compose operations on this somewhat conveniently, like we would on a monad (like `Option`).

### Simple parser primitives
All of the above boilerplate allows us to define a parser, which succeeds if the first token in the input satisfies some given predicate `pred`. When it succeeds, it reads the token string, and splits the input there.

{% highlight scala linenos %}
def token(kind: String)(pred: Token => boolean) = new Parser[String] {
    def apply(in : Input) =
        if (pred(in.head)) Success(in.head.chars, in.tail)
        else Failure(kind + " expected ", in)
}
{% endhighlight %}

We can use this to define a keyword parser:

{% highlight scala linenos %}
implicit def keyword(chars: String) = token("'" + chars + "'") {
    case Keyword(chars1) => chars == chars1
    case _ => false
}
{% endhighlight %}

Marking it as `implicit` allows us to write keywords as normal strings, where we can omit the `keyword` call (this helps us simplify the notation in our DSL; we can write `"if"` instead of `keyword("if")`).

We can make other parsers for our other case classes quite simply:

{% highlight scala linenos %}
def numericLit = token("number")( .isInstanceOf[NumericLit])
def stringLit = token("string literal")( .isInstanceOf[StringLit])
def ident = token("identifier")( .isInstanceOf[Identifier])
{% endhighlight %}

### Parser combinators
We are going to define the following parser combinators:

- `~`: sequential composition
- `<~`, `>~`: sequential composition, keeping left / right only
- `|`: alternative
- `opt(X)`: option (like a `?` quantifier in a regex)
- `rep(X)`: repetition (like a `*` quantifier in a regex)
- `repsep(P, Q)`: interleaved repetition
- `^^`: result conversion (like a `map` on an `Option`)
- `^^^`: constant result (like a `map` on an `Option`, but returning a constant value regardless of result)

But first, we'll write some very basic parser combinators: `success` and `failure`, that respectively always succeed and always fail:

{% highlight scala linenos %}
def success[T](result: T) = new Parser[T] {
    def apply(in: Input) = Success(result, in)
}

def failure(msg: String) = new Parser[Nothing] {
    def apply(in: Input) = Failure(msg, in)
}
{% endhighlight %}

All of the above are methods on a `Parser[T]` class. Thanks to infix space notation in Scala, we can denote `x.y(z)` as `x y z`, which allows us to simplify our DSL notation; for instance `A ~ B` corresponds to `A.~(B)`.

{% highlight scala linenos %}
abstract class Parser[T] {
    // An abstract method that defines the parser function
    def apply(in : Input): ParseResult

    def ~[U](rhs: Parser[U]) = new Parser[T ~ U] {
        def apply(in: Input) = Parser.this(in) match {
            case Success(x, tail) => rhs(tail) match {
                case Success(y, rest) => Success(new ~(x, y), rest)
                case failure => failure
            }
            case failure => failure
        }
    }

    def |(rhs: => Parser[T]) = new Parser[T] {
        def apply(in : Input) = Parser.this(in) match {
            case s1 @ Success(_, _) => s1
            case failure => rhs(in)
        }
    }

    def ^^[U](f: T => U) = new Parser[U] {
        def apply(in : Input) = Parser.this(in) match {
            case Success(x, tail) => Success(f(x), tail)
            case x => x
        }
    }

    def ^^^[U](r: U): Parser[U] = ^^(x => r)
}
{% endhighlight %}

> ðŸ‘‰ In Scala, `T ~ U` is syntactic sugar for `~[T, U]`, which is the type of the case class we'll define below

For the `~` combinator, when everything works, we're using `~`, a case class that is equivalent to `Pair`, but prints the way we want to and allows for the concise type-level notation above.

{% highlight scala linenos %}
case class ~[T, U](_1 : T, _2 : U) {
    override def toString = "(" + _1 + " ~ " + _2 +")"
}
{% endhighlight %}

At this point, we thus have **two** different meanings for `~`: a *function* `~` that produces a `Parser`, and the `~(a, b)` *case class* pair that this parser returns (all of this is encoded in the function signature of the `~` function).

Note that the `|` combinator takes the right-hand side parser as a call-by-name argument. This is because we don't want to evaluate it unless it is strictly neededâ€”that is, if the left-hand side fails.

`^^` is like a `map` operation on `Option`; `P ^^ f` succeeds iff `P` succeeds, in which case it applies the transformation `f` on the result of P. Otherwise, it fails.

### Shorthands

We can now define shorthands for common combinations of parser combinators:

{% highlight scala linenos %}
def opt[T](p : Parser[T]): Parser[Option[T]] = p ^^ Some | success(None)

def rep[T](p : Parser[T]): Parser[List[T]] = 
    p ~ rep(p) ^^ { case x ~ xs => x :: xs } | success(Nil)

def repsep[T, U](p : Parser[T], q : Parser[U]): Parser[List[T]] = 
    p ~ rep(q ~> p) ^^ { case r ~ rs => r :: rs } | success(Nil)
{% endhighlight %}

Note that none of the above can fail. They may, however, return `None` or `Nil` wrapped in `success`.


As an exercise, we can implement the `rep1(P)` parser combinator, which corresponds to the `+` regex quantifier:

{% highlight scala linenos %}
def rep1[T](p: Parser[T]) = p ~ rep(p)
{% endhighlight %}

### Example: JSON parser

We did not mention `lexical.delimiters` and `lexical.reserved` in the above, and for the sake of brevity, we omit the implementation of `stringLit` and `numericLit`.

{% highlight scala linenos %}
object JSON extends StandardTokenParsers {
    lexical.delimiters += ("{", "}", "[", "]", ":")
    lexical.reserved += ("null", "true", "false")

    // Return Map
    def obj: Parser[Any] = "{" ~ repsep(member, ",") ~ "}" ^^ (ms => Map() ++ ms)

    // Return List
    def arr: Parser[Any] = "[" ~> repsep(value, ",") <~ "]"

    // Return name/value pair:
    def member: Parser[Any] = stringLit ~ ":" ~ value ^^ {
        case name ~ ":" ~ value => (name, value) 
    }

    // Return correct Scala type
    def value: Parser[Any] =
          obj 
        | arr 
        | stringLit
        | numericLit ^^ (_.toInt)
        | "null" ^^^ null
        | "true" ^^^ true
        | "false" ^^^ false
}
{% endhighlight %}

### The trouble with left-recursion

Parser combinators work top-down and therefore do not allow for left-recursion. For example, the following would go into an infinite loop, where the parser keeps recursively matching the same token unto `expr`:

{% highlight scala linenos %}
def expr = expr ~ "-" ~ term
{% endhighlight %}

Let's take a look at an arithmetic expression parser:

{% highlight scala linenos %}
object Arithmetic extends StandardTokenParsers {
    lexical.delimiters ++= List("(", ")", "+", "âˆ’", "âˆ—", "/")
    def expr: Parser[Any] = term ~ rep("+" ~ term | "âˆ’" ~ term)
    def term: Parser[Any] = factor ~ rep("âˆ—" ~ factor | "/" ~ factor)
    def factor: Parser[Any] = "(" ~ expr ~ ")" | numericLit
}
{% endhighlight %}

This definition of `expr`, namely `term ~ rep("-" ~ term)` produces a right-leaning tree. For instance, `1 - 2 - 3` produces `1 ~ List("-" ~ 2, ~ "-" ~ 3)`. 

The solution is to combine calls to `rep` with a final foldLeft on the list:

{% highlight scala linenos %}
object Arithmetic extends StandardTokenParsers {
    lexical.delimiters ++= List("(", ")", "+", "âˆ’", "âˆ—", "/")
    def expr: Parser[Any] = term ~ rep("+" ~ term | "âˆ’" ~ term) ^^ reduceList
    def term: Parser[Any] = factor ~ rep("âˆ—" ~ factor | "/" ~ factor) ^^ reduceList
    def factor: Parser[Any] = "(" ~ expr ~ ")" | numericLit

    private def reduceList(list: Expr ~ List[String ~ Expr]): Expr = list match {
        case x ~ xs => (x foldLeft ps)(reduce)
    }

    private def reduce(x: Int, r: String ~ Int) = r match {
        case "+" ~ y => x + y
        case "âˆ’" ~ y => x âˆ’ y
        case "âˆ—" ~ y => x âˆ— y
        case "/" ~ y => x / y
        case => throw new MatchError("illegal case: " + r)
    }
}
{% endhighlight %}

> ðŸ‘‰ It used to be that the standard library contained parser combinators, but those are now a [separate module](https://github.com/scala/scala-parser-combinators). This module contains a `chainl` (chain-left) method that reduces after a `rep` for you.

## Arithmetic expressions â€” abstract syntax and proof principles
This section follows Chapter 3 in TAPL.

### Basics of induction
Ordinary induction is simply:

```
Suppose P is a predicate on natural numbers.
Then:
	If P(0)
	and, for all i, P(i) implies P(i + 1)
	then P(n) holds for all n
```

We can also do complete induction:

```
Suppose P is a predicate on natural numbers.
Then:
	If for each natural number n,
	given P(i) for all i < n we can show P(n)
	then P(n) holds for all n
```

It proves exactly the same thing as ordinary induction, it is simply a restated version. They're *interderivable*; assuming one, we can prove the other. Which one to use is simply a matter of style or convenience. We'll see some more equivalent styles as we go along.

### Mathematical representation of syntax
Let's assume the following grammar:

{% highlight antlr linenos %}
t ::= 
    true
    false
    if t then t else t
    0
    succ t
    pred t
    iszero t
{% endhighlight %}

What does this really define? A few suggestions:

- A set of character strings
- A set of token lists
- A set of abstract syntax trees

It depends on how you read it; a grammar like the one above contains information about all three.

However, we are mostly interested in the ASTs. The above grammar is therefore called an **abstract grammar**. Its main purpose is to suggest a mapping from character strings to trees.

For our use of these, we won't be too strict with these. For instance, we'll freely use parentheses to disambiguate what tree we mean to describe, even though they're not strictly supported by the grammar. What matters to us here aren't strict implementation semantics, but rather that we have a framework to talk about ASTs. For our purposes, we'll consider that two terms producing the same AST are basically the same; still, we'll distinguish terms that only have the same evaluation result, as they don't necessarily have the same AST.

How can we express our grammar as mathematical expressions? A grammar describes the legal *set* of terms in a program by offering a recursive definition. While recursive definitions may seem obvious and simple to a programmer, we have to go through a few hoops to make sense of them mathematically.

#### Mathematical representation 1
We can use a set $\mathcal{T}$ of terms. The grammar is then the smallest set such that:

1. $\left\\{ \text{true}, \text{false}, 0 \right\\} \subseteq \mathcal{T}$,
2. If $t_1 \in \mathcal{T}$ then $\left\\{ \text{succ } t_1, \text{pred } t_1, \text{iszero } t_1 \right\\} \subseteq \mathcal{T}$,
3. If $t_1, t_2, t_3 \in \mathcal{T}$ then we also have $\text{if } t_1 \text{ then } t_2 \text{ else } t_3 \in \mathcal{T}$.

#### Mathematical representation 2
We can also write this somewhat more graphically:

$$
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\if}{\text{if }}
\newcommand{\then}{\text{ then }}
\newcommand{\else}{\text{ else }}
\newcommand{\ifelse}{\if t_1 \then t_2 \else t_3}
\newcommand{\defeq}{\overset{\text{def}}{=}}
\newenvironment{rcases}
  {\left.\begin{aligned}}
  {\end{aligned}\right\rbrace}

\text{true } \in \mathcal{T},  \quad
\text{false } \in \mathcal{T}, \quad
0 \in \mathcal{T}              \\ \\

\frac{t_1 \in \mathcal{T}}{\text{succ } t_1 \in \mathcal{T}}, \quad
\frac{t_1 \in \mathcal{T}}{\text{pred } t_1 \in \mathcal{T}}, \quad
\frac{t_1 \in \mathcal{T}}{\text{iszero } t_1 \in \mathcal{T}} \\ \\

\frac{t_1 \in \mathcal{T}, \quad t_2 \in \mathcal{T}, \quad t_3 \in \mathcal{T}}{\ifelse \in \mathcal{T}}
$$

This is exactly equivalent to representation 1, but we have just introduced a different notation. Note that "the smallest set closed under..." is often not stated explicitly, but implied.

#### Mathematical representation 3
Alternatively, we can build up our set of terms as an infinite union:

$$
\begin{align}
\mathcal{S}_0 = & & \emptyset \\
\mathcal{S}_{i+1} = 
    &      & \left\{ \text{true}, \text{ false}, 0 \right\} \\
    & \cup & \left\{ \text{succ } t_1, \text{pred } t_1, \text{iszero } t_1 \mid t_1 \in \mathcal{S}_i \right\} \\
    & \cup & \left\{ \ifelse \mid t_1, t_2, t_3 \in \mathcal{S}_i \right\}
\end{align}
$$

We can thus build our final set as follows:

$$
\mathcal{S} = \bigcup_i{\mathcal{S}_i}
$$

Note that we can "pull out" the definition into a generating function $F$:

$$
\begin{align}
\mathcal{S}_0     & = \emptyset \\
\mathcal{S}_{i+1} & = F(\mathcal{S}_i) \\
\mathcal{S}       & = \bigcup_i{\mathcal{S}_i} \\
\end{align}
$$

The generating function is thus defined as:

$$
\begin{align}
F_1(U) & = \left\{ \text{true} \right\} \\
F_2(U) & = \left\{ \text{false} \right\} \\
F_3(U) & = \left\{ 0 \right\} \\
F_4(U) & = \left\{ \text{succ } t_1 \mid t_1 \in U \right\} \\
F_5(U) & = \left\{ \text{pred } t_1 \mid t_1 \in U \right\} \\
F_6(U) & = \left\{ \text{iszero } t_1 \mid t_1 \in U \right\} \\
F_7(U) & = \left\{ \ifelse \mid t_1, t_2, t_3 \in U \right\} \\
\end{align} \\

F(U) = \bigcup_{i=1}^7{F_i(U)}
$$

Each function takes a set of terms $U$ as input and produces "terms justified by $U$" as output; that is, all terms that have the items of $U$ as subterms.

The set $U$ is said to be **closed under F** or **F-closed** if $F(U) \subseteq U$.

The set of terms $T$ as defined above is the smallest F-closed set. If $O$ is another F-closed set, then $T \subseteq O$.

#### Comparison of the representations
We've seen essentially two ways of defining the set (as representation 1 and 2 are equivalent, but with different notation):

1. The smallest set that is closed under certain rules. This is compact and easy to read.
2. The limit of a series of sets. This gives us an *induction principle* on which we can prove things on terms by induction. 

The first one defines the set "from above", by intersecting F-closed sets.

The second one defines it "from below", by starting with $\emptyset$ and getting closer and closer to being F-closed.

These are equivalent (we won't prove it, but Proposition 3.2.6 in TAPL does so), but can serve different uses in practice.

### Induction on terms
First, let's define depth: the **depth** of a term $t$ is the smallest $i$ such that $t\in\mathcal{S_i}$.

The way we defined $\mathcal{S}_i$, it gets larger and larger for increasing $i$; the depth of a term $t$ gives us the step at which $t$ is introduced into the set.

We see that if a term $t$ is in $$\mathcal{S}_i$$, then all of its immediate subterms must be in $\mathcal{S}_{i-1}$, meaning that they must have smaller depth.

This justifies the principle of **induction on terms**, or **structural induction**. Let P be a predicate on a term:

```
If, for each term s,
    given P(r) for all immediate subterms r of s we can show P(s)
    then P(t) holds for all t
```

All this says is that if we can prove the induction step from subterms to terms (under the induction hypothesis), then we have proven the induction.

We can also express this structural induction using generating functions, which we [introduced previously](#mathematical-representation-3).

```
Suppose T is the smallest F-closed set.
If, for each set U,
    from the assumption "P(u) holds for every u âˆˆ U",
    we can show that "P(v) holds for every v âˆˆ F(U)"
then
    P(t) holds for all t âˆˆ T
```

Why can we use this?

- We assumed that $T$ was the smallest F-closed set, which means that $T\subseteq O$ for any other F-closed set $O$.
- Showing the pre-condition ("for each set $U$, from the assumption...") amounts to showing that the set of all terms satisfying $P$ (call it $O$) is itself an F-closed set. 
- Since $T\subseteq O$, every element of $T$ satisfies $P$.

### Inductive function definitions
An [inductive definition](https://en.wikipedia.org/wiki/Recursive_definition) is used to define the elements in a set recursively, as we have done above. The [recursion theorem](https://en.wikipedia.org/wiki/Recursion#The_recursion_theorem) states that a well-formed inductive definition defines a function. To understand what being well-formed means, let's take a look at some examples.

Let's define our grammar function a little more formally. Constants are the basic values that can't be expanded further; in our example, they are `true`, `false`, `0`. As such, the set of constants appearing in a term $t$, written $\text{Consts}(t)$, is defined recursively as follows:

$$
\begin{align}
\text{Consts}(\text{true})  & = \left\{ \text{true}  \right\} \\
\text{Consts}(\text{false}) & = \left\{ \text{false} \right\} \\
\text{Consts}(0)            & = \left\{ 0            \right\} \\

\text{Consts}(\text{succ } t_1) & = \text{Consts}(t_1) \\
\text{Consts}(\text{pred } t_1) & = \text{Consts}(t_1) \\
\text{Consts}(\text{iszero } t_1) & = \text{Consts}(t_1) \\
\text{Consts}(\ifelse & = \text{Consts}(t_1) \cup \text{Consts}(t_2) \cup \text{Consts}(t_3) \\
\end{align}
$$

This seems simple, but these semantics aren't perfect. First off, a mathematical definition simply assigns a convenient name to some previously known thing. But here, we're defining the thing in terms of itself, recursively. And the semantics above also allow us to define ill-formed inductive definitions:

$$
\begin{align}
\text{BadConsts}(\text{true})  & = \left\{ \text{true}  \right\} \\
\text{BadConsts}(\text{false}) & = \left\{ \text{false} \right\} \\
\text{BadConsts}(0)            & = \left\{ 0            \right\} \\
\text{BadConsts}(0)            & = \left\{ \right\} = \emptyset  \\

\text{BadConsts}(\text{succ } t_1) & = \text{BadConsts}(t_1) \\
\text{BadConsts}(\text{pred } t_1) & = \text{BadConsts}(t_1) \\
\text{BadConsts}(\text{iszero } t_1) & = \text{BadConsts}(\text{iszero iszero }t_1) \\
\end{align}
$$

The last rule produces infinitely large rules (if we implemented it, we'd expect some kind of stack overflow). We're missing the rules for if-statements, and we have a useless rule for `0`, producing empty sets.

How do we tell the difference between a well-formed inductive definition, and an ill-formed one as above? What is well-formedness anyway? 

#### What is a function?

A relation over $T, U$ is a subset of $T \times U$, where the Cartesian product is defined as:

$$
T\times U = \left\{ (t, u) : t\in T, u\in U \right\}
$$

A function $f$ from $A$ (domain) to $B$ (co-domain) can be viewed as a two-place relation, albeit with two additional properties:

- It is **total**: $\forall a \in A, \exists b \in B : (a, b) \in f$
- It is **deterministic**: $(a, b_1) \in f, (a, b_2) \in f \implies b_1 = b_2$

Totality ensures that the A domain is covered, while being deterministic just means that the function always produces the same result for a given input.

#### Induction example 1
As previously stated, $\text{Consts}$ is a *relation*. It maps terms (A) into the set of constants that they contain (B). The induction theorem states that it is also a *function*. The proof is as follows.

$\text{Consts}$ is total and deterministic: for each term $t$ there is exactly one set of terms $C$ such that $(t, C) \in \text{Consts}$[^in-relation-notation] . The proof is done by induction on $t$.

[^in-relation-notation]: $(t, C) \in \text{Consts}$ is equivalent to $\text{Consts}(t) = C$

To be able to apply the induction principle for terms, we must first show that for an arbitrary term $t$, under the following induction hypothesis:

> For each immediate subterm $s$ of $t$, there is exactly one set of terms $C_s$ such that $(s, C_s) \in \text{Consts}$

Then the following needs to be proven as an induction step:

> There is **exactly one** set of terms $C$ such that $(t, C) \in \text{Consts}$

We proceed by cases on $t$:

- If $t$ is $0$, $\text{true}$ or $\text{false}$
  
  We can immediately see from the definition that of $\text{Consts}$ that there is exactly one set of terms $C = \left\\{ t \right\\}$) such that $(t, C) \in \text{Consts}$.

  This constitutes our base case.
  
- If $t$ is $\text{succ } t_1$, $\text{pred } t_1$ or $\text{iszero } t_1$
  
  The immediate subterm of $t$ is $t_1$, and the induction hypothesis tells us that there is exactly one set of terms $C_1$ such that $(t_1, C_1) \in \text{Consts}$. But then it is clear from the definition that there is exactly one set of terms $C = C_1$ such that $(t, C) \in \text{Consts}$.
  
- If $t$ is $\ifelse$
  
  The induction hypothesis tells us:

    - There is exactly one set of terms $C_1$ such that $(t_1, C_1) \in \text{Consts}$
    - There is exactly one set of terms $C_2$ such that $(t_2, C_2) \in \text{Consts}$
    - There is exactly one set of terms $C_3$ such that $(t_3, C_3) \in \text{Consts}$
  
  It is clear from the definition of $\text{Consts}$ that there is exactly one set $C = C_1 \cup C_2 \cup C_3$ such that $(t, C) \in \text{Consts}$.

This proves that $\text{Consts}$ is indeed a function.

But what about $\text{BadConsts}$? It is also a relation, but it isn't a function. For instance, we have $\text{BadConsts}(0) = \left\\{ 0 \right\\}$ and $\text{BadConsts}(0) = \left\\{ \right\\}$, which violates determinism. To reformulate this in terms of the above, there are two sets $C$ such that $(0, C) \in \text{BadConsts}$, namely $C = \left\\{ 0 \right\\}$ and $C = \left\\{ \right\\}$.

Note that there are many other problems with $\text{BadConsts}$, but this is sufficient to prove that it isn't a function.

#### Induction example 2
Let's introduce another inductive definition:

$$
\begin{align}
\text{size}(\text{true})  & = 1 \\
\text{size}(\text{false}) & = 1 \\
\text{size}(0)            & = 1 \\
\text{size}(\text{succ}\ t_1)   & = \text{size}(t_1) + 1 \\
\text{size}(\text{pred}\ t_1)   & = \text{size}(t_1) + 1 \\
\text{size}(\text{iszero}\ t_1) & = \text{size}(t_1) + 1 \\
\text{size}(\ifelse) & = \text{size}(t_1) + \text{size}(t_2) + \text{size}(t_3)\\
\end{align}
$$

We'd like to prove that the number of distinct constants in a term is at most the size of the term. In other words, that $\abs{\text{Consts}(t)} \le \text{size}(t)$

The proof is by induction on $t$:

- $t$ is a constant; $t=\text{true}$, $t=\text{false}$ or $t=0$
  
  The proof is immediate. For constants, the number of constants and the size are both one: $\abs{\text{Consts(t)}} = \abs{\left\\{t\right\\}} = 1 = \text{size}(t)$

- $t$ is a function; $t = \text{succ}\ t_1$, $t = \text{pred}\ t_1$ or $t = \text{iszero}\ t_1$
  
  By the induction hypothesis, $\abs{\text{Consts}(t1)} \le \text{size}(t_1)$.

  We can then prove the proposition as follows: $\abs{\text{Consts}(t)} = \abs{\text{Consts}(t_1)} \overset{\text{IH}}{\le} \text{size}(t_1) = \text{size}(t) + 1 < \text{size}(t)$

- $t$ is an if-statement: $t = \ifelse$
  
  By the induction hypothesis, $\abs{\text{Consts}(t_1)} \le \text{size}(t_1)$, $\abs{\text{Consts}(t_2)} \le \text{size}(t_2)$ and $\abs{\text{Consts}(t_3)} \le \text{size}(t_3)$.

  We can then prove the proposition as follows: 

$$
\begin{align}
\abs{\text{Consts}}
    & = \abs{\text{Consts}(t_1)\cup\text{Consts}(t_2)\cup\text{Consts}(t_3)} \\
    & \le \abs{\text{Consts}(t_1)}+\abs{\text{Consts}(t_2)}+\abs{\text{Consts}(t_3)} \\
    & \overset{\text{IH}}{\le} \text{size}(t_1) + \text{size}(t_2) + \text{size}(t_3) \\
    & < \text{size}(t)
\end{align}
$$


### Operational semantics and reasoning

#### Evaluation
Suppose we have the following syntax

{% highlight antlr linenos %}
t ::=                  // terms
    true                   // constant true
    false                  // constant false 
    if t then t else t     // conditional
{% endhighlight %}

The evaluation relation $t \longrightarrow t'$ is the smallest relation closed under the following rules.

The following are *computation rules*, defining the "real" computation steps:

$$
\begin{align}
\text{if true then } t_2 \else t_3 \longrightarrow t_2 
\tag{E-IfTrue}
\label{eq:e-iftrue} \\

\text{if false then } t_2 \else t_3 \longrightarrow t_3 
\tag{E-IfFalse}
\label{eq:e-iffalse} \\
\end{align}
$$

The following is a *congruence rule*, defining where the computation rule is applied next:

$$
\frac{t_1 \longrightarrow t_1'}
     {\ifelse \longrightarrow \if t_1' \then t_2 \else t_3} 
\tag{E-If}
\label{eq:e-if}
$$

We want to evaluate the condition before the conditional clauses in order to save on evaluation; we're not sure which one should be evaluated, so we need to know the condition first.

#### Derivations
We can describe the evaluation logically from the above rules using derivation trees. Suppose we want to evaluate the following (with parentheses added for clarity): `if (if true then true else false) then false else true`.

In an attempt to make all this fit onto the screen, `true` and `false` have been abbreviated `T` and `F` in the derivation below, and the `then` keyword has been replaced with a parenthesis notation for the condition.

$$
\frac{
    \frac{
        \if (T)\ T \else F
        \longrightarrow
        T
        \quad (\ref{eq:e-iftrue})
    }{
        \if (\if (T)\ T \else F) \ F \else T
        \longrightarrow
        \if (T) \ F \else T
        \quad (\ref{eq:e-if})
    }

    \qquad 

    \small{
        \if (T) \ F \else T
        \longrightarrow
        F
        \quad (\ref{eq:e-iftrue})
    }
}{
    \if (\if (T) \ T \else F) \ F \else T
    \longrightarrow
    T
}
$$

The final statement is a **conclusion**. We say that the derivation is a **witness** for its conclusion (or a **proof** for its conclusion). The derivation records all reasoning steps that lead us to the conclusion.

#### Inversion lemma
We can introduce the **inversion lemma**, which tells us how we got to a term. 

Suppose we are given a derivation $\mathcal{D}$ witnessing the pair $(t, t')$ in the evaluation relation. Then either:

1. If the final rule applied in $\mathcal{D}$ was $(\ref{eq:e-iftrue})$, then we have $\if true \then t_2 \else t_3$ and $t'=t_2$ for some $t_2$ and $t_3$
2. If the final rule applied in $\mathcal{D}$ was $(\ref{eq:e-iffalse})$, then we have $\if false \then t_2 \else t_3$ and $t'=t_2$ for some $t_2$ and $t_3$
3. If the final rule applied in $\mathcal{D}$ was $(\ref{eq:e-if})$, then we have $t = \if t_1 \then t_2 \else t_3$ and $t' = t = \if t_1' \then t_2 \else t_3$, for some $t_1, t_1', t_2, t_3$. Moreover, the immediate subderivation of $\mathcal{D}$ witnesses $(t_1, t_1') \in \longrightarrow$. 

This is super boring, but we do need to acknowledge the inversion lemma before we can do induction proofs on derivations. Thanks to the inversion lemma, given an arbitrary derivation $\mathcal{D}$ with conclusion $t \longrightarrow t'$, we can proceed with a case-by-case analysis on the final rule used in the derivation tree.

Let's recall our [definition of the size function](#induction-example-2). In particular, we'll need the rule for if-statements:

$$
\text{size}(\ifelse) = \text{size}(t_1) + \text{size}(t_2) + \text{size}(t_3)
$$

We want to prove that if $t \longrightarrow t'$, then $\text{size}(t) > \text{size}(t')$.

1. If the final rule applied in $\mathcal{D}$ was $(\ref{eq:e-iftrue})$, then we have $t = \if true \then t_2 \else t_3$ and $t'=t_2$, and the result is immediate from the definition of $\text{size}$
2. If the final rule applied in $\mathcal{D}$ was $(\ref{eq:e-iffalse})$, then we have $t = \if false \then t_2 \else t_3$ and $t'=t_2$, and the result is immediate from the definition of $\text{size}$
3. If the final rule applied in $\mathcal{D}$ was $(\ref{eq:e-if})$, then we have $t = \ifelse$ and $t' = \if t_1' \then t_2 \else t_3$. In this case, $t_1 \longrightarrow t_1'$ is witnessed by a derivation $\mathcal{D}_1$. By the induction hypothesis, $\text{size}(t_1) > \text{size}(t_1')$, and the result is then immediate from the definition of $\text{size}$

### Abstract machines
An abstract machine consists of:

- A set of **states**
- A **transition** relation of states, written $\longrightarrow$

$t \longrightarrow t'$ means that $t$ evaluates to $t'$ in one step. Note that $\longrightarrow$ is a relation, and that $t \longrightarrow t'$ is shorthand for $(t, t') \in \longrightarrow$. Often, this relation is a partial function (not necessarily covering the domain A; there is at most one possible next state). But without loss of generality, there may be many possible next states, determinism isn't a criterion here.

### Normal forms
A normal form is a term that cannot be evaluated any further. More formally, a term $t$ is a normal form if there is no $t'$ such that $t \longrightarrow t'$.  A normal form is a state where the abstract machine is halted; we can regard it as the result of a computation. 

#### Values that are normal form
Previously, we intended for our values (true and false) to be exactly that, the result of a computation. Did we get that right? 

Let's prove that a term $t$ is a value $\iff$ it is in normal form.

- The $\implies$ direction is immediate from the definition of the evaluation relation $\longrightarrow$.
- The $\impliedby$ direction is more conveniently proven as its contrapositive: if $t$ is not a value, then it is not a normal form, which we can prove by induction on the term $t$.

  Since $t$ is not a value, it must be of the form $\ifelse$. If $t_1$ is directly `true` or `false`, then $\ref{eq:e-iftrue}$ or $\ref{eq:e-iffalse}$ apply, and we are done.

  Otherwise, if $t = \ifelse$ where $t_1$ isn't a value, by the induction hypothesis, there is a $t_1'$ such that $t_1 \longrightarrow t_1'$. Then rule $\ref{eq:e-if}$ yields $\if t_1' \then t_2 \else t_3$, which proves that $t$ is not in normal form.


#### Values that are not normal form
Let's introduce new syntactic forms, with new evaluation rules.

{% highlight antlr linenos %}
t ::=        // terms
    0            // constant 0
    succ t       // successor
    pred t       // predecessor 
    iszero t     // zero test

v ::=  nv     // values

nv ::=        // numeric values
    0             // zero value
    succ nv       // successor value
{% endhighlight %}

The evaluation rules are given as follows:

$$
\begin{align}
& \frac{t_1 \longrightarrow t_1'}{\text{succ } t_1 \longrightarrow \text{succ } t_1'} 
\tag{E-Succ} \label{eq:e-succ}
\\ \\
& \text{pred } 0 \longrightarrow 0
\tag{E-PredZero} \label{eq:e-predzero} 
\\ \\
& \text{pred succ } nv_1 \longrightarrow nv_1
\tag{E-PredSucc} \label{eq:e-predsucc}
\\ \\
& \frac{t_1 \longrightarrow t_1'}{\text{pred } t_1 \longrightarrow \text{pred } t_1'}
\tag{E-Pred} \label{eq:e-pred}
\\ \\
& \text{iszero } 0 \longrightarrow true
\tag{E-IszeroZero} \label{eq:e-iszerozero}
\\ \\
& \text{iszero succ } nv_1 \longrightarrow false
\tag{E-IszeroSucc} \label{eq:e-iszerosucc}
\\ \\
& \frac{t_1 \longrightarrow t_1'}{\text{iszero } t_1 \longrightarrow \text{iszero } t_1'}
\tag{E-Iszero} \label{eq:e-iszero} \\
\end{align}
$$

All values are still normal forms. But are all normal forms values? Not in this case. For instance, `succ true`, `iszero true`, etc, are normal forms. These are **stuck terms**: they are in normal form, but are not values. In general, these correspond to some kind of type error, and one of the main purposes of a type system is to rule these kinds of situations out.

### Multi-step evaluation
Let's introduce the *multi-step evaluation* relation, $\longrightarrow^*$. It is the reflexive, transitive closure of single-step evaluation, i.e. the smallest relation closed under these rules:

$$
\begin{align}
\frac{t\longrightarrow t'}{t \longrightarrow^* t'} \\ \\
t \longrightarrow^* t \\ \\
\frac{t \longrightarrow^* t' \qquad t' \longrightarrow^* t''}{t \longrightarrow^* t''}
\end{align}
$$

In other words, it corresponds to any number of single consecutive evaluations.

### Termination of evaluation
We'll prove that evaluation terminates, i.e. that for every term $t$ there is some normal form $t'$ such that $t\longrightarrow^* t'$.

First, let's [recall our proof](#induction-example-2) that $t\longrightarrow t' \implies \text{size}(t) > \text{size}(t')$. Now, for our proof by contradiction, assume that we have an infinite-length sequence $t_0, t_1, t_2, \dots$ such that:

$$
t_0 \longrightarrow t_1 \longrightarrow t_2 \longrightarrow \dots
\quad \implies \quad 
\text{size}(t_0) > \text{size}(t_1) > \text{size}(t_2) > \dots
$$

But this sequence cannot exist: since $\text{size}(t_0)$ is a finite, natural number, we cannot construct this infinite descending chain from it. This is a contradiction.

Most termination proofs have the same basic form. We want to prove that the relation $R\subseteq X \times X$ is terminating &mdash; that is, there are no infinite sequences $x_0, x_1, x_2, \dots$ such that $(x_i, x_{i+1}) \in R$ for each $i$. We proceed as follows:

1. Choose a well-suited set $W$ with partial order $<$ such that there are no infinite descending chains $w_0 > w_1 > w_2 > \dots$ in $W$. Also choose a function $f: X \rightarrow W$.
2. Show $f(x) > f(y) \quad \forall (x, y) \in R$
3. Conclude that are no infinite sequences $(x_0, x_1, x_2, \dots)$ such that $(x_i, x_{i+1}) \in R$ for each $i$. If there were, we could construct an infinite descending chain in $W$.

As a side-note, **partial order** is defined as the following properties:

1. **Anti-symmetry**: $\neg(x < y \land y < x)$
2. **Transitivity**: $x<y \land y<z \implies x < z$

We can add a third property to achieve **total order**, namely $x \ne y \implies x <y \lor y<x$.

## Lambda calculus
Lambda calculus is Turing complete, and is higher-order (functions are data). In lambda calculus, all computation happens by means of function abstraction and application.

Lambda calculus is isomorphic to Turing machines. 

Suppose we wanted to write a function `plus3` in our previous language:

{% highlight linenos %}
plus3 x = succ succ succ x
{% endhighlight %}

The way we write this in lambda calculus is:

$$
\text{plus3 } = \lambda x. \text{ succ}(\text{succ}(\text{succ}(x)))
$$

$\lambda x. t$ is written `x => t` in Scala, or `fun x -> t` in OCaml. Application of our function, say `plus3(succ 0)`, can be written as:

$$
(\lambda x. \text{succ succ succ } x)(\text{succ } 0)
$$

Abstraction over functions is possible using higher-order functions, which we call $\lambda$-abstractions. An example of such an abstraction is the function $g$ below, which takes an argument $f$ and uses it in the function position. 

$$
g = \lambda f. f(f(\text{succ } 0))
$$

If we apply $g$ to an argument like $\text{plus3}$, we can just use the substitution rule to see how that defines a new function.

Another example: the double function below takes two arguments, as a curried function would. First, it takes the function to apply twice, then the argument on which to apply it, and then returns $f(f(y))$.

$$
\text{double} = \lambda f. \lambda y. f(f(y))
$$

### Pure lambda calculus
Once we have $\lambda$-abstractions, we can actually throw out all other language primitives like booleans and other values; all of these can be expressed as functions, as we'll see below. In pure lambda-calculus, *everything* is a function.

Variables will always denote a function, functions always take other functions as parameters, and the result of an evaluation is always a function.

The syntax of lambda-calculus is very simple:

{% highlight antlr linenos %}
t ::=      // terms, also called Î»-terms
    x         // variable
    Î»x. t     // abstraction, also called Î»-abstractions
    t t       // application
{% endhighlight %}

A few rules and syntactic conventions:

- Application associates to the left, so $t\ u\ v$ means $(t\ u)\ v$, not $t\ (u\ v)$.
- Bodies of lambda abstractions extend as far to the right as possible, so $\lambda x. \lambda y.\ x\ y$ means $\lambda x.\ (\lambda y. x\ y)$, not $\lambda x.\ (\lambda y.\ x)\ y$

#### Scope
The lambda expression $\lambda x.\ t$ **binds** the variable $x$, with a **scope** limited to $t$. Occurrences of $x$ inside of $t$ are said to be *bound*, while occurrences outside are said to be *free*. 

#### Operational semantics
As we saw with our previous language, the rules could be distinguished into *computation* and *congruence* rules. For lambda calculus, the only computation rule is:

$$
(\lambda x. t_{12})\ v_2 \longrightarrow \left[ x \mapsto v_2 \right] t_{12}
\tag{E-AppAbs}\label{eq:e-appabs}
$$

The notation $\left[ x \mapsto v_2 \right] t_{12}$ means "the term that results from substituting free occurrences of $x$ in $t_{12}$ with $v_2$".

The congruence rules are:

$$
\begin{align}
& \frac{t_1 \longrightarrow t_1'}{t_1\ t_2 \longrightarrow t_1'\ t_2} \tag{E-App1}\label{eq:e-app1} \\ \\
& \frac{t_2 \longrightarrow t_2'}{t_1\ t_2 \longrightarrow t_1\ t_2'} \tag{E-App2}\label{eq:e-app2} \\
\end{align}
$$

A lambda-expression applied to a value, $(\lambda x.\ t)\ v$, is called a **reducible expression**, or **redex**.

#### Evaluation strategies
There are alternative evaluation strategies. In the above, we have chosen call by value (which is the standard in most mainstream languages), but we could also choose:

- **Full beta-reduction**: any redex may be reduced at any time. This offers no restrictions, but in practice, we go with a set of restrictions like the ones below (because coding a fixed way is easier than coding probabilistic behavior).
- **Normal order**: the leftmost, outermost redex is always reduced first. This strategy allows to reduce inside unapplied lambda terms
- **Call-by-name**: allows no reductions inside lambda abstractions. Arguments are not reduced before being substituted in the body of lambda terms when applied. Haskell uses an optimized version of this, call-by-need (aka lazy evaluation).

### Classical lambda calculus
Classical lambda calculus allows for full beta reduction. 

#### Confluence in full beta reduction
The congruence rules allow us to apply in different ways; we can choose between $\ref{eq:e-app1}$ and $\ref{eq:e-app2}$ every time we reduce an application, and this offers many possible reduction paths. 

While the path is non-deterministic, is the result also non-deterministic? This question took a very long time to answer, but after 25 years or so, it was proven that the result is always the same. This is known the **Church-Rosser confluence theorem**:

Let $t, t_1, t_2$ be terms such that $t \longrightarrow^* t_1$ and $t \longrightarrow^* t_2$. Then there exists a term $t_3$ such that $t_1 \longrightarrow^* t_3$ and $t_2 \longrightarrow^* t_3$

#### Alpha conversion
Substitution is actually trickier than it looks! For instance, in the  expression $\lambda x.\ (\lambda y.\ x)\ y$, the first occurrence of $y$ is bound (it refers to a parameter), while the second is free (it does not refer to a parameter). This is comparable to scope in most programming languages, where we should understand that these are two different variables in different scopes, $y_1$ and $y_2$.

The above example had a variable that is both bound and free, which is something that we'll try to avoid. This is called a hygiene condition.

We can transform a unhygienic expression to a hygienic one by renaming bound variables before performing the substitution. This is known as **alpha conversion**.

Let $\text{fv}(t)$ be the set of free variables in a term $t$. It's defined as follows:

$$
\begin{align}
\text{fv}(x) & = \left\{ x\right\} \\
\text{fv}(\lambda x.\ t_1) & = \text{fv}(t_1) \setminus \text{fv}(t_2) \\ 
\text{fv}(t_1 \ t_2) & = \text{fv}(t_1)\cup\text{fv}(t_2) \\
\end{align}
$$

Alpha conversion is given by the following conversion rule:

$$
\frac{y \notin \text{fv}(t)}{(\lambda x.\ t) =_\alpha (\lambda y.\ \left[ x\mapsto y\right]\ t)}
\tag{$\alpha$}
\label{eq:alpha-conv}
$$

And these equivalence rules (in mathematics, equivalence is defined as symmetry and transitivity):

$$
\begin{align}
& \frac{t_1 =_\alpha t_2}{t_2 =_\alpha t_1} 
\tag{$\alpha \text{-Symm}$}
\label{eq:alpha-sym}
\\ \\

& \frac{t_1 =_\alpha t_2 \quad t_2 =_\alpha t_3}{t_1 =_\alpha t_3}
\tag{$\alpha \text{-Trans}$}
\label{eq:alpha-trans}
\\
\end{align}
$$

The congruence rules are as usual.

### Programming in lambda-calculus

#### Multiple arguments
The way to handle multiple arguments is by currying: $\lambda x.\ \lambda y.\ t$

#### Booleans
The fundamental, universal operator on booleans is if-then-else, which is what we'll replicate to model booleans. We'll denote our booleans as $\text{tru}$ and $\text{fls}$ to be able to distinguish these pure lambda-calculus abstractions from the true and false values of our previous toy language.

We want `true` to be equivalent to `if (true)`, and `false` to `if (false)`. The terms $\text{tru}$ and $\text{fls}$ *represent* boolean values, in that we can use them to test the truth of a boolean value:

$$
\begin{align}
\text{tru } & = \lambda t.\ \lambda f.\ t \\
\text{fls } & = \lambda t.\ \lambda f.\ f \\
\end{align}
$$

We can consider these as booleans. Equivalently `tru` can be considered as a function performing `(t1, t2) => if (true) t1 else t2`. To understand this, let's try to apply $\text{tru}$ to two arguments:

$$
\begin{align}
&   && \text{tru } v\ w \\
& = && (\lambda t.\ (\lambda f.\  t))\ v\ w \\
& \longrightarrow && (\lambda f.\ v)\ w \\
& \longrightarrow && v \\
\end{align}
$$

This works equivalently for `fls`. 

We can also do inversion, conjunction and disjunction with lambda calculus, which can be read as particular if-else statements:

$$
\begin{align}
\text{not } & = \lambda b.\ b\ \text{fls}\ \text{true} \\
\text{and } & = \lambda b.\ \lambda c.\ b\ c\ \text{fls} \\
\text{or }  & = \lambda b.\ \lambda c.\ b\ \text{tru}\ c \\
\end{align}
$$


- `not` is a function that is equivalent to `not(b) = if (b) false else true`. 
- `and` is equivalent to `and(b, c) = if (b) c else false`
- `or` is equivalent to `or(b, c) = if (b) true else c`

#### Pairs
The fundamental operations are construction `pair(a, b)`, and selection `pair._1` and `pair._2`.

$$
\begin{align}
\text{pair } & = \lambda f.\ \lambda s.\ \lambda b.\ b\ f\ s\\
\text{fst }  & = \lambda p.\ p\ \text{tru} \\
\text{snd }  & = \lambda p.\ p\ \text{fls} \\
\end{align}
$$

- `pair` is equivalent to `pair(f, s) = (b => b f s)`
- When `tru` is applied to `pair`, it selects the first element, by definition of the boolean, and that is therefore the definition of `fst`
- Equivalently for `fls` applied to `pair`, it selects the second element

#### Numbers
We've actually been representing numbers as lambda-calculus numbers all along! Our `succ` function represents what's more formally called **Church numerals**.

$$
\begin{align}
c_0 & = \lambda s.\ \lambda z.\ z \\
c_1 & = \lambda s.\ \lambda z.\ s\ z \\
c_2 & = \lambda s.\ \lambda z.\ s\ s\ z \\
c_3 & = \lambda s.\ \lambda z.\ s\ s\ s\ z \\
\end{align}
$$

Note that $c_0$'s implementation is the same as that of $\text{fls}$ (just with renamed variables).

Every number $n$ is represented by a term $c_n$ taking two arguments, which are $s$ and $z$ (for "successor" and "zero"), and applies $s$ to $z$, $n$ times. Fundamentally, a number is equivalent to the following:

$$
c_n = \lambda f.\ \lambda x.\ \underbrace{f\ \dots\ f}_{n \text{ times}}\ x
$$

With this in mind, let us implement some functions on numbers.

$$
\begin{align}
\text{scc } & = \lambda n.\ \lambda s.\ \lambda z.\ s\ (n\ s\ z) \\
\text{add } & = \lambda s.\ \lambda z.\ m\ s\ (n\ s\ z) \\
\text{mul } & = \lambda m.\ \lambda n.\ m\ (\text{add } n)\ c_0 \\
\text{sub } & = \lambda m.\ \lambda n.\ n\ \text{pred}\ m \\
\text{iszero } & = \lambda m.\ m\ (\lambda x.\ \text{fls})\ \text{tru}
\end{align}
$$

- **Successor** $\text{scc}$: we apply the successor function to $n$ (which has been correctly instantiated with $s$ and $z$)
- **Addition** $\text{add}$: we pass the instantiated $n$ as the zero of $m$
- **Subtraction** $\text{sub}$: we apply $\text{pred}$ $n$ times to $m$
- **Multiplication** $\text{mul}$: instead of the successor function, we pass the addition by $n$ function.
- **Zero test** $\text{iszero}$: zero has the same implementation as false, so we can lean on that to build an iszero function

What about predecessor? This is a little harder, and it'll take a few steps to get there. The main idea is that we find the predecessor by rebuilding the whole succession up until our number. At every step, we must generate the number and its predecessor: zero is $(c_0, c_0)$, and all other numbers are $(c_{n-1}, c_n)$. Once we've reconstructed this pair, we can get the predecessor by taking the first element of the pair.

$$
\begin{align}
\text{zz} & = \text{pair } c_0 \  c_0 \\
\text{ss} & = \lambda p.\ \text{pair } (\text{snd } p)\ (\text{scc } (\text{snd } p)) \\
\text{prd} & = \lambda m.\ \text{fst } (m\ \text{ss zz}) \\
\end{align}
$$

{% details Sidenote %}
The story goes that Church was stumped by predecessors for a long time. This solution finally came to him while he was at the barber, and he jumped out half shaven to write it down.
{% enddetails %}

#### Lists
Now what about lists? 

$$
\begin{align}
\text{nil} & = \lambda f.\ \lambda g.\ g \\
\text{cons} & = \lambda x.\ \lambda xs.\ (\lambda f.\ \lambda g.\ f\ x\ xs) \\
\text{head} & = \lambda xs.\ (\lambda y.\ \lambda ys.\  y) \\
\text{isEmpty} & = \lambda xs.\ xs\ (\lambda y.\ \lambda ys.\ \text{fls}) \\
\end{align}
$$

### Recursion in lambda-calculus
Let's start by taking a step back. We talked about normal forms and terms for which we terminate; does lambda calculus always terminate? It's Turing complete, so it must be able to loop infinitely (otherwise, we'd have solved the halting problem!).

The trick to recursion is self-application:

$$
\lambda x.\ x\ x
$$

From a type-level perspective, we would cringe at this. This should not be possible in the typed world, but in the untyped world we can do it. We can construct a simple infinite loop in lambda calculus as follows:

$$
\begin{align}
\text{omega } 
    & = & (\lambda x.\ x\ x)\ (\lambda x.\ x\ x) \\
    & \longrightarrow & \ (\lambda x.\ x\ x)\ (\lambda x.\ x\ x)
\end{align}
$$

The expression evaluates to itself in one step; it never reaches a normal form, it loops infinitely, diverges. This is not a stuck term though; evaluation is always possible.

In fact, there are no stuck terms in pure lambda calculus. Every term is either a value or reduces further.

So it turns out that $\text{omega}$ isn't so terribly useful. Let's try to construct something more practical:

$$
Y_f = (\lambda x.\ f\ (x\ x))\ (\lambda x.\ f\ (x\ x))
$$

Now, the divergence is a little more interesting:

$$
\begin{align}
Y_f & = & (\lambda x.\ f\ (x\ x))\ (\lambda x.\ f\ (x\ x)) \\
& \longrightarrow & f\ ((\lambda x.\ f\ (x\ x))\ (\lambda x.\ f\ (x\ x))) \\
& = & f\ (Y_f) \\
& \longrightarrow & \dots \\
& = & f\ (f\ (Y_f)) \\
\end{align}
$$

This $Y_f$ function is known as a **Y-combinator**. It still loops infinitely (though note that while it works in classical lambda calculus, it blows up in call-by-name), so let's try to build something more useful.

To delay the infinite recursion, we could build something like a poison pill:

$$
\text{poisonpill} = \lambda y.\ \text{omega}
$$

It can be passed around (after all, it's just a value), but evaluating it will cause our program to loop infinitely. This is the core idea we'll use for defining the **fixed-point combinator** $\text{fix}$, which allows us to do recursion. It's defined as follows:

$$
\text{fix} = \lambda f.\ (\lambda x.\ f\ (\lambda y.\ x\ x\ y))\ (\lambda x.\ f\ (\lambda y.\ x\ x\ y))
$$

This looks a little intricate, and we won't need to fully understand the definition. What's important is mostly how it is used to define a recursive function. For instance, if we wanted to define a modulo function in our toy language, we'd do it as follows:

{% highlight scala linenos %}
def mod(x, y) = 
    if (y > x) x
    else mod(x - y, y)
{% endhighlight %}

In lambda calculus, we'd define this as:

$$
\text{mod} = \text{fix } (\lambda f.\ \lambda x.\ \lambda y.\ 
    (\text{gt } y\ x)\ x\ (f (\text{sub } a\ b)\ b)
)
$$

We've assumed that a greater-than $\text{gt}$ function was available here.

More generally, we can define a recursive function as:

$$
\text{fix } \bigl(\lambda f.\ (\textit{recursion on } f)\bigr)
$$

### Equivalence of lambda terms
We've seen how to define Church numerals and successor. How can we prove that $\text{succ } c_n$ is equal to $c_{n+1}$? 

The naive approach unfortunately doesn't work; they do not evaluate to the same value.

$$
\begin{align*}
\text{scc } c_2 
    & = (\lambda n.\ \lambda s.\ \lambda z.\  s\ (n\ s\ z))\ (\lambda s.\ \lambda z.\ s\ (s\ z)) \\
    & \longrightarrow \lambda s.\ \lambda z.\ s\ ((\lambda s.\ \lambda z.\ s\ (s\ z))\ s\ z) \\
    & \neq \lambda s.\ \lambda z.\ s\ (s\ (s\ z)) \\
    & = c_3 \\
\end{align*}
$$

This still seems very close. If we could simplify a little further, we do see how they would be the same.

The intuition behind the Church numeral representation was that a number $n$ is represented as a term that "does something $n$ times to something else". $\text{scc}$ takes a term that "does something $n$ times to something else", and returns a term that "does something $n+1$ times to something else".

What we really care about is that $\text{scc } c_2$ *behaves* the same as $c_3$ when applied to two arguments. We want *behavioral equivalence*. But what does that mean? Roughly, two terms $s$ and $t$ are behaviorally equivalent if there is no "test" that distinguishes $s$ and $t$.

Let's define this notion of "test" this a little more precisely, and specify how we're going to observe the results of a test. We can use the notion of **normalizability** to define a simple notion of a test:

> Two terms $s$ and $t$ are said to be **observationally equivalent** if they are either both normalizable (i.e. they reach a normal form after a finite number of evaluation steps), or both diverge.

In other words, we observe a term's behavior by running it and seeing if it halts. Note that this is not decidable (by the halting problem).

For instance, $\text{omega}$ and $\text{tru}$ are not observationally equivalent (one diverges, one halts), while $\text{tru}$ and $\text{fls}$ are (they both halt).

Observational equivalence isn't strong enough of a test for what we need; we need behavioral equivalence.

> Two terms $s$ and $t$ are said to be **behaviorally equivalent** if, for every finite sequence of values $v_1, v_2, \dots, v_n$ the applications $s\ v_1\ v_2\ \dots\ v_n$ and $t\ v_1\ v_2\ \dots\ v_n$ are observationally equivalent.

This allows us to assert that true and false are indeed different: 

$$
\begin{align}
\text{tru}\ x\ \Omega & \longrightarrow x \\
\text{fls}\ x\ \Omega & \longrightarrow \Omega \\
\end{align}
$$

The former returns a normal form, while the latter diverges.

## Types
As previously, to define a language, we start with a *set of terms* and *values*, as well as an *evaluation relation*. But now, we'll also define a set of **types** (denoted with a first capital letter) classifying values according to their "shape". We can define a *typing relation* $t:\ T$. We must check that the typing relation is *sound* in the sense that:

$$
\frac{t: T \qquad t\longrightarrow^* v}{v: T} 
\qquad\text{and}\qquad
\frac{t: T}{\exists t' \text{ such that } t\longrightarrow t'}
$$

These rules represent some kind of safety and liveness, but are more commonly referred to as [progress and preservation](#properties-of-the-typing-relation), which we'll talk about later. The first one states that types are preserved throughout evaluation, while the second says that if we can type-check, then evaluation of $t$ will not get stuck.

In our previous toy language, we can introduce two types, booleans and numbers:

{% highlight antlr linenos %}
T ::=     // types
    Bool     // type of booleans
    Nat      // type of numbers
{% endhighlight %}

Our typing rules are then given by:

$$
\begin{align}
\text{true } : \text{ Bool} 
\tag{T-True} \label{eq:t-true} \\ \\

\text{false } : \text{ Bool} 
\tag{T-False} \label{eq:t-false} \\ \\

0: \text{ Nat} 
\tag{T-Zero} \label{eq:t-zero} \\ \\

\frac{t_1: \text{Bool} \quad t_2 : T \quad t_3: T}{\ifelse}
\tag{T-If} \label{eq:t-if} \\ \\

\frac{t_1: \text{Nat}}{\text{succ } t_1: \text{Nat}}
\tag{T-Succ} \label{eq:t-succ} \\ \\

\frac{t_1: \text{Nat}}{\text{pred } t_1: \text{Nat}}
\tag{T-Pred} \label{eq:t-pred} \\ \\

\frac{t_1: \text{Nat}}{\text{iszero } t_1: \text{Nat}}
\tag{T-IsZero} \label{eq:t-iszero} \\ \\
\end{align}
$$

With these typing rules in place, we can construct typing derivations to justify every pair $t: T$ (which we can also denote as a $(t, T)$ pair) in the typing relation, as we have done previously with evaluation. Proofs of properties about the typing relation often proceed by induction on these typing derivations.

Like other static program analyses, type systems are generally imprecise. They do not always predict exactly what kind of value will be returned, but simply a conservative approximation. For instance, `if true then 0 else false` cannot be typed with the above rules, even though it will certainly evaluate to a number. We could of course add a typing rule for `if true` statements, but there is still a question of how useful this is, and how much complexity it adds to the type system, and especially for proofs. Indeed, the inversion lemma below becomes much more tedious when we have more rules.

### Properties of the Typing Relation
The safety (or soundness) of this type system can be expressed by the following two properties:

- **Progress**: A well-typed term is not stuck. 
  
  If $t\ :\ T$ then either $t$ is a value, or else $t\longrightarrow t'$ for some $t'$.

- **Preservation**: Types are preserved by one-step evaluation. 
  
  If $t\ :\ T$ and $t\longrightarrow t'$, then $t'\ :\ T$.


We will prove these later, but first we must state a few lemmas.

#### Inversion lemma
Again, for types we need to state the same (boring) inversion lemma:

1. If $\text{true}: R$, then $R = \text{Bool}$.
2. If $\text{false}: R$, then $R = \text{Bool}$.
3. If $\ifelse: R$, then $t_1: \text{ Bool}$, $t_2: R$ and $t_3: R$
4. If $0: R$ then $R = \text{Nat}$
5. If $\text{succ } t_1: R$ then $R = \text{Nat}$ and $t_1: \text{Nat}$
6. If $\text{pred } t_1: R$ then $R = \text{Nat}$ and $t_1: \text{Nat}$
7. If $\text{iszero } t_1: R$ then $R = \text{Bool}$ and $t_1: \text{Nat}$

From the inversion lemma, we can directly derive a typechecking algorithm:

{% highlight scala linenos %}
def typeof(t: Expr): T = t match {
    case True | False => Bool
    case If(t1, t2, t3) =>
        val type1 = typeof(t1)
        val type2 = typeof(t2)
        val type3 = typeof(t3)
        if (type1 == Bool && type2 == type3) type2
        else throw Error("not typable")
    case Zero => Nat
    case Succ(t1) => 
        if (typeof(t1) == Nat) Nat
        else throw Error("not typable")
    case Pred(t1) => 
        if (typeof(t1) == Nat) Nat
        else throw Error("not typable")
    case IsZero(t1) => 
        if (typeof(t1) == Nat) Bool
        else throw Error("not typable")
}
{% endhighlight %}

#### Canonical form
A simple lemma that will be useful for lemma is that of canonical forms. Given a type, it tells us what kind of values we can expect:

1. If $v$ is a value of type Bool, then $v$ is either $\text{true}$ or $\text{false}$
2. If $v$ is a value of type Nat, then $v$ is a numeric value

The proof is somewhat immediate from the syntax of values.

#### Progress Theorem
**Theorem**: suppose that $t$ is a well-typed term of type $T$. Then either $t$ is a value, or else there exists some $t'$ such that $t\longrightarrow t'$.

**Proof**: by induction on a derivation of $t: T$.

- The $\ref{eq:t-true}$, $\ref{eq:t-false}$ and $\ref{eq:t-zero}$ are immediate, since $t$ is a value in these cases.
- For $\ref{eq:t-if}$, we have $t=\ifelse$, with $t_1: \text{Bool}$, $t_2: T$ and $t_3: T$. By the induction hypothesis, there is some $t_1'$ such that $t_1 \longrightarrow t_1'$. 

  If $t_1$ is a value, then rule 1 of the [canonical form lemma](#canonical-form) tells us that $t_1$ must be either $\text{true}$ or $\text{false}$, in which case $\ref{eq:e-iftrue}$ or $\ref{eq:e-iffalse}$ applies to $t$.

  Otherwise, if $t_1 \longrightarrow t_1'$, then by $\ref{eq:e-if}$, $t\longrightarrow \if t_1' \then t_2 \text{ else } t_3$

- For $\ref{eq:t-succ}$, we have $t = \text{succ } t_1$. 
  
  $t_1$ is a value, by rule 5 of the [inversion lemma](#inversion-lemma) and by rule 2 of the [canonical form](#canonical-form), $t_1 = nv$ for some numeric value $nv$. Therefore, $\text{succ }(t_1)$ is a value. If $t_1 \longrightarrow t_1'$, then $t\longrightarrow \text{succ }t_1$.

- The cases for $\ref{eq:t-zero}$, $\ref{eq:t-pred}$ and $\ref{eq:t-iszero}$ are similar.

#### Preservation Theorem
**Theorem**: Types are preserved by one-step evaluation. If $t: T$ and $t\longrightarrow t'$, then $t': T$.

**Proof**: by induction on the given typing derivation

- For $\ref{eq:t-true}$ and $\ref{eq:t-false}$, the precondition doesn't hold (no reduction is possible), so it's trivially true. Indeed, $t$ is already a value, either $t=\text{ true}$ or $t=\text{ false}$.
- For $\ref{eq:t-if}$, there are three evaluation rules by which $t\longrightarrow t'$ can be derived, depending on $t_1$
    + If $t_1 = \text{true}$, then by $\ref{eq:e-iftrue}$ we have $t'=t_2$, and from rule 3 of the [inversion lemma](#inversion-lemma-1) and the assumption that $t: T$, we have $t_2: T$, that is $t': T$
    + If $t_1 = \text{false}$, then by $\ref{eq:e-iffalse}$ we have $t'=t_3$, and from rule 3 of the [inversion lemma](#inversion-lemma-1) and the assumption that $t: T$, we have $t_3: T$, that is $t': T$
    + If $t_1 \longrightarrow t_1'$, then by the induction hypothesis, $t_1': \text{Bool}$. Combining this with the assumption that $t_2: T$ and $t_3: T$, we can apply $\ref{eq:t-if}$ to conclude $\if t_1' \then t_2 \else t_3: T$, that is $t': T$

### Messing with it
#### Removing a rule
What if we remove $\ref{eq:e-predzero}$? Then `pred 0` type checks, but it is stuck and is not a value; the [progress theorem](#progress-theorem) fails.

#### Changing type-checking rule
What if we change the $\ref{eq:t-if}$ to the following?

$$
\frac{
    t_1 : \text{Bool} \quad
    t_2 : \text{Nat} \quad 
    t_3 : \text{Nat}
}{
    (\ifelse) : \text{Nat}
}
\tag{T-If 2}
\label{eq:t-if2}
$$

This doesn't break our type system. It's still sound, but it rejects if-else expressions that return other things than numbers (e.g. booleans). But that is an expressiveness problem, not a soundness problem; our type system disallows things that would otherwise be fine by the evaluation rules.

#### Adding bit
We could add a boolean to natural function `bit(t)`. We'd have to add it to the grammar, add some evaluation and typing rules, and prove progress and preservation.

$$
\begin{align}
\text{bit true} \longrightarrow 0 \\ \\
\text{bit false} \longrightarrow 1 \\ \\

\frac{t_1 \longrightarrow t_1'}{\text{bit }t_1 \longrightarrow \text{bit }t_1'}
\\ \\
\frac{t : \text{Bool}}{\text{bit } t : \text{Nat}}
\end{align}
$$

We'll do something similar this below, so the full proof is omitted.

## Simply typed lambda calculus
Simply Typed Lambda Calculus (STLC) is also denoted $\lambda_\rightarrow$. The "pure" form of STLC is not very interesting on the type-level (unlike for the term-level of pure lambda calculus), so we'll allow base values that are not functions, like booleans and integers. To talk about STLC, we always begin with some set of "base types":

{% highlight antlr linenos %}
T ::=     // types
    Bool    // type of booleans
    T -> T  // type of functions
{% endhighlight %}

In the following examples, we'll work with a mix of our previously defined toy language, and lambda calculus. This will give us a little syntactic sugar.

{% highlight antlr linenos %}
t ::=                // terms
    x                   // variable
    Î»x. t               // abstraction
    t t                 // application
    true                // constant true
    false               // constant false
    if t then t else t  // conditional

v ::=   // values
    Î»x. t  // abstraction value
    true   // true value
    false  // false value
{% endhighlight %}


### Type annotations
We will annotate lambda-abstractions with the expected type of the argument, as follows:

$$
\lambda x: T_1 .\ t_1
$$

We could also omit it, and let type inference do the job (as in OCaml), but for now, we'll do the above. This will make it simpler, as we won't have to discuss inference just yet.

### Typing rules
In STLC, we've introduced abstraction. To add a typing rule for that, we need to encode the concept of an environment $\Gamma$, which is a set of variable assignments. We also introduce the "turnstile" symbol $\vdash$, meaning that the environment can verify the right hand-side typing, or that $\Gamma$ must imply the right-hand side.

$$
\begin{align}

\frac{
    \bigl( \Gamma \cup (x_1 : T_1) \bigr) \vdash  t_2 : T_2
}{ \Gamma\vdash(\lambda x: T_1.\ t_2): T_1 \rightarrow T_2 }
\tag{T-Abs} \label{eq:t-abs} \\ \\

\frac{x: T \in \Gamma}{\Gamma\vdash x: T}
\tag{T-Var} \label{eq:t-var} \\ \\

\frac{
    \Gamma\vdash t_1 : T_{11}\rightarrow T_{12}
    \quad
    \Gamma\vdash t_2 : T_{11}
}{\Gamma\vdash t_1\ t_2 : T_{12}}
\tag{T-App} \label{eq:t-app} 

\end{align}
$$

This additional concept must be taken into account in our definition of progress and preservation:

- **Progress**: If $\Gamma\vdash t : T$, then either $t$ is a value or else $t\longrightarrow t'$ for some $t'$
- **Preservation**: If $\Gamma\vdash t : T$ and $t\longrightarrow t'$, then $\Gamma\vdash t' : T$

To prove these, we must take the same steps as above. We'll introduce the inversion lemma for typing relations, and restate the canonical forms lemma in order to prove the progress theorem.

### Inversion lemma
Let's start with the inversion lemma.

1. If $\Gamma\vdash\text{true} : R$ then $R = \text{Bool}$
2. If $\Gamma\vdash\text{false} : R$ then $R = \text{Bool}$
3. If $\Gamma\vdash\ifelse : R$ then $\Gamma\vdash t_1 : \text{Bool}$ and $\Gamma\vdash t_2, t_3: R$.
4. If $\Gamma\vdash x: R$ then $x: R \in\Gamma$
5. If $\Gamma\vdash\lambda x: T_1 .\ t_2 : R$ then $R = T_1 \rightarrow T_2$ for some $R_2$ with $\Gamma\cup(x: T_1)\vdash t_2: R_2$
6. If $\Gamma\vdash t_1\ t_2 : R$ then there is some type $T_{11}$ such that $\Gamma\vdash t_1 : T_{11} \rightarrow R$ and $\Gamma\vdash t_2 : T_{11}$.

### Canonical form
The canonical forms are given as follows:

1. If $v$ is a value of type Bool, then it is either $\text{true}$ or $\text{false}$
2. If $v$ is a value of type $T_1 \rightarrow T_2$ then $v$ has the form $\lambda x: T_1 .\ t_2$

### Progress
Finally, we get to prove the progress by induction on typing derivations.

**Theorem**: Suppose that $t$ is a closed, well typed term (that is, $\Gamma\vdash t: T$ for some type $T$). Then either $t$ is a value, or there is some $t'$ such that $t\longrightarrow t'$.

- For boolean constants, the proof is immediate as $t$ is a value
- For variables,  the proof is immediate as $t$ is closed, and the precondition therefore doesn't hold
- For abstraction, the proof is immediate as $t$ is a value
- Application is the only case we must treat.
  
  Consider $t = t_1\ t_2$, with $\Gamma\vdash t_1: T_{11} \rightarrow T_{12}$ and $\Gamma\vdash t_2: T_{11}$.

  By the induction hypothesis, $t_1$ is either a value, or it can make a step of evaluation. The same goes for $t_2$.

  If $t_1$ can reduce, then rule $\ref{eq:e-app1}$ applies to $t$. Otherwise, if it is a value, and $t_2$ can take a step, then $\ref{eq:e-app2}$ applies. Otherwise, if they are both values (and we cannot apply $\beta$-reduction), then the canonical forms lemma above tells us that $t_1$ has the form $\lambda x: T_11.\ t_{12}$, and so rule $\ref{eq:e-appabs}$ applies to $t$.


### Preservation
**Theorem**: If $\Gamma\vdash t: T$ and $t \longrightarrow t'$ then $\Gamma\vdash t': T$.

**Proof**: by induction on typing derivations. We proceed on a case-by-case basis, as we have done so many times before. But one case is hard: application.


For $t = t_1\ t_2$, such that $\Gamma\vdash t_1 : T_{11} \rightarrow T_{12}$ and $\Gamma\vdash t_2 : T_{11}$, and where $T=T_{12}$, we want to show $\Gamma\vdash t' : T_{12}$. 

To do this, we must use the [inversion lemma for evaluation](#inversion-lemma) (note that we haven't written it down for STLC, but the idea is the same). There are three subcases for it, starting with the following: 

The left-hand side is $t_1 = \lambda x: T_{11}.\ t_{12}$, and the right-hand side of application $t_2$ is a value $v_2$. In this case, we know that the result of the evaluation is given by $t' = \left[ x\mapsto v_2 \right] t_{12}$.

And here, we already run into trouble, because we do not know about how types act under substitution. We will therefore need to introduce some lemmas.

#### Weakening lemma
Weakening tells us that we can *add* assumptions to the context without losing any true typing statements:

If $\Gamma\vdash t: T$, and the environment $\Gamma$ has no information about $x$&mdash;that is, $x\notin \text{dom}(\Gamma)$&mdash;then the initial assumption still holds if we add information about $x$ to the environment:

$$
\bigl(\Gamma \cup (x: S)\bigr)\vdash t: T
$$

Moreover, the latter $\vdash$ derivation has the same depth as the former.

#### Permutation lemma
Permutation tells us that the order of assumptions in $\Gamma$ does not matter.

If $\Gamma \vdash t: T$ and $\Delta$ is a permutation of $\Gamma$, then $\Delta\vdash t: T$.

Moreover, the latter $\vdash$ derivation has the same depth as the former.

#### Substitution lemma
Substitution tells us that types are preserved under substitution.

That is, if $\Gamma\cup(x: S) \vdash t: T$ and $\Gamma\vdash s: S$, then $\Gamma\vdash \left[x\mapsto s\right] t: T$.

The proof goes by induction on the derivation of $\Gamma\cup(x: S) \vdash t: T$, that is, by cases on the final typing rule used in the derivation.

- Case $\ref{eq:t-app}$: in this case, $t = t_1\ t_2$. 
  
  Thanks to typechecking, we know that the environment validates $\bigl(\Gamma\cup (x: S)\bigr)\vdash t_1: T_2 \rightarrow T_1$ and $\bigl(\Gamma\cup (x: S)\bigr)\vdash t_2: T_2$. In this case, the resulting type of the application is $T=T_1$. 
   
   By the induction hypothesis, $\Gamma\vdash[x\mapsto s]t_1 : T_2 \rightarrow T_1$, and $\Gamma\vdash[x\mapsto s]t_2 : T_2$. 

   By $\ref{eq:t-app}$, the environment then also verifies the application of these two substitutions as $T$: $\Gamma\vdash[x\mapsto s]t_1\ [x\mapsto s]t_2: T$. We can factorize the substitution to obtain the conclusion, i.e. $\Gamma\vdash \left\[x\mapsto s\right\](t_1\ t_2): T$

- Case $\ref{eq:t-var}$: if $t=z$ ($t$ is a simple variable $z$) where $z: T \in \bigl(\Gamma\cup (x: S)\bigr)$. There are two subcases to consider here, depending on whether $z$ is $x$ or another variable:
    + If $z=x$, then $\left[x\mapsto s\right] z = s$. The result is then $\Gamma\vdash s: S$, which is among the assumptions of the lemma
    + If $z\ne x$, then $\left[x\mapsto s\right] z = z$, and the desired result is immediate
- Case $\ref{eq:t-abs}$: if $t=\lambda y: T_2.\ t_1$, with $T=T_2\rightarrow T_1$, and $\bigl(\Gamma\cup (x: S)\cup (y: T_2)\bigr)\vdash t_1 : T_1$.
  
  Based on our [hygiene convention](#alpha-conversion), we may assume $x\ne y$ and $y \notin \text{fv}(s)$. 

  Using [permutation](#permutation-lemma) on the first given subderivation in the lemma ($\Gamma\cup(x: S) \vdash t: T$), we obtain $\bigl(\Gamma\cup (y: T_2)\cup (x: S)\bigr)\vdash t_1 : T_1$ (we have simply changed the order of $x$ and $y$).

  Using [weakening](#weakening-lemma) on the other given derivation in the lemma ($\Gamma\vdash s: S$), we obtain $\bigl(\Gamma\cup (y: T_2)\bigr)\vdash s: S$.

  By the induction hypothesis, $\bigl(\Gamma\cup (y: T_2)\bigr)\vdash\left[x\mapsto s\right] t_1: T_1$.

  By $\ref{eq:t-abs}$, we have $\Gamma\vdash(\lambda y: T_2.\ [x\mapsto s]t_1): T_1$

  By the definition of substitution, this is $\Gamma\vdash([x\mapsto s]\lambda y: T_2.\ t_1): T_2 \rightarrow T_1$.

#### Proof
We've now proven the following lemmas:

- Weakening
- Permutation
- Type preservation under substitution
- Type preservation under reduction (i.e. preservation)

We won't actually do the proof, we've just set up the pieces we need for it.

### Erasure
Type annotations do not play any role in evaluation. In STLC, we don't do any run-time checks, we only run compile-time type checks. Therefore, types can be removed before evaluation. This often happens in practice, where types do not appear in the compiled form of a program; they're typically encoded in an untyped fashion. The semantics of this conversion can be formalized by an erasure function:

$$
\begin{align}
\text{erase}(x) & = x \\
\text{erase}(\lambda x: T_1. t_2) & = \lambda x. \text{erase}(t_2) \\
\text{erase}(t_1\ t_2) & = \text{erase}(t_1)\ \text{erase}(t_2)
\end{align}
$$

### Curry-Howard Correspondence
The Curry-Howard correspondence tells us that there is a correspondence between propositional logic and types.

An implication $P\supset Q$ (which could also be written $P\implies Q$) can be proven by transforming evidence for $P$ into evidence for $Q$. A conjunction $P\land Q$ is a [pair](#pairs-1) of evidence for $P$ and evidence for $Q$. For more examples of these correspondences, see the [Brouwerâ€“Heytingâ€“Kolmogorov (BHK) interpretation](https://en.wikipedia.org/wiki/Brouwerâ€“Heytingâ€“Kolmogorov_interpretation).

| Logic                           | Programming languages                |
| :------------------------------ | :----------------------------------- |
| Propositions                    | Types                                |
| $P \supset Q$                   | Type $P\rightarrow Q$                |
| $P \land Q$                     | [Pair type](#pairs-1) $P\times Q$    |
| $P \lor Q$                      | [Sum type](#sum-type) $P+Q$          |
| $\exists x\in S: \phi(x)$       | Dependent type $\sum{x: S, \phi(x)}$ |
| $\forall x\in S: \phi(x)$       | $\forall (x:S): \phi(x)$             |
| Proof of $P$                    | Term $t$ of type $P$                 |
| $P$ is provable                 | Type $P$ is inhabited                |
| Proof simplification            | Evaluation                           |

In Scala, all types are inhabited except for the bottom type `Nothing`. Singleton types are only inhabited by a single term.

As an example of the equivalence, we'll see that application is equivalent to [modus ponens](https://en.wikipedia.org/wiki/Modus_ponens):

$$
\frac{\Gamma\vdash t_1 : P \supset Q \quad \Gamma\vdash t_2 : P}{\Gamma\vdash t_1\ t_2 : Q}
$$

This also tells us that if we can prove something, we can evaluate it.

How can we prove the following? Remember that $\rightarrow$ is right-associative.

$$
(A \land B) \rightarrow C \rightarrow ((C\land A)\land B)
$$

The proof is actually a somewhat straightforward conversion to lambda calculus:

$$
\lambda p: A\times B.\ \lambda c: C.\ \text{pair} (\text{pair} (c\ \text{fst}(p)) \text{snd}(p))
$$

### Extensions to STLC

#### Base types
Up until now, we've defined our base types (such as $\text{Nat}$ and $\text{Bool}$) manually: we've added them to the syntax of types, with associated constants ($\text{zero}, \text{true}, \text{false}$) and operators ($\text{succ}, \text{pred}$), as well as associated typing and evaluation rules.

This is a lot of minutiae though, especially for theoretical discussions. For those, we can often ignore the term.level inhabitants of the base types, and just treat them as uninterpreted constants: we don't really need the distinction between constants and values. For theory, we can just assume that some generic base types (e.g. $B$ and $C$) exist, without defining them further.

#### Unit type
In C-like languages, this type is usually called `void`. To introduce it, we do not add any computation rules. We must only add it to the grammar, values and types, and then add a single typing rule that trivially verifies units.

$$
\Gamma\vdash\text{unit}:\text{Unit}
\label{eq:t-unit} \tag{T-Unit}
$$

Units are not too interesting, but *are* quite useful in practice, in part because they allow for other extensions.

#### Sequencing
We can define sequencing as two statements following each other:

{% highlight antlr linenos %}
t ::=
    ...
    t1; t2
{% endhighlight %}

This implies adding some evaluation and typing rules, defined below:

$$
\begin{align}
\frac{t_1 \longrightarrow t_1'}{t_1;\ t_2 \longrightarrow t_1';\ t_2}
\label{eq:e-seq}\tag{E-Seq} \\ \\

(\text{unit};\ t_2) \longrightarrow t_2
\label{eq:e-seqnext}\tag{E-SeqNext} \\ \\

\frac{\Gamma\vdash t_1 : \text{Unit} \quad \Gamma\vdash t_2: T_2}{\Gamma\vdash t_1;\ t_2 : T_2}
\label{eq:t-seq}\tag{T-Seq} \\
\end{align}
$$

But there's another way that we could define sequencing: simply as syntactic sugar, a derived form for something else. In this way, we define an external language, that is transformed to an internal language by the compiler in the desugaring step.

$$
t_1;\ t_2 \defeq (\lambda x: \text{Unit}.\ t_2)\ t_1
\qquad \text{where } x\notin\text{ FV}(t_2)
$$

This is useful to know, because it makes proving soundness much easier. We do not need to re-state the inversion lemma, re-prove preservation and progress. We can simple rely on the proof for the underlying internal language.

#### Ascription
{% highlight antlr linenos %}
t ::=
    ...
    t as T
{% endhighlight %}

Ascription allows us to have a compiler type-check a term as really being of the correct type.

The typing rule is simply:

$$
\frac{\Gamma\vdash t_1 : T}{\Gamma\vdash t_1 \text{ as } T: T}
\label{eq:t-ascribe}\tag{T-Ascribe}
$$

This seems like it preserves soundness, but instead of doing the whole proof over again, we'll just propose a simple desugaring:

$$
t \text{ as } T \defeq (\lambda x: T.\ x)\ t
$$

An ascription is equivalent to the term $t$ applied the identity function, typed to return $T$.

#### Pairs
We can introduce pairs into our grammar.

{% highlight antlr linenos %}
t ::= 
    ...
    {t, t}    // pair
    t.1       // first projection
    t.2       // second projection

v ::=
    ...
    {v, v}    // pair value

T ::=
    ...
    T1 x T2   // product types
{% endhighlight %}

We can also introduce evaluation rules for pairs:

$$
\begin{align}
\left\{v_1, v_2\right\}.1 \longrightarrow v_1
\tag{E-PairBeta1}\label{eq:e-pairbeta1} \\ \\

\left\{v_1, v_2\right\}.2 \longrightarrow v_2
\tag{E-PairBeta2}\label{eq:e-pairbeta2} \\ \\

\frac{t_1 \longrightarrow t_1'}{t_1.1\longrightarrow t_1'.1}
\tag{E-Proj1}\label{eq:e-proj1} \\ \\

\frac{t_1 \longrightarrow t_1'}{t_1.2\longrightarrow t_1'.2}
\tag{E-Proj2}\label{eq:e-proj2} \\ \\

\frac{t_1 \longrightarrow t_1'}{\left\{t_1, t_2\right\} \longrightarrow \left\{t_1', t_2\right\}}
\tag{E-Pair1}\label{eq:e-pair1} \\ \\

\frac{t_2 \longrightarrow t_2'}{\left\{t_1, t_2\right\} \longrightarrow \left\{t_1, t_2'\right\}}
\tag{E-Pair2}\label{eq:e-pair2} \\ \\
\end{align}
$$

The typing rules are then:

$$
\begin{align}
\frac{
    \Gamma\vdash t_1: T_1 \quad \Gamma\vdash t_2: T_2
}{
    \Gamma\vdash\left\{ t_1, t_2 \right\} : T_1 \times T_2
} \label{eq:t-pair} \tag{T-Pair} \\ \\

\frac{\Gamma\vdash t_1 : T_{11}\times T_{12}}{\Gamma\vdash t_1.1:T_{11}}
\label{eq:t-proj1}\tag{T-Proj1} \\ \\

\frac{\Gamma\vdash t_1 : T_{11}\times T_{12}}{\Gamma\vdash t_1.2:T_{12}}
\label{eq:t-proj2}\tag{T-Proj2} \\ \\
\end{align}
$$

Pairs have to be added "the hard way": we do not really have a way to define them in a derived form, as we have no existing language features to piggyback onto.

#### Tuples
Tuples are like pairs, except that we do not restrict it to 2 elements; we allow an arbitrary number from 1 to n. We can use pairs to encode tuples: `(a, b, c)` can be encoded as `(a, (b, c))`. Though for performance and convenience, most languages implement them natively.

#### Records
We can easily generalize tuples to records by annotating each field with a label. A record is a bundle of values with labels; it's a map of labels to values and types. Order of records doesn't matter, the only index is the label. 

If we allow numeric labels, then we can encode a tuple as a record, where the index implicitly encodes the numeric label of the record representation. 

No mainstream language has language-level support for records (two case classes in Scala may have the same arguments but a different constructor, so it's not quite the same; records are more like anonymous objects). This is because they're often quite inefficient in practice, but we'll still use them as a theoretical abstraction.

### Sums and variants

#### Sum type
A sum type $T = T_1 + T_2$ is a *disjoint* union of $T_1$ and $T_2$. Pragmatically, we can have sum types in Scala with case classes extending an abstract object:

{% highlight scala linenos %}
sealed trait Option[+T]
case class Some[+T] extends Option[T]
case object None extends Option[Nothing]
{% endhighlight %}

In this example, `Option = Some + None`. We say that $T_1$ is on the left, and $T_2$ on the right. Disjointness is ensured by the tags $\text{inl}$ and $\text{inr}$. We can *think* of these as functions that inject into the left or right of the sum type $T$:

$$
\text{inl}: T_1 \rightarrow T_1 + T_2 \\
\text{inr}: T_2 \rightarrow T_1 + T_2
$$

Still, these aren't really functions, they don't actually have function type. Instead, we use them them to tag the left and right side of a sum type, respectively.

Another way to think of these stems from  [Curry-Howard correspondence](/#curry-howard-correspondence). Recall that in the [BHK interpretation](https://en.wikipedia.org/wiki/Brouwer%E2%80%93Heyting%E2%80%93Kolmogorov_interpretation), a proof of $P \lor Q$ is a pair `<a, b>` where `a` is 0 (also denoted $\text{inl}$) and `b` a proof of $P$, *or* `a` is 1 (also denoted $\text{inr}$) and `b` is a proof of $Q$.

To use elements of a sum type, we can introduce a `case` construct that allows us to pattern-match on a sum type, allowing us to distinguishing the left type from the right one. 

We need to introduce these three special forms in our syntax:

{% highlight antlr linenos %}
t ::= ...                           // terms
    inl t                              // tagging (left)
    inr t                              // tagging (right)
    case t of inl x => t | inr x => t  // case

v ::= ... // values
    inl v   // tagged value (left)
    inr v   // tagged value (right)

T ::= ...  // types
    T + T     // sum type
{% endhighlight %}


This also leads us to introduce some new evaluation rules:

$$
\begin{align}
    \begin{rcases}
        \text{case } (& \text{inl } v_0) \text{ of} \\
                      & \text{inl } x_1 \Rightarrow t_1 \ \mid \\
                      & \text{inr } x_2 \Rightarrow t_2 \\
    \end{rcases} \longrightarrow [x_1 \mapsto v_0] t_1
    \label{eq:e-caseinl}\tag{E-CaseInl} \\ \\

    \begin{rcases}
        \text{case } (& \text{inr } v_0) \text{ of} \\
                      & \text{inl } x_1 \Rightarrow t_1 \ \mid \\
                      & \text{inl } x_2 \Rightarrow t_2 \\
    \end{rcases} \longrightarrow [x_2 \mapsto v_0] t_2
    \label{eq:e-caseinr}\tag{E-CaseInr} \\ \\

    \frac{t_0 \longrightarrow t_0'}{
        \begin{rcases}
            \text{case } & t_0 \text{ of} \\
                         & \text{inl } x_1 \Rightarrow t_1 \ \mid \\
                         & \text{inr } x_2 \Rightarrow t_2
        \end{rcases} \longrightarrow \begin{cases}
            \text{case } & t_0' \text{ of} \\
                         & \text{inl } x_1 \Rightarrow t_1 \ \mid \\
                         & \text{inr } x_2 \Rightarrow t_2
        \end{cases}
    } \label{eq:e-case}\tag{E-Case} \\ \\

\frac{t_1 \longrightarrow t_1'}{\text{inl }t_1 \longrightarrow \text{inl }t_1'}
\label{eq:e-inl}\tag{E-Inl} \\ \\

\frac{t_1 \longrightarrow t_1'}{\text{inr }t_1 \longrightarrow \text{inr }t_1'}
\label{eq:e-inr}\tag{E-Inr} \\ \\
\end{align}
$$

And we'll also introduce three typing rules:

$$
\begin{align}
\frac{\Gamma\vdash t_1 : T_1}{\Gamma\vdash\text{inl } t_1 : T_1 + T_2}
\label{eq:t-inl}\tag{T-Inl} \\ \\

\frac{\Gamma\vdash t_1 : T_2}{\Gamma\vdash\text{inr } t_1 : T_1 + T_2}
\label{eq:t-inr}\tag{T-Inr} \\ \\

\frac{
    \Gamma\vdash t_0 : T_1 + T_2 \quad
    \Gamma\cup(x_1: T_1) \vdash t_1 : T \quad
    \Gamma\cup(x_2: T_2) \vdash t_2 : T
}{
    \Gamma\vdash\text{case } t_0 \text{ of inl } x_1 \Rightarrow t_1 \mid \text{inr } x_2 \Rightarrow t_2 : T
}
\label{eq:t-case}\tag{T-Case} \\
\end{align}
$$

#### Sums and uniqueness of type
The rules $\ref{eq:t-inr}$ and $\ref{eq:t-inl}$ may seem confusing at first. We only have one type to deduce from, so what do we assign to $T_2$ and $T_1$, respectively? These rules mean that we have lost uniqueness of types: if $t$ has type $T$, then $\text{inl } t$ has type $T+U$ **for every** $U$.

There are a couple of solutions to this:

1. We can infer $U$ as needed during typechecking
2. Give constructors different names and only allow each name to appear in one sum type. This requires generalization to [variants](#variants), which we'll see next. OCaml adopts this solution.
3. Annotate each inl and inr with the intended sum type.

For now, we don't want to look at type inference and variance, so we'll choose the third approach for simplicity. We'll introduce these annotation as ascriptions on the injection operators in our grammar:

{% highlight antlr linenos %}
t ::=
    ...
    inl t as T
    inr t as T

v ::=
    ...
    inl v as T
    inr v as T
{% endhighlight %}

The evaluation rules would be exactly the same as previously, but with ascriptions in the syntax. The injection operators just now also specify *which* sum type we're injecting into, for the sake of uniqueness of type.

#### Variants
Just as we generalized binary products to labeled records, we can generalize binary sums to labeled variants. We can label the members of the sum type, so that we write $\langle l_1: T_1, l_2: T_2 \rangle$ instead of $T_1 + T_2$ ($l_1$ and $l_2$ are the labels). 

As a motivating example, we'll show a useful idiom that is possible with variants, the optional value. We'll use this to create a table. The example below is just like in OCaml.

{% highlight scala linenos %}
OptionalNat = <none: Unit,  some: Nat>;
Table = Nat -> OptionalNat;
emptyTable = Î»t: Nat. <none=unit> as OptionalNat;

extendTable = 
    Î»t: Table. Î»key: Nat. Î»val: Nat.
        Î»search: Nat.
            if (equal search key) then <some=val> as OptionalNat
            else (t search)
{% endhighlight %}

The implementation works a bit like a linked list, with linear look-up. We can use the result from the table by distinguishing the outcome with a `case`:

{% highlight scala linenos %}
x = case t(5) of
    <none=u> => 999
  | <some=v> => v
{% endhighlight %}

### Recursion 
In STLC, all programs terminate. We won't go into too much detail on this topic, but the main idea is that evaluation of a well-typed program is guaranteed to halt; we say that the well-typed terms are *normalizable*. 

Indeed, the infinite recursions from untyped lambda calculus (terms like $\text{omega}$ and $\text{fix}$) are not typable, and thus cannot appear in STLC. Since we can't express $\text{fix}$ in STLC, instead of defining it as a term in the language, we can add it as a primitive instead to get recursion.

{% highlight antlr linenos %}
t ::=
    ...
    fix t
{% endhighlight %}

We'll need to add evaluation rules recreating its behavior, and a typing rule that restricts its use to the intended use-case.

$$
\begin{align}
\text{fix } (\lambda x: T_1.\ t_2) \longrightarrow \left[
    x\mapsto (\text{fix }(\lambda x: T_1.\ t_2))
\right] t_2
\label{eq:e-fixbeta}\tag{E-FixBeta} \\ \\

\frac{t_1 \longrightarrow t_1'}{\text{fix }t_1 \longrightarrow \text{fix }t_1'}
\label{eq:e-fix}\tag{E-Fix} \\ \\

\frac{\Gamma\vdash t_1 : T_1 \rightarrow T_1}{\Gamma\vdash\text{fix }t_1:T_1}
\label{eq:t-fix}\tag{T-Fix}
\end{align}
$$

In order for a function to be recursive, the function needs to map a type to the same type, hence the restriction of $T_1 \rightarrow T_1$. The type $T_1$ will itself be a function type if we're doing a recursion. Still, note that the type system doesn't enforce this. There will actually be situations in which it will be handy to use something else than a function type inside a fix operator. 

Seeing that this fixed-point notation can be a little involved, we can introduce some nice syntactic sugar to work with it:

$$
\text{letrec } x: T_1 = t_1 \text{ in } t_2
\quad \defeq \quad
\text{let } x = \text{fix } (\lambda x: T_1.\ t_1) \text{ in } t_2
$$

This $t_1$ can now refer to the $x$; that's the convenience offered by the construct. Although we don't strictly need to introduce typing rules (it's syntactic sugar, we're relying on existing constructs), a typing rule for this could be:

$$
\frac{\Gamma\cup(x:T_1)\vdash t_1:T_1 \quad \Gamma\cup(x: T_1)\vdash t_2:T_2}{\Gamma\vdash\text{letrec } x: T_1 = t_1 \text{ in } t_2:T_2}
$$

In Scala, a common error message is that a recursive function needs an explicit return type, for the same reasons as the typing rule above.

### References
#### Mutability 
In most programming languages, variables are (or can be) mutable. That is, variables can provide a name referring to a previously calculated value, as well as a way of overwriting this value with another (under the same name). How can we model this in STLC?

Some languages (e.g. OCaml) actually formally separate variables from mutation. In OCaml, variables are only for naming, the binding between a variable and a value is immutable. However, there is the concept of *mutable values*, also called *reference cells* or *references*. This is the style we'll study, as it is easier to work with formally. A mutable value is represented in the type-level as a `Ref T` (or perhaps even a `Ref(Option T)`, since the null pointer cannot produce a value).

The basic operations are allocation with the `ref` operator, dereferencing with `!` (in C, we use the `*` prefix), and assignment with `:=`, which updates the content of the reference cell. Assignment returns a `unit` value.

#### Aliasing
Two variables can reference the same cell: we say that they are *aliases* for the same cell. Aliasing is when we have different references (under different names) to the same cell. Modifying the value of the reference cell through one alias modifies the value for all other aliases.

The possibility of aliasing is all around us, in object references, explicit pointers (in C), arrays, communication channels, I/O devices; there's practically no way around it. Yet, alias analysis is quite complex, costly, and often makes is hard for compilers to do optimizations they would like to do.

With mutability, the order of operations now matters; `r := 1; r := 2` isn't the same as `r := 2; r := 1`. If we recall the [Church-Rosser theorem](#confluence-in-full-beta-reduction), we've lost the principle that all reduction paths lead to the same result. Therefore, some language designers disallow it (Haskell). But there are benefits to allowing it, too: efficiency, dependency-driven data flow (e.g. in GUI), shared resources for concurrency (locks), etc. Therefore, most languages provide it.

Still, languages without mutability have come up with a bunch of abstractions that allow us to have some of the benefits of mutability, like monads and lenses.

#### Typing rules
We'll introduce references as a type `Ref T` to represent a variable of type `T`. We can construct a reference as `r = ref 5`, and access the contents of the reference using `!r` (this would return `5` instead of `ref 5`).

Let's define references in our language:

{% highlight antlr linenos %}
t ::=          // terms
    unit          // unit constant
    x             // variable
    Î»x: T. t      // abstraction
    t t           // application
    ref t         // reference creation
    !t            // dereference
    t := t        // assignment
{% endhighlight %}

$$
\begin{align}
\frac{\Gamma\vdash t_1 : T_1}{\Gamma\vdash \text{ref } t_1 : \text{Ref } T_1}
\label{eq:t-ref}\tag{T-Ref} \\ \\

\frac{\Gamma\vdash t_1: \text{Ref } T_1}{\Gamma\vdash !t_1 : T_1}
\label{eq:t-deref}\tag{T-Deref} \\ \\

\frac{\Gamma\vdash t_1 : \text{Ref } T_1 \quad \Gamma\vdash t_2: T_1}{\Gamma\vdash t_1 := t_2 : \text{Unit}}
\label{eq:t-assign}\tag{T-Assign} \\ \\
\end{align}
$$

#### Evaluation
What is the *value* of `ref 0`? The crucial observation is that evaluation `ref 0` must *do* something. Otherwise, the two following would behave the same:

{% highlight c linenos %}
r = ref 0
s = ref 0

r = ref 0 
s = r
{% endhighlight %}

Evaluating `ref 0` should allocate some storage, and return a reference (or pointer) to that storage. A reference names a location in the **store** (also known as the *heap*, or just *memory*). Concretely, the store could be an array of 8-bit bytes, indexed by 32-bit integers. More abstractly, it's an array of values, or even more abstractly, a partial function from locations to values.

We can introduce this idea of locations in our syntax. This syntax is exactly the same as the previous one, but adds the notion of locations:

{% highlight antlr linenos %}
v ::=         // values
    unit         // unit constant
    Î»x: T. t     // abstraction value
    l            // store location

t ::=         // terms
    unit         // unit constant
    x            // variable
    Î»x: T. t     // abstraction
    t t          // application
    ref t        // reference creation
    !t           // dereference
    t := t       // assignment
    l            // store location 
{% endhighlight %}

This doesn't mean that we'll allow programmers to write explicit locations in their programs. We just use this as a modeling trick; we're enriching the internal language to include some run-time structures.

With this added notion of stores and locations, the result of an evaluation now depends on the store in which it is evaluated, which we need to reflect in our evaluation rules. Evaluation must now include terms $t$ **and** store $\mu$:

$$
t \mid \mu \longrightarrow t' \mid \mu'
$$

Let's take a look for the evaluation rules for STLC with references, operator by operator.

$$
\begin{align}
\frac{t_1 \mid \mu \longrightarrow t_1'\mid\mu'}{t_1 := t_2 \mid \mu \longrightarrow t_1' := t_2 \mid \mu'}
\label{eq:e-assign1}\tag{E-Assign1} \\ \\

\frac{t_2 \mid \mu \longrightarrow t_2'\mid\mu'}{t_1 := t_2 \mid \mu \longrightarrow t_1 := t_2' \mid \mu'}
\label{eq:e-assign2}\tag{E-Assign2} \\ \\

l := v_2 \mid \mu \longrightarrow \text{unit}\mid[l\mapsto v_2]\mu
\label{eq:e-assign}\tag{E-Assign} \\ \\
\end{align}
$$

The assignments $\ref{eq:e-assign1}$ and $\ref{eq:e-assign2}$ evaluate terms until they become values. When they have been reduced, we can do that actual assignment: as per $\ref{eq:e-assign}$, we update the store and return return `unit`.

$$
\begin{align}
\frac{t_1 \mid \mu \longrightarrow t_1' \mid \mu'}{\text{ref } t_1 \mid \mu \longrightarrow \text{ref } t_1' \mid \mu'}
\label{eq:e-ref}\tag{E-Ref} \\ \\

\frac{l \notin \text{dom}(\mu)}{\text{ref } v_1 \mid \mu \longrightarrow l \mid (\mu \cup (l\mapsto v_1))}
\label{eq:e-refv}\tag{E-RefV}
\end{align}
$$

A reference $\text{ref }t_1$ first evaluates $t_1$ until it is a value ($\ref{eq:e-ref}$). To evaluate the reference operator, we find a fresh location $l$ in the store, to which it binds $v_1$, and it returns the location $l$.

$$
\begin{align}
\frac{t_1 \mid \mu \longrightarrow t_1' \mid \mu'}{!t_1 \mid \mu \longrightarrow !t_1' \mid \mu'}
\label{eq:e-deref}\tag{E-Deref} \\ \\

\frac{\mu(l) = v}{!l\mid\mu \longrightarrow v\mid\mu}
\label{eq:e-derefloc}\tag{E-DerefLoc}
\end{align}
$$

We find the same congruence rule as usual in $\ref{eq:e-deref}$, where a term $!t_1$ first evaluates $t_1$ until it is a value. Once it is a value, we can return the value in the current store using $\ref{eq:e-derefloc}$.

The evaluation rules for abstraction and application are augmented with stores, but otherwise unchanged.

#### Store typing
What is the type of a location? The answer to this depends on what is in the store. Unless we specify it, a store could contain anything at a given location, which is problematic for typechecking. The solution is to type the locations themselves. This leads us to a typed store:

$$
\begin{align}
\mu = (& l_1 \mapsto \text{Nat}, \\
       & l_2 \mapsto \lambda x: \text{Unit}. x)
\end{align}
$$

As a first attempt at a typing rule, we can just say that the type of a location is given by the type of the value in the store at that location:

$$
\frac{\Gamma\vdash\mu(l) : T_1}{\Gamma\vdash l : \text{Ref } T_1}
$$

This is problematic though; in the following, the typing derivation for $!l_2$ would be infinite because we have a cyclic reference:

$$
\begin{align}
\mu =\ (& l_1 \mapsto \lambda x: \text{Nat}.\ !l_2\ x, \\
        & l_2 \mapsto \lambda x: \text{Nat}.\ !l_1\ x)
\end{align}
$$

The core of the problem here is that we would need to recompute the type of a location every time. But shouldn't be necessary. Seeing that references are strongly typed as `Ref T`, we know exactly what type of value we can place in a given store location. Indeed, the typing rules we chose for references guarantee that a given location in the store always is used to hold values of the same type.

So to fix this problem, we need to introduce a **store typing**. This is a partial function from location to types, which we'll denote by $\Sigma$. 

Suppose we're given a store typing $\Sigma$ describing the store $\mu$. We can use $\Sigma$ to look up the types of locations, without doing a lookup in $\mu$:

$$
\frac{\Sigma(l) = T_1}{\Gamma\mid\Sigma\vdash l : \text{Ref } T_1}
\label{eq:t-loc}\tag{T-Loc}
$$

This tells us how to check the store typing, but how do we create it? We can start with an empty typing $\Sigma = \emptyset$, and add a typing relation with the type of $v_1$  when a new location is created during evaluation of $\ref{eq:e-refv}$.

The rest of the typing rules remain the same, but are augmented with the store typing. So in conclusion, we have updated our evaluation rules with a *store* $\mu$, and our typing rules with a *store typing* $\Sigma$.

#### Safety
Let's take a look at progress and preservation in this new type system. Preservation turns out to be more interesting, so let's look at that first.

We've added a store and a store typing, so we need to add those to the statement of preservation to include these. Naively, we'd write:

$$
\Gamma\mid\Sigma\vdash t: T \text{ and }
t\mid\mu\longrightarrow t'\mid\mu'
\quad \implies \quad 
\Gamma\mid\Sigma\vdash t': T
$$

But this would be wrong! In this statement, $\Sigma$ and $\mu$ would not be constrained to be correlated at all, which they need to be. This constraint can be defined as follows:

A store $\mu$ is well typed with respect to a typing context $\Gamma$ and a store typing $\Sigma$ (which we denote by $\Gamma\mid\Sigma\vdash\mu$) if the following is satisfied:

$$
\text{dom}(\mu) = \text{dom}(\Sigma)
\quad \text{and} \quad 
\Gamma\mid\Sigma\vdash\mu(l) : \Sigma(l),\ \forall l\in\text{dom}(\mu)
$$

This gets us closer, and we can write the following preservation statement:

$$
\Gamma\mid\Sigma \vdash t : T \text{ and }
t\mid\mu \longrightarrow t'\mid\mu \text{ and }
\Gamma\mid\Sigma \vdash \mu
\quad \implies \quad
\Gamma\mid\Sigma\vdash t' : T
$$

But this is still wrong! When we create a new cell with $\ref{eq:e-refv}$, we would break the correspondence between store typing and store.

The correct version of the progress theorem is the following:

$$
\Gamma\mid\Sigma \vdash t : T \text{ and }
t\mid\mu \longrightarrow t'\mid\mu \text{ and }
\Gamma\mid\Sigma \vdash \mu
\quad \implies \quad
\text{for some } \Sigma' \supseteq \Sigma, \;\;
\Gamma\mid\Sigma'\vdash t' : T
$$

This progress theorem just asserts that there is *some* store typing $\Sigma' \supseteq \Sigma$ (agreeing with $\Sigma$ on the values of all old locations, but that may have also add new locations), such that $t'$ is well typed in $\Sigma'$.

The progress theorem must also be extended with stores and store typings:

Suppose that $t$ is a closed, well-typed term; that is, $\emptyset\mid\Sigma\vdash t: T$ for some type $T$ and some store typing $\Sigma$. Then either $t$ is a value or else, for any store $\mu$ such that $\emptyset\mid\Sigma\vdash\mu$[^well-typed-store-notation], there is some term $t'$ and store $\mu'$ with $t\mid\mu \longrightarrow t'\mid\mu'$.

[^well-typed-store-notation]: Recall that this notation is used to say a store $\mu$ is well typed with respect to a typing context $\Gamma$ and a store typing $\Sigma$, as defined in the section on [safety in STLC with stores](#safety).