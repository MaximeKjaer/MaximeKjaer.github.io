---
title: CS-452 Foundations of Software
description: "My notes from the CS-452 Foundations of Software course given at EPFL, in the 2018 autumn semester (MA1)"
unlisted: true
---

* TOC
{:toc}

## Writing a parser with parser combinators
In Scala, you can (ab)use the operator overload to create an embedded DSL (EDSL) for grammars. While a grammar may look as follows in a grammar description language (Bison, Yak, ANTLR, ...):

{% highlight antlr linenos %}
Expr ::= Term {'+' Term | 'âˆ’' Term}
Term ::= Factor {'âˆ—' Factor | '/' Factor}
Factor ::= Number | '(' Expr ')'
{% endhighlight %}

In Scala, we can model it as follows:

{% highlight scala linenos %}
def expr: Parser[Any] = term ~ rep("+" ~ term | "âˆ’" ~ term)
def term: Parser[Any] = factor ~ rep("âˆ—" ~ factor | "/" ~ factor)
def factor: Parser[Any] = "(" ~ expr ~ ")" | numericLit
{% endhighlight %}

This is perhaps a little less elegant, but allows us to encode it directly into our language, which is often useful for interop.

The `~`, `|`, `rep` and `opt` are **parser combinators**. These are primitives with which we can construct a full parser for the grammar of our choice.

### Boilerplate

First, let's define a class `ParseResult[T]` as an ad-hoc monad; parsing can either succeed or fail:

{% highlight scala linenos %}
sealed trait ParseResult[T]
case class Success[T](result: T, in: Input) extends ParseResult[T]
case class Failure(msg : String, in: Input) extends ParseResult[Nothing]
{% endhighlight %}

> ðŸ‘‰ `Nothing` is the bottom type in Scala; it contains no members, and nothing can extend it

Let's also define the tokens produced by the lexer (which we won't define) as case classes extending `Token`:

{% highlight scala linenos %}
sealed trait Token
case class Keyword(chars: String) extends Token
case class NumericLit(chars: String) extends Token
case class StringLit(chars: String) extends Token
case class Identifier(chars: String) extends Token
{% endhighlight %}

Input into the parser is then a lazy stream of tokens (with positions for error diagnostics, which we'll omit here):

{% highlight scala linenos %}
type Input = Reader[Token]
{% endhighlight %}

We can then define a standard, sample parser which looks as follows on the type-level:

{% highlight scala linenos %}
class StandardTokenParsers {
    type Parser = Input => ParseResult
}
{% endhighlight %}

### The basic idea
For each language (defined by a grammar symbol `S`), define a function `f` that, given an input stream `i` (with tail `i'`):

- if a prefix of `i` is in `S`, return `Success(Pair(x, i'))`, where `x` is a result for `S`
- otherwise, return `Failure(msg, i)`, where `msg` is an error message string

The first is called *success*, the second is *failure*. We can compose operations on this somewhat conveniently, like we would on a monad (like `Option`).

### Simple parser primitives
All of the above boilerplate allows us to define a parser, which succeeds if the first token in the input satisfies some given predicate `pred`. When it succeeds, it reads the token string, and splits the input there.

{% highlight scala linenos %}
def token(kind: String)(pred: Token => boolean) = new Parser[String] {
    def apply(in : Input) =
        if (pred(in.head)) Success(in.head.chars, in.tail)
        else Failure(kind + " expected ", in)
}
{% endhighlight %}

We can use this to define a keyword parser:

{% highlight scala linenos %}
implicit def keyword(chars: String) = token("'" + chars + "'") {
    case Keyword(chars1) => chars == chars1
    case _ => false
}
{% endhighlight %}

Marking it as `implicit` allows us to write keywords as normal strings, where we can omit the `keyword` call (this helps us simplify the notation in our DSL; we can write `"if"` instead of `keyword("if")`).

We can make other parsers for our other case classes quite simply:

{% highlight scala linenos %}
def numericLit = token("number")( .isInstanceOf[NumericLit])
def stringLit = token("string literal")( .isInstanceOf[StringLit])
def ident = token("identifier")( .isInstanceOf[Identifier])
{% endhighlight %}

### Parser combinators
We are going to define the following parser combinators:

- `~`: sequential composition
- `<~`, `>~`: sequential composition, keeping left / right only
- `|`: alternative
- `opt(X)`: option (like a `?` quantifier in a regex)
- `rep(X)`: repetition (like a `*` quantifier in a regex)
- `repsep(P, Q)`: interleaved repetition
- `^^`: result conversion (like a `map` on an `Option`)
- `^^^`: constant result (like a `map` on an `Option`, but returning a constant value regardless of result)

But first, we'll write some very basic parser combinators: `success` and `failure`, that respectively always succeed and always fail:

{% highlight scala linenos %}
def success[T](result: T) = new Parser[T] {
    def apply(in: Input) = Success(result, in)
}

def failure(msg: String) = new Parser[Nothing] {
    def apply(in: Input) = Failure(msg, in)
}
{% endhighlight %}

All of the above are methods on a `Parser[T]` class. Thanks to infix space notation in Scala, we can denote `x.y(z)` as `x y z`, which allows us to simplify our DSL notation; for instance `A ~ B` corresponds to `A.~(B)`.

{% highlight scala linenos %}
abstract class Parser[T] {
    // An abstract method that defines the parser function
    def apply(in : Input): ParseResult

    def ~[U](rhs: Parser[U]) = new Parser[T ~ U] {
        def apply(in: Input) = Parser.this(in) match {
            case Success(x, tail) => rhs(tail) match {
                case Success(y, rest) => Success(new ~(x, y), rest)
                case failure => failure
            }
            case failure => failure
        }
    }

    def |(rhs: => Parser[T]) = new Parser[T] {
        def apply(in : Input) = Parser.this(in) match {
            case s1 @ Success(_, _) => s1
            case failure => rhs(in)
        }
    }

    def ^^[U](f: T => U) = new Parser[U] {
        def apply(in : Input) = Parser.this(in) match {
            case Success(x, tail) => Success(f(x), tail)
            case x => x
        }
    }

    def ^^^[U](r: U): Parser[U] = ^^(x => r)
}
{% endhighlight %}

> ðŸ‘‰ In Scala, `T ~ U` is syntactic sugar for `~[T, U]`, which is the type of the case class we'll define below

For the `~` combinator, when everything works, we're using `~`, a case class that is equivalent to `Pair`, but prints the way we want to and allows for the concise type-level notation above.

{% highlight scala linenos %}
case class ~[T, U](_1 : T, _2 : U) {
    override def toString = "(" + _1 + " ~ " + _2 +")"
}
{% endhighlight %}

At this point, we thus have **two** different meanings for `~`: a *function* `~` that produces a `Parser`, and the `~(a, b)` *case class* pair that this parser returns (all of this is encoded in the function signature of the `~` function).

Note that the `|` combinator takes the right-hand side parser as a call-by-name argument. This is because we don't want to evaluate it unless it is strictly neededâ€”that is, if the left-hand side fails.

`^^` is like a `map` operation on `Option`; `P ^^ f` succeeds iff `P` succeeds, in which case it applies the transformation `f` on the result of P. Otherwise, it fails.

### Shorthands

We can now define shorthands for common combinations of parser combinators:

{% highlight scala linenos %}
def opt[T](p : Parser[T]): Parser[Option[T]] = p ^^ Some | success(None)

def rep[T](p : Parser[T]): Parser[List[T]] = 
    p ~ rep(p) ^^ { case x ~ xs => x :: xs } | success(Nil)

def repsep[T, U](p : Parser[T], q : Parser[U]): Parser[List[T]] = 
    p ~ rep(q ~> p) ^^ { case r ~ rs => r :: rs } | success(Nil)
{% endhighlight %}

Note that none of the above can fail. They may, however, return `None` or `Nil` wrapped in `success`.


As an exercise, we can implement the `rep1(P)` parser combinator, which corresponds to the `+` regex quantifier:

{% highlight scala linenos %}
def rep1[T](p: Parser[T]) = p ~ rep(p)
{% endhighlight %}

### Example: JSON parser

We did not mention `lexical.delimiters` and `lexical.reserved` in the above, and for the sake of brevity, we omit the implementation of `stringLit` and `numericLit`.

{% highlight scala linenos %}
object JSON extends StandardTokenParsers {
    lexical.delimiters += ("{", "}", "[", "]", ":")
    lexical.reserved += ("null", "true", "false")

    // Return Map
    def obj: Parser[Any] = "{" ~ repsep(member, ",") ~ "}" ^^ (ms => Map() ++ ms)

    // Return List
    def arr: Parser[Any] = "[" ~> repsep(value, ",") <~ "]"

    // Return name/value pair:
    def member: Parser[Any] = stringLit ~ ":" ~ value ^^ {
        case name ~ ":" ~ value => (name, value) 
    }

    // Return correct Scala type
    def value: Parser[Any] =
          obj 
        | arr 
        | stringLit
        | numericLit ^^ (_.toInt)
        | "null" ^^^ null
        | "true" ^^^ true
        | "false" ^^^ false
}
{% endhighlight %}

### The trouble with left-recursion

Parser combinators work top-down and therefore do not allow for left-recursion. For example, the following would go into an infinite loop, where the parser keeps recursively matching the same token unto `expr`:

{% highlight scala linenos %}
def expr = expr ~ "-" ~ term
{% endhighlight %}

Let's take a look at an arithmetic expression parser:

{% highlight scala linenos %}
object Arithmetic extends StandardTokenParsers {
    lexical.delimiters ++= List("(", ")", "+", "âˆ’", "âˆ—", "/")
    def expr: Parser[Any] = term ~ rep("+" ~ term | "âˆ’" ~ term)
    def term: Parser[Any] = factor ~ rep("âˆ—" ~ factor | "/" ~ factor)
    def factor: Parser[Any] = "(" ~ expr ~ ")" | numericLit
}
{% endhighlight %}

This definition of `expr`, namely `term ~ rep("-" ~ term)` produces a right-leaning tree. For instance, `1 - 2 - 3` produces `1 ~ List("-" ~ 2, ~ "-" ~ 3)`. 

The solution is to combine calls to `rep` with a final foldLeft on the list:

{% highlight scala linenos %}
object Arithmetic extends StandardTokenParsers {
    lexical.delimiters ++= List("(", ")", "+", "âˆ’", "âˆ—", "/")
    def expr: Parser[Any] = term ~ rep("+" ~ term | "âˆ’" ~ term) ^^ reduceList
    def term: Parser[Any] = factor ~ rep("âˆ—" ~ factor | "/" ~ factor) ^^ reduceList
    def factor: Parser[Any] = "(" ~ expr ~ ")" | numericLit

    private def reduceList(list: Expr ~ List[String ~ Expr]): Expr = list match {
        case x ~ xs => (x foldLeft ps)(reduce)
    }

    private def reduce(x: Int, r: String ~ Int) = r match {
        case "+" ~ y => x + y
        case "âˆ’" ~ y => x âˆ’ y
        case "âˆ—" ~ y => x âˆ— y
        case "/" ~ y => x / y
        case => throw new MatchError("illegal case: " + r)
    }
}
{% endhighlight %}

> ðŸ‘‰ It used to be that the standard library contained parser combinators, but those are now a [separate module](https://github.com/scala/scala-parser-combinators). This module contains a `chainl` (chain-left) method that reduces after a `rep` for you.

## Arithmetic expressions â€” abstract syntax and proof principles
This section follows Chapter 3 in TAPL.

### Basics of induction
Ordinary induction is simply:

```
Suppose P is a predicate on natural numbers.
Then:
	If P(0)
	and, for all i, P(i) implies P(n) holds for all n
	then P(n) holds for all n
```

We can also do complete induction:

```
Suppose P is a predicate on natural numbers.
Then:
	If for each natural number n,
	given P(i) for all i < n we can show P(n)
	then P(n) holds for all n
```

It proves exactly the same thing as ordinary induction, it is simply a restated version. They're *interderivable*; assuming one, we can prove the other. Which one to use is simply a matter of style or convenience. We'll see some more equivalent styles as we go along.

### Mathematical representation
Let's assume the following grammar:

{% highlight antlr linenos %}
t ::= 
    true
    false
    if t then t else t
    0
    succ t
    pred t
    iszero t
{% endhighlight %}

What does this really define? A few suggestions:

- A set of character strings
- A set of token lists
- A set of abstract syntax trees

It depends on how you read it; a grammar like the one above contains information about all three.

However, we are mostly interested in the ASTs. The above grammar is therefore called an **abstract grammar**. Its main purpose is to suggest a mapping from character strings to trees.

For our use of these, we won't be too strict with these. For instance, we'll freely use parentheses to disambiguate what tree we mean to describe, even though they're not strictly supported by the grammar. What matters to us here aren't strict implementation semantics, but rather that we have a framework to talk about ASTs. For our purposes, we'll consider that two terms producing the same AST are basically the same; still, we'll distinguish terms that only have the same evaluation result, as they don't necessarily have the same AST.

How can we express our grammar as mathematical expressions? A grammar describes the legal *set* of terms in a program by offering a recursive definition. While recursive definitions may seem obvious and simple to a programmer, we have to go through a few hoops to make sense of them mathematically.

#### Mathematical representation 1
We can use a set $$\mathcal{T}$$ of terms; it is the smallest set such that:

1. $$\left\{ \text{true}, \text{false}, 0 \right\} \subseteq \mathcal{T}$$,
2. If $$t_1 \in \mathcal{T}$$ then $$\left\{ \text{succ } t_1, \text{pred } t_1, \text{iszero } t_1 \right\} \subseteq \mathcal{T}$$,
3. If $$t_1, t_2, t_3 \in \mathcal{T}$$ then we also have $$\text{if } t_1 \text{ then } t_2 \text{ else } t_3 \in \mathcal{T}$$.

#### Mathematical representation 2
We can also write this somewhat more graphically:

$$
\text{true } \in \mathcal{T},  \quad
\text{false } \in \mathcal{T}, \quad
0 \in \mathcal{T}              \\ \\

\frac{t_1 \in \mathcal{T}}{\text{succ } t_1 \in \mathcal{T}}, \quad
\frac{t_1 \in \mathcal{T}}{\text{pred } t_1 \in \mathcal{T}}, \quad
\frac{t_1 \in \mathcal{T}}{\text{iszero } t_1 \in \mathcal{T}} \\ \\

\frac{t_1 \in \mathcal{T}, \quad t_2 \in \mathcal{T}, \quad t_3 \in \mathcal{T}}{\text{if } t_1 \text{ then } t_2 \text{ else } t_3 \in \mathcal{T}}
$$

This is exactly equivalent to representation 1, but we have just introduced a different notation. Note that "the smallest set closed under..." is often not stated explicitly, but implied.

#### Mathematical representation 3
Alternatively, we can build up our set of terms as an infinite union:

$$
\begin{align}
\mathcal{S}_0 = & & \emptyset \\
\mathcal{S}_{i+1} = 
    &      & \left\{ \text{true}, \text{ false}, 0 \right\} \\
    & \cup & \left\{ \text{succ } t_1, \text{pred } t_1, \text{iszero } t_1 | t_1 \in \mathcal{S}_i \right\} \\
    & \cup & \left\{ \text{if } t_1 \text{ then } t_2 \text{ else } t_3 | t_1, t_2, t_3 \in \mathcal{S}_i \right\}
\end{align}
$$

We can thus build our final set as follows:

$$
\mathcal{S} = \bigcup_i{\mathcal{S}_i}
$$

Note that we can "pull out" the definition into a generating function $$F$$:

$$
\mathcal{S}_0 = \emptyset \\
\mathcal{S}_{i+1} = F(\mathcal{S}) \\
\mathcal{S} = \bigcup_i{\mathcal{S}_i}
$$

The generating function is thus defined as:

$$
\begin{align}
F_1(U) & = \left\{ \text{true} \right\} \\
F_2(U) & = \left\{ \text{false} \right\} \\
F_3(U) & = \left\{ 0 \right\} \\
F_4(U) & = \left\{ \text{succ } t_1 | t_1 \in U \right\} \\
F_5(U) & = \left\{ \text{pred } t_1 | t_1 \in U \right\} \\
F_6(U) & = \left\{ \text{iszero } t_1 | t_1 \in U \right\} \\
F_7(U) & = \left\{ \text{if } t_1 \text{ then } t_2 \text{ else } t_3 | t_1, t_2, t_3 \in U \right\} \\
\end{align} \\

F(U) = \bigcup_{i=1}^7{F_i(U)}
$$

Each function takes a set of terms $$U$$ as input and produces "terms justified by $$U$$" as output; that is, all terms that have the items of $$U$$ as subterms.

The set $$U$$ is said to be **closed under F** or **F-closed** if $$F(U) \subseteq U$$.

The set of terms $$\mathcal{T}$$ as defined above is the smallest F-closed set. If $$\mathcal{O}$$ is another F-closed set, then $$\mathcal{T} \subseteq \mathcal{O}$$.

#### Comparison of the representations
We've seen essentially two ways of defining the set (as representation 1 and 2 are equivalent, but with different notation):

1. The smallest set that is closed under certain rules. This is compact and easy to read.
2. The limit of a series of sets. This gives us an *induction principle* on which we can prove things on terms by induction. 

The first one defines the set "from above", by intersecting F-closed sets.

The second one defines it "from below", by starting with $$\emptyset$$ and getting closer and closer to being F-closed.

These are equivalent (we won't prove it, but Proposition 3.2.6 in TAPL does so), but can serve different uses in practice.

### Induction on terms
First, let's define depth: 

The **depth** of a term $$t$$ is the smallest $$i$$ such that $$t\in\mathcal{S_i}$$.

The way we defined $$\mathcal{S}_i$$, it gets larger and larger for increasing $$i$$; the depth gives us the step at which it is introduced into the set.

We see that if a term $$t$$ is in $$\mathcal{S}_i$$, then all of its immediate subterms must be in $$\mathcal{S}_{i-1}$$, meaning that they must have smaller depth.

This justifies the principle of **induction on terms**. Let P be a predicate on a term:

```
If, for each term s,
    given P(r) for all immediate subterms r of s we can show P(s)
    then P(t) holds for all t
```

All this says is that if we can prove the induction step from subterms to terms (under the induction hypothesis), then we have proven the induction.

### Inductive function definitions
An [inductive definition](https://en.wikipedia.org/wiki/Recursive_definition) is used to define the elements in a set recursively, as we have done above. The [recursion theorem](https://en.wikipedia.org/wiki/Recursion#The_recursion_theorem) states that a well-formed inductive definition indeed defines a function, but we'll get into that later.

Let's define our grammar function a little more formally. Constants are the basic values that can't be expanded further; in our example, they are `true`, `false`, `0`. As such, the set of constants appearing in a term $$t$$, written $$\text{Consts}(t)$$, is defined as follows:

$$
\begin{align}
\text{Consts}(\text{true})  & = \left\{ \text{true}  \right\} \\
\text{Consts}(\text{false}) & = \left\{ \text{false} \right\} \\
\text{Consts}(0)            & = \left\{ 0            \right\} \\

\text{Consts}(\text{succ } t_1) & = \text{Consts}(t_1) \\
\text{Consts}(\text{pred } t_1) & = \text{Consts}(t_1) \\
\text{Consts}(\text{iszero } t_1) & = \text{Consts}(t_1) \\
\text{Consts}(\text{if } t_1 \text{ then } t_2 \text{ else } t_3) & = \text{Consts}(t_1) \cup \text{Consts}(t_2) \cup \text{Consts}(t_3) \\
\end{align}
$$

This seems simple, but these semantics aren't perfect. First off, a mathematical definition simply assigns a convenient name to some previously known thing. But here, we're defining the thing in terms of itself, recursively. And the semantics above also allow us to define ill-formed inductive definitions:

$$
\begin{align}
\text{BadConsts}(\text{true})  & = \left\{ \text{true}  \right\} \\
\text{BadConsts}(\text{false}) & = \left\{ \text{false} \right\} \\
\text{BadConsts}(0)            & = \left\{ 0            \right\} \\
\text{BadConsts}(0)            & = \left\{ \right\} = \emptyset  \\

\text{BadConsts}(\text{succ } t_1) & = \text{BadConsts}(t_1) \\
\text{BadConsts}(\text{pred } t_1) & = \text{BadConsts}(t_1) \\
\text{BadConsts}(\text{iszero } t_1) & = \text{BadConsts}(\text{iszero iszero }t_1) \\
\end{align}
$$

The last rule produces infinitely large rules (if we implemented it, we'd expect some kind of stack overflow). We're missing the rules for if-statements, and we have a useless rule for `0`, producing empty sets.

How do we tell the difference between a well-formed inductive definition, and an ill-formed one as above? What is well-formedness anyway? 

#### What is a function?

A relation over $$T, U$$ is a subset of $$T \times U$$, where the Cartesian product is defined as:

$$T\times U = \left\{ (t, u) : t\in T, u\in U \right\}$$

A function $$f$$ from $$A$$ (domain) to $$B$$ (co-domain) can be viewed as a two-place relation, albeit with two additional properties:

- It is **total**: $$\forall a \in A, \exists b \in B : (a, b) \in f$$
- It is **deterministic**: $$(a, b_1) \in f, (a, b_2) \in f \implies b_1 = b_2$$

Totality ensures that the A domain is covered, while being deterministic just means that the function always produces the same result for a given input.

#### Induction theorem 
As previously stated, $$\text{Consts}$$ is a *relation*. It maps terms (A) into the set of constants that they contain (B). The induction theorem states that it is also a *function*. The proof is as follows.

$$\text{Consts}$$ is total and deterministic: for each term $$t$$ there is exactly one set of terms $$C$$ such that $$(t, C) \in \text{Consts}$$. Not that this inclusion notation is equivalent to saying $$\text{Consts}(t) = C$$. The proof is done by induction on $$t$$.

To be able to apply the induction principle for terms, we must first show that for an arbitrary term $$t$$, under the following induction hypothesis:

> For each immediate subterm $$s$$ of $$t$$, there is exactly one set of terms $$C_s$$ such that $$(s, C_s) \in \text{Consts}$$

Then the following needs to be proven as an induction step:

> There is **exactly one** set of terms $$C$$ such that $$(t, C) \in \text{Consts}$$

We proceed by cases on $$t$$ (as if we were pattern matching ðŸ˜‰):

- If $$t$$ is $$0$$, $$\text{true}$$ or $$\text{false}$$
  
  We can immediately see from the definition that of $$\text{Consts}$$ that there is exactly one set of terms $$C = \left\{ t \right\}$$) such that $$(t, C) \in \text{Consts}$$.

  This constitutes our base case.
  
- If $$t$$ is $$\text{succ } t_1$$, $$\text{pred } t_1$$ or $$\text{iszero } t_1$$
  
  The immediate subterm of $$t$$ is $$t_1$$, and the induction hypothesis tells us that there is exactly one set of terms $$C_1$$ such that $$(t_1, C_1) \in \text{Consts}$$. But then it is clear from the definition that there is exactly one set of terms $$C = C_1$$ such that $$(t, C) \in \text{Consts}$$.
  
- If $$t$$ is $$\text{if } t_1 \text{ then } t_2 \text{ else } t_3$$
  
  The induction hypothesis tells us:

    - There is exactly one set of terms $$C_1$$ such that $$(t_1, C_1) \in \text{Consts}$$
    - There is exactly one set of terms $$C_2$$ such that $$(t_2, C_2) \in \text{Consts}$$
    - There is exactly one set of terms $$C_3$$ such that $$(t_3, C_3) \in \text{Consts}$$
  
  It is clear from the definition of $$\text{Consts}$$ that there is exactly one set $$C = C_1 \cup C_2 \cup C_3$$ such that $$(t, C) \in \text{Consts}$$.

This proves that $$\text{Consts}$$ is indeed a function.

But what about $$\text{BadConsts}$$? It is also a relation, but it isn't a function. For instance, $$\text{BadConsts}(0) = \left\{ 0 \right\}$$ and $$\text{BadConsts}(0) = \left\{ \right\}$$, which violates determinism. To reformulate this in terms of the above, there are two sets $$C$$ such that $$(0, C) \in \text{BadConsts}$$, namely $$C = \left\{ 0 \right\}$$ and $$C = \left\{ \right\}$$.

Note that there are many other problems with $$\text{BadConsts}$$, but this is sufficient to prove that it isn't a function.

The [week 2 slides](https://fos2018.github.io/slides/week02.pdf) have another example of a proof by induction, which I won't go into here.

### Operational semantics and reasoning

#### Evaluation
Suppose we have the following syntax

{% highlight antlr linenos %}
t ::=                  // terms
    true                   // constant true
    false                  // constant false 
    if t then t else t     // conditional
{% endhighlight %}

The evaluation relation $$t \longrightarrow t'$$ is the smallest relation closed under the following rules.

The following are *computation rules*, defining the "real" computation steps:

$$
\begin{align}
\text{if true then } t_2 \text{ else } t_3 \longrightarrow t_2 & \quad \text{(E-IfTrue)} \\

\text{if false then } t_2 \text{ else } t_3 \longrightarrow t_3 & \quad \text{(E-IfFalse)} \\
\end{align}
$$

The following is a *congruence rule*, defining where the computation rule is applied next:

$$
\frac{t_1 \longrightarrow t_1'}
     {\text{if } t_1 \text{ then } t_2 \text{ else } t_3 \longrightarrow \text{if } t_1' \text{ then } t_2 \text{ else } t_3} 
\text{(E-If)} 
$$

We want to evaluate the condition before the conditional clauses in order to save on evaluation; we're not sure which one should be evaluated, so we need to know the condition first.

#### Derivations
We can describe the evaluation logically from the above rules using derivation trees. Suppose we want to evaluate the following (with parentheses added for clarity): `if (if true then true else false) then false else true`.

In an attempt to make all this fit onto the screen, `true` and `false` have been abbreviated `T` and `F` in the derivation below, and the `then` keyword has been replaced with a parenthesis notation for the condition.

$$
\frac{
    \frac{
        \text{if } (T) \text{ } T \text{ else } F
        \longrightarrow
        T
        \quad \text{(E-IfT)}
    }{
        \text{if } (\text{if } (T) \text{ } T \text{ else } F) \text{ } F \text{ else } T
        \longrightarrow
        \text{if } (T) \text{ } F \text{ else } T
        \quad \text{(E-If)}
    }

    \qquad 

    \small{
        \text{if } (T) \text{ } F \text{ else } T
        \longrightarrow
        F
        \quad \text{(E-IfT)}
    }
}{
    \text{if } (\text{if } (T) \text{ } T \text{ else } F) \text{ } F \text{ else } T
    \longrightarrow
    T
}
$$

The final statement is a **conclusion**. We say that the derivation is a **witness** for its conclusion (or a **proof** for its conclusion). The derivation records all reasoning steps that lead us to the conclusion.

#### Abstract machines
An abstract machine consists of:

- A set of **states**
- A **transition** relation of states, written $$\longrightarrow$$

$$t \longrightarrow t'$$ means that $$t$$ evaluates to $$t'$$ in one step. Note that $$\longrightarrow$$ is a relation, and that $$t \longrightarrow t'$$ is shorthand for $$(t, t') \in \longrightarrow$$. Often, this relation is a partial function (not necessarily covering the domain A; there is at most one possible next state). But without loss of generality, there may be many possible next states, determinism isn't a criterion here.
