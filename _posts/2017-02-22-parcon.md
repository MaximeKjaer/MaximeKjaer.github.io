---
title: CS-206 Parallelism and Concurrency
description: "My notes from the CS-206 course at EPFL, spring semester 2017: Parallelism and Concurrency"
image: /images/hero/epfl-bc.jpg
fallback-color: "#4a4c45"
unlisted: true
edited: true
math: true
---

* TOC
{:toc}

These are my notes from the [CS-206 Parallelism and Concurrency course](http://lara.epfl.ch/w/parcon17:top). Prerequisites are:

- [Functional Programming](/funprog/)
- [Algorithms](/algorithms/)
- Computer Architecture

## Introduction
Almost every desktop, laptop, mobile device today has multiple processors; it is therefore important to learn how to harness these resources. We'll see how functional programming applies to parallelization. We'll also learn how to estimate and measure performance.

### What is parallel computing?
*Parallel computing* is a type of computation in which many calculations are performed at the same time. The basic principle is to divide the computation into smaller subproblems, each of which can be solved simultaneously. This is, of course, assuming that parallel hardware is at our disposal, with shared access to memory. Parallel programming is much harder than sequential programming, but we can get significant *speedups*.

Parallelism and concurrency are closely related concepts:

- **Parallel program**: uses parallel hardware to execute computation more quickly. It is mainly concerned with division into subproblems and optimal use of parallel hardware
- **Concurrent program**: may or may not execute multiple executions at the same time. Mainly concerned with modularity, responsiveness or maintainability (convenience).

The two often overlap; neither is the superset of the other.

Parallelism manifests itself at different granularity levels.

- **Bit-level parallelism**: processing multiple bits of data in parallel
- **Instruction-level parallelism**: executing different instructions from the same instruction stream in parallel
- **Task-level parallelism**: executing separate instruction streams in parallel

The first two are mainly implemented in hardware or in compilers; as developers, we focus on task-level parallelism.

## Parallelism on the JVM

### Definitions
A process is an instance of a program that is executing in the OS. The same program can be started as a process more than once, or even simultaneously in the same OS. The operating system *multiplexes* many different processes and a limited number of CPUs, so that they get *time slices* of execution. This
mechanism is called *multitasking*.

Two different processes cannot access each other’s memory directly &mdash; they
are isolated. Interprocess communication methods exist, but they aren't particularly straightforward.

Each process can contain multiple independent concurrency units called
*threads*. They can be started programmatically within the program, and they share the same memory address space &mdash; this allows them to exchange information by doing memory read/writes.

Each thread has a program counter and a program stack. JVM threads can't modify each other's stack memory, they can only modify the heap memory.

### Implementation
Each JVM process starts with a **main thread**. To start additional threads:

1. Define a `Thread` subclass.
2. Instantiate a new `Thread` object.
3. Call `start` on the `Thread` object.

Notice that the same class can be used to start multiple threads.

{% highlight scala linenos %}
class HelloThread extends Thread {
    override def run() {
        println("Hello world!")
    }
}

val t = new HelloThread // new thread instance

t.start() // start thread
t.join() // wait for its completion
{% endhighlight %}

`t.join()` blocks the main thread's execution until the `t` thread is done executing.

Let's look at a more complex example:

{% highlight scala linenos %}
class HelloThread extends Thread {
    override def run() {
        println("Hello")
        println("world!")
    }
}
def main() {
    val t = new HelloThread
    val s = new HelloThread

    t.start()
    s.start()
    t.join()
    s.join()
}
{% endhighlight %}

Running it multiple times might yield the following output:

{% highlight text linenos %}
Hello
world!
Hello
world!

Hello
world!
Hello
world!

Hello
Hello
world!
world!
{% endhighlight %}

On the first two executions, the threads happened to execute linearly; first `t`, then `s`. But on the third attempt, the first thread printed `Hello`, but then the second thread kicked in, also printed `Hello` &mdash; before the first had time to print out `world!`, and then they both completed.


## Atomicity
The above shows that **two parallel threads can overlap arbitrarily**. However, we sometimes want to ensure that a sequence of statements executes at once, as if they were just one statement, meaning that we don't want them to overlap. This is called atomicity.

An operation is *atomic* if it appears as if it occurred instantaneously from the point of view of other threads.

The implementation of `getUniqueId()` below isn't atomic, as it suffers from the same problem as the hello world example above.

{% highlight scala linenos %}
private var uidCount = 0L // 0 as a long
def getUniqueId(): Long = {
    uidCount = uidCount + 1
    uidCount
}
{% endhighlight %}

### Synchronized blocks
How can we secure it from this problem? How do we get it to execute atomically?

{% highlight scala linenos %}
private val x = new AnyRef {}
private var uidCount = 0L
def getUniqueId(): Long = x.synchronized {
    uidCount = uidCount + 1
    uidCount
}
{% endhighlight %}

The `synchronized` block is used to achieve atomicity. Code blocks after a `synchronized` call on an object `x` are never executed on two threads at once. The JVM ensures this by storing an object called the *monitor* in each object. At most one thread can own the monitor at any particular time, and releases it when it's done executing.

`synchronized` blocks can even be nested.

{% highlight scala linenos %}
class Account(private var amount: Int = 0) {
    def transfer(target: Account, n: Int) =
        this.synchronized { // synchronized block on source account
            target.synchronized { // and on target account
                this.amount -= n
                target.amount += n
        }
    }
}
{% endhighlight %}

This way, the thread gets a monitor on account A, and then on account B. Once it has monitors on both, it can transfer the amount from A to B. Another thread can do this with C and D in parallel.

### Deadlocks
Sometimes though, this may cause the code to freeze, or to *deadlock*. This is a scenario in which two or more threads compete for resources (such as monitor ownership) and wait for each to finish without releasing
the already acquired resources.

The following code should cause a deadlock:

{% highlight scala linenos %}
val a = new Account(50)
val b = new Account(70)

// thread T1
a.transfer(b, 10)

// thread T2
b.transfer(a, 10)
{% endhighlight %}

`T1` gets the monitor for `a`, `T2` gets the monitor for `b`. Then they both wait for each other to release the monitor, leaving us in a deadlock.

#### Resolving deadlocks
One approach is to always acquire resources in the same order. This assumes an ordering relationship on the resources. In our example, we can simply assign unique IDs on the accounts, and order our `synchronized` calls according to this ID.

{% highlight scala linenos %}
val uid = getUniqueUid()
private def lockAndTransfer(target: Account, n: Int) =
    this.synchronized {
        target.synchronized {
            this.amount -= n
            target.amount += n
        }
    }

def transfer(target: Account, n: Int) =
    if (this.uid < target.uid) this.lockAndTransfer(target, n)
    else target.lockAndTransfer(this, -n)
{% endhighlight %}

### Memory model
A *memory model* is a set of rules describing how threads interact when accessing shared memory. Java Memory Model is the memory model for the JVM. There are many rules, but the ones we chose to remember in the context of this course are:

1. Two threads writing to separate locations in memory do not need synchronization.
2. A thread X that calls `join` on another thread Y is guaranteed to observe all the writes by thread Y after `join` returns. Note that if we don't call `join`, there's no guarantee that X will see any of Y's changes when it reads in memory.

We will not be using threads and the `synchronized` primitive directly in the remainder of the course. However, the methods in the course are based on these, and knowledge about them is indeed useful.

## Running computations in parallel
How can we run the following code in parallel?

{% highlight scala linenos %}
def pNormTwoPart(a: Array[Int], p: Double): Int = {
    val m = a.length / 2
    val (sum1, sum2) = (sumSegment(a, p, 0, m),
                        sumSegment(a, p, m, a.length))
    power(sum1 + sum2, 1/p)
}
{% endhighlight %}

We just add `parallel`!

{% highlight scala linenos %}
def pNormTwoPart(a: Array[Int], p: Double): Int = {
    val m = a.length / 2
    val (sum1, sum2) = parallel(sumSegment(a, p, 0, m),
                                sumSegment(a, p, m, a.length))
    power(sum1 + sum2, 1/p)
}
{% endhighlight %}

Recursion works very well with parallelism. We can for instance spin up an arbitrary number of threads:

{% highlight scala linenos %}
def pNormRec(a: Array[Int], p: Double): Int =
    power(segmentRec(a, p, 0, a.length), 1/p)

// like sumSegment but parallel
def segmentRec(a: Array[Int], p: Double, s: Int, t: Int) = {
    if (t - s < threshold)
        sumSegment(a, p, s, t) // small segment: do it sequentially
    else {
        val m = s + (t - s)/2
        val (sum1, sum2) = parallel(segmentRec(a, p, s, m),
                                    segmentRec(a, p, m, t))
        sum1 + sum2
    }
}
{% endhighlight %}

#### Signature of parallel

{% highlight scala linenos %}
def parallel[A, B](taskA: => A, taskB: => B): (A, B) = { ... }
{% endhighlight %}

It returns the same value as it is given, but can do it faster than its sequential counterpart. From the point of view of the value, it is an identity function. Its arguments are taken *by name* (CBN); otherwise it wouldn't be able to do much with them, as they would be evaluated sequentially before being sent to `parallel`. `parallel` needs the unevaluated computations to function, thus CBN.

### Underlying hardware architecture affects performance
Sometimes, we do not achieve any speedup even though we ran computations in parallel. For instance, if we sum up array elements instead of summing *powers* of array elements like above, we don't see any speedups using parallelism. This is because this computation is bound by the memory bandwidth, which acts as a bottleneck to any speedup. 

Therefore, when considering opportunities for speed-up, we must take into account not only the number of cores, but also the parallelism available for any other shared resources that we might need in order to perform computation, such as memory in this case. 

In general, parallel computation takes as long as its slowest / longest thread.

## Tasks
Instead of invoking threads, we can use a more flexible construct for parallel computation: tasks.

{% highlight scala linenos %}
val t1 = task(e1)
val t2 = task(e2)
val v1 = t1.join
val v2 = t2.join
{% endhighlight %}

`t = task(e)` starts a computation "in the background"; the main thread continues while the task is running (unless we use `join` in which case it waits). Tasks are easier to use; instead of this mess with nested calls to `parallel`:

{% highlight scala linenos %}
val ((part1, part2),(part3,part4)) =
    parallel(parallel(sumSegment(a, p, 0, mid1),
                      sumSegment(a, p, mid1, mid2)),
             parallel(sumSegment(a, p, mid2, mid3),
                      sumSegment(a, p, mid3, a.length)))
power(part1 + part2 + part3 + part4, 1/p)
{% endhighlight %}

We can easily get 4 tasks by doing:

{% highlight scala linenos %}
val t1 = task {sumSegment(a, p, 0, mid1)}
val t2 = task {sumSegment(a, p, mid1, mid2)}
val t3 = task {sumSegment(a, p, mid2, mid3)}
val t4 = task {sumSegment(a, p, mid3, a.length)}
power(t1 + t2 + t3 + t4, 1/p)
{% endhighlight %}

We don't call `join`, it's implicit?

## How do we measure performance?

### Asymptotic analysis
Asymptotic analysis allows us to understand how the runtime of our algorithm changes when the inputs get larger or when we have more parallel hardware available. Just like in [Algorithms](/algorithms/), we consider the worst case to get an upper bound using big-O notation.

*INSERT NOTES HERE WHEN RELEVANT*

https://www.coursera.org/learn/parprog1/lecture/OjNsc/how-fast-are-parallel-programs

#### Work and depth
We introduce two measures for a program:

- **Work** `W(e)`: number of steps `e` would take if there was no parallelism. This is simply the sequential execution time
- **Depth** `D(e)`: number of steps if we had unbounded parallelism

The key rules are:

- $$W(parallel(e_1, e_2)) = W(e_1) + W(e_2) + c_2$$.
- $$D(parallel(e_1, e_2)) = max(D(e_1), D(e_2)) + c_1$$.

For parts of code where we do not use parallel explicitly, we must add up
costs. For function call or operation $$f(e_1, ..., e_n)$$:

- $$W(f(e_1, ..., e_n)) = W(e_1) + ... + W(e_n) + W(f)(v_1, ..., v_n)$$.
- $$D(f(e_1, ..., e_n)) = D(e_1) + ... + D(e_n) + D(f)(v_1, ..., v_n)$$.

Here $$v_i$$ denotes values of $$e_i$$. If $$f$$ is a primitive operation on integers, then $$W(f)$$ and $$D(f)$$ are constant functions, regardless of $$v_i$$.

Suppose we know $$W(e)$$ and $$D(e)$$ and our platform has *P* parallel threads. It is reasonable to use this estimate for running time:

$$D(e) + \frac{W(e)}{P}$$


*Insert Amdahl's Law*


### Empirical analysis: Benchmarking
Measuring performance is difficult. To ensure somewhat reliable results, we need a strict measurement methodology involving:

- Multiple repetitions
- Statistical treatment – computing mean and variance
- Eliminating outliers
- Ensuring steady state (warm-up)
- Preventing anomalies (GC, JIT compilation, aggressive optimizations)

This is all quite complex, so we use a tool to do it for us: ScalaMeter. To use it, we first need to add it as a dependency in `build.sbt`:

{% highlight scala linenos %}
libraryDependencies += 
    "com.storm-enroute" %% "scalameter-core" % "0.6"
{% endhighlight %}

Then we can use it as such:

{% highlight scala linenos %}
import org.scalameter._

val time = measure {
    (0 until 1000000).toArray
}

println(s"Array initialization time: $time ms")
{% endhighlight %}

But this yields unreliable results due to garbage collection and dynamic optimization and stuff like that. Running it multiple times can yield anything from  7 to 50ms. We notice that the program runs in about 7ms after a few runs; this is the *JVM Warmup*.

ScalaMeter can ensure that warm-up has taken place if we do:

{% highlight scala linenos %}
import org.scalameter._

val time = withWarmer(new Warmer.Default) measure {
    (0 until 1000000).toArray
}
{% endhighlight %}

If we are not entirely satisfied with the defaults settings of ScalaMeter, we can change them as such:

{% highlight scala linenos %}
val time = config(
    Key.exec.minWarmupRuns -> 20,
    Key.exec.maxWarmupRuns -> 60,
    Key.verbose -> true // increase verbosity
) withWarmer(new Warmer.Default) measure {
    (0 until 1000000).toArray
}
{% endhighlight %}

Finally, ScalaMeter can measure more than just the running time:

- `Measurer.Default`: plain running time
- `IgnoringGC`: running time without GC pauses
- `OutlierElimination`: removes statistical outliers
- `MemoryFootprint`: memory footprint of an object
- `GarbageCollectionCycles`: total number of GC pauses

## Parallelizing important algorithms

### Parallel merge sort
As we mentioned in Algorithms, [Merge Sort](/algorithms/#merge-sort) works very well in parallel. We'll now see how to do it in parallel. In order to do that we're going to use two arrays, one of which is going to be a temporary array. And we are going to be copying elements between the temporary array `ys`and the original array `xs`.

{% highlight scala linenos %}
def sort(from: Int, until: Int, depth: Int): Unit = {
    if (depth == maxDepth) { // base case
        quickSort(xs, from, until - from)
    } else { // recursively parallelize
        // Divide
        val mid = (from + until) / 2
        parallel(sort(mid, until, depth + 1), sort(from, mid, depth + 1))
        // Merge two sorted sublists
        val flip = (maxDepth - depth) % 2 == 0
        val src = if (flip) ys else xs
        val dst = if (flip) xs else ys
        merge(src, dst, from, mid, until)
    }
}

def merge(src: Array[Int], dst: Array[Int], 
          from: Int, mid: Int, until: Int): Unit

sort(0, xs.length, 0)
{% endhighlight %}

The `merge` implementation is sequential, so we will not go through it. Benchmarking this parallel merge sort to the Scala `quicksort` implementation shows up to a two-fold speedup in practice.

#### Copying array in parallel
To copy the temporary array into the original one, we need an optimized algorithm: 

{% highlight scala linenos %}
def copy(src: Array[Int], target: Array[Int],
from: Int, until: Int, depth: Int): Unit = {
    if (depth == maxDepth) {
        Array.copy(src, from, target, from, until - from)
    } else {
        val mid = (from + until) / 2
        val right = parallel(
            copy(src, target, mid, until, depth + 1),
            copy(src, target, from, mid, depth + 1)
        )
    }
}
if (maxDepth % 2 == 0) copy(ys, xs, 0, xs.length, 0)
{% endhighlight %}

### Parallel map
Some operations we saw in the previous course were [map](/funprog/#map), [fold](/funprog/#reduce) and scan (like `fold` but stores intermediate results).

Lists aren't terribly efficient, as splitting them in half and combining them take linear time. As alternatives, we'll use *arrays* and *trees* in our implementation. We'll see more about Scala’s parallel collection libraries in future lectures.

{% highlight scala linenos %}
def mapASegPar[A,B](inp: Array[A], left: Int, right: Int, f : A => B,
                    out: Array[B]): Unit = {
    // Writes to out(i) for left <= i <= right-1
    if (right - left < threshold)
        mapASegSeq(inp, left, right, f, out) // assuming a sequential implementation has been defined as such
    else {
        val mid = left + (right - left)/2
        parallel(mapASegPar(inp, left, mid, f, out),
                 mapASegPar(inp, mid, right, f, out))
    }
}
{% endhighlight %}

Parallelization yields 5x or 6x speedup in certain benchmarks. From the benchmarks we can also tell that the parallelized `map` is basically as efficient as specialized implementations of operations in parallel.

If we use trees instead of arrays: 

{% highlight scala linenos %}
def mapTreePar[A:Manifest,B:Manifest](t: Tree[A], f: A => B) : Tree[B] =
    t match {
        case Leaf(a) => { // base case
            val len = a.length
            val b = new Array[B](len)
            var i= 0
            while (i < len) {
                b(i)= f(a(i))
                i= i + 1
            }
            Leaf(b)
        }
        case Node(l,r) => { // recursive parallelization
            val (lb,rb) = parallel(mapTreePar(l,f), mapTreePar(r,f))
            Node(lb, rb) // combine computations into new node
        }
    }
{% endhighlight %}

#### Comparison of arrays and immutable trees
<p></p>

**Arrays**:

- (+) random access to elements, on shared memory can share array
- (+) good memory locality
- (-) imperative: must ensure parallel tasks write to disjoint parts
- (-) expensive to concatenate

**Immutable trees**:

- (+) purely functional, produce new trees, keep old ones
- (+) no need to worry about disjointness of writes by parallel tasks
- (+) efficient to combine two trees
- (-) high memory allocation overhead
- (-) bad locality


### Parallel reduce
For reduce (or fold), the order of operation matters. When we process the elements in parallel, we must therefore impose that the operation be **associative**, meaning that the order doesn't matter.  Examples of associative operations include addition or concatenation of strings, but not subtraction.

An operation `f: (A, A) => A` is associative *if and only if* for every `x, y, z`, `f(x, f(y, z)) == f(f(x, y), z)`.

We can represent the reduction as an operation tree, where every node corresponds to a single operation (say, addition or concatenation). If `t1` and `t2` are different tree representations of the same reduction (so they correspond to the same reduction, but in a different order), and `f: (A, A) => A` is associative, then:

{% highlight scala linenos %}
reduce(t1, f) == reduce(t2, f)
{% endhighlight %}

If we want to implement `reduce` for arrays instead of trees, we can just conceptually consider arrays as trees by cutting them in half at every step (until a certain `threshold` size):

{% highlight scala linenos %}
def reduceSeg[A](inp: Array[A], left: Int, right: Int, f: (A,A) => A): A = {
    if (right - left < threshold) {
        var res= inp(left); var i= left+1
        while (i < right) {
            res= f(res, inp(i))
            i= i+1
        }
        res
    } else {
        val mid = left + (right - left)/2
        val (a1,a2) = parallel(reduceSeg(inp, left, mid, f),
                               reduceSeg(inp, mid, right, f))
        f(a1,a2)
    }
}

def reduce[A](inp: Array[A], f: (A,A) => A): A =
    reduceSeg(inp, 0, inp.length, f)
{% endhighlight %}

### Associative and/or commutative operations

*Associative* **and** *commutative* operations: 

- Addition and multiplication of mathematical integers (`BigInt`) and of
exact rational numbers (given as, e.g., pairs of `BigInts`)
- Addition and multiplication modulo a positive integer (e.g. 232),
including the usual arithmetic on 32-bit `Int` or 64-bit `Long` values
- Union, intersection, and symmetric difference of sets
- Union of bags (multisets) that preserves duplicate elements
- Boolean operations `&&`, `||`, `xor`
- Addition and multiplication of polynomials
- Addition of vectors
- Addition of matrices of fixed dimension

*Associative* **but not** *commutative* operations: 

- Concatenation (append) of lists: `(x ++ y) ++ z == x ++ (y ++ z)`
- Concatenation of `Strings` (which can be viewed as lists of `Char`)
- Matrix multiplication AB for matrices A and B of compatible dimensions
- Composition of relations $$r \odot s = \left\{(a,c) : \exists b : (a,b)\in r \wedge (b, c)\in s\right\}$$
- Composition of functions $$(f \circ g)(x) = f(g(x))$$

Many operations *Commutative* **but not** *associative*, such as $$f(x, y) = x^2 + y^2$$. Interestingly, addition or multiplication of floating point numbers is commutative, but not associative. This is because of floating point errors (where they're off by `0.000...01`), so we don't always have 

{% highlight scala linenos %}
(x + mx) + e == x + (mx + e)
{% endhighlight %}

As a conclusion, proving commutativity alone does not prove associativity. Another thing to look out for is that associativity is not preserved by mapping; when combining and optimizing `reduce` and `map` invocations, we need to be careful that operations given to `reduce` remain associative.

#### Making an operation commutative is easy
Suppose we have a binary operation `g` and a strict total ordering `less` (e.g. lexicographical ordering of bit representations). Then this operation is commutative:

{% highlight scala linenos %}
def f(x: A, y: A) = if (less(y,x)) g(y,x) else g(x,y)
{% endhighlight %}

There is no such trick for associativity, though.

#### Constructing associative operations
Suppose `f1: (A1,A1) => A1` and `f2: (A2,A2) => A2` are associative.

Then `f: ((A1,A2), (A1,A2)) => (A1,A2)` defined by 

{% highlight scala linenos %}
f((x1,x2), (y1,y2)) = (f1(x1,y1), f2(x2,y2))
{% endhighlight %}

is also associative.

The following functions are also associative:

{% highlight scala linenos %}
times((x1,y1), (x2, y2)) = (x1*x2, y1*y2)

// Calculating average
val sum = reduce(collection, _ + _)
val length = reduce(map(collection, (x:Int) => 1), _ + _)
sum/length

// Equivalently
val (sum, length) = reduce(map(collection, (x:Int) => (x,1)), f)
sum/length
{% endhighlight %}


There are some situations where commutativity can help us establish associativity, but we need some additional property. Let:

{% highlight scala linenos %}
E(x,y,z) = f(f(x,y), z)
{% endhighlight %}

We say arguments of `E` can rotate if:
{% highlight scala linenos %}
E(x,y,z) = E(y,z,x)
// equivalent to
f(f(x,y), z) = f(f(y,z), x)
{% endhighlight %}

If the above function `f` is commutative and the arguments if E can rotate, **then `f` is also associative**. Proof:

{% highlight scala linenos %}
f(f(x,y), z) = f(f(y,z), x) = f(x, f(y,z))
{% endhighlight %}

We can use this to prove associativity for the following examples:

{% highlight scala linenos %}
plus((x1,y1), (x2, y2)) = (x1*y2 + x2*y1, y1*y2)
{% endhighlight %}

Again, we should be wary of floating point numbers in proving associativity!

### Parallel scan
Sequentially, `scanLeft` can be implemented as:

{% highlight scala linenos %}
def scanLeft[A](inp: Array[A], a0: A,
                f: (A,A) => A, out: Array[A]): Unit = {
    out(0) = a0
    var a = a0
    var i = 0
    while (i < inp.length) {
        a = f(a,inp(i))
        i = i + 1
        out(i) = a
    }
}
{% endhighlight %}

Can this be made parallel? We'll assume `f` is associative. Our goal is to have an $$O(\log{n})$$ algorithm (given infinite parallelism).

At first, this task seems almost impossible, because the value of the last element in sequence is computed from the previous element. And for every element, it looks like the natural way is indeed what we gave in the sequential algorithm. But even if we parallelize the individual applications of `f`, we would not be able to parallelize the traversal of the array itself. So this would give us still a linear algorithm even with infinite parallelism. 

So, we need to perform computation in a different order, the idea is to give up reusing all intermediate results. And in fact, we will do more work and more applications of f that need the simple sequential version. However, this will allow us to improve parallelism and in terms of the parallel running time, more than compensate for the fact that we are applying f a few more times than in the sequential algorithm. 

To show that this is even possible in parallel, here's how we'd define it in terms of the parallel `map` and `reduce`:

{% highlight scala linenos %}
def scanLeft[A](inp: Array[A], a0: A, f: (A,A) => A, out: Array[A]) = {
    val fi = { (i:Int,v:A) => reduceSeg1(inp, 0, i, a0, f) }
    mapSeg(inp, 0, inp.length, fi, out)
    val last = inp.length - 1
    out(last + 1) = f(out(last), inp(last))
}
{% endhighlight %}

#### On trees
Let's implement `scanLeft` on trees.

{% highlight scala linenos %}
def upsweep[A](t: Tree[A], f: (A,A) => A): TreeRes[A] = t match {
    case Leaf(v) => LeafRes(v)
    case Node(l, r) => {
        val (tL, tR) = parallel(upsweep(l, f), upsweep(r, f))
        NodeRes(tL, f(tL.res, tR.res), tR)
    }
}

// ’a0’ is reduce of all elements left of the tree ’t’
def downsweep[A](t: TreeRes[A], a0: A, f : (A,A) => A): Tree[A] = t match {
    case LeafRes(a) => Leaf(f(a0, a))
    case NodeRes(l, _, r) => {
        val (tL, tR) = parallel(downsweep[A](l, a0, f),
                                downsweep[A](r, f(a0, l.res), f))
        Node(tL, tR)
    }
}

def prepend[A](x: A, t: Tree[A]): Tree[A] = t match {
    case Leaf(v) => Node(Leaf(x), Leaf(v))
    case Node(l, r) => Node(prepend(x, l), r)
}

def scanLeft[A](t: Tree[A], a0: A, f: (A,A) => A): Tree[A] = {
    val tRes = upsweep(t, f)
    val scan1 = downsweep(tRes, a0, f)
    prepend(a0, scan1)
}
{% endhighlight %}

Here's how downsweep works:

![Gif of downsweep](/images/parcon/downsweep.gif)

#### On arrays

{% highlight scala linenos %}
def upsweep[A](inp: Array[A], from: Int, to: Int,
               f: (A,A) => A): TreeResA[A] = {
    if (to - from < threshold)
        Leaf(from, to, reduceSeg1(inp, from + 1, to, inp(from), f))
    else {
        val mid = from + (to - from)/2
        val (tL,tR) = parallel(upsweep(inp, from, mid, f),
                               upsweep(inp, mid, to, f))
        Node(tL, f(tL.res,tR.res), tR)
    }
}

def reduceSeg1[A](inp: Array[A], left: Int, right: Int,
                  a0: A, f: (A,A) => A): A = {
    var a = a0
    var i = left
    while (i < right) {
        a = f(a, inp(i))
        i = i+1
    }
    a
}

def downsweep[A](inp: Array[A], a0: A, f: (A,A) => A,
                 t: TreeResA[A], out: Array[A]): Unit = t match {
    case Leaf(from, to, res) =>
    scanLeftSeg(inp, from, to, a0, f, out)
    case Node(l, _, r) => {
    val (_,_) = parallel(
        downsweep(inp, a0, f, l, out),
        downsweep(inp, f(a0,l.res), f, r, out))
    }
}
def scanLeftSeg[A](inp: Array[A], left: Int, right: Int,
                   a0: A, f: (A,A) => A, out: Array[A]) = {
    if (left < right) {
        var i = left
        var a = a0
        while (i < right) {
            a = f(a, inp(i))
            i = i+1
            out(i) = a
        }
    }
}

def scanLeft[A](inp: Array[A], a0: A, f: (A,A) => A,
                out: Array[A]) = {
    val t = upsweep(inp, 0, inp.length, f)
    downsweegp(inp, a0, f, t, out) // fills out[1..inp.length]
    out(0) = a0 // prepends a0
}
{% endhighlight %}

## Data parallelism
So far, we've learned about task-parallel programming:

> A form of parallelization that distributes execution processes across computing nodes.

In Scala, we express this with `task` and `parallel`.

Data-parallel programs have a different approach:

> A form of parallelization that distributes data across computing nodes.

Why would we want to use data-parallelism? It can be much faster than task parallelism (in the demo with Mandelbrot sets, it was 2x faster).

### Workload
Why are data parallel programs sometimes faster than task parallel ones?

Different data-parallel programs have different workloads. **Workload** is a function that maps each input element to the amount of work required to process it. 

`initializeArray` had a workload defined by a constant function, $$w(i) = \text{const}$$. We call this **uniform workload**, and it's really easy to parallelize.

An **irregular workload** is one where the work is described by an arbitrary function: $$w(i) = f(i)$$. This is where we have a **data-parallel scheduler**, whose role is to efficiently balance the workload across processors without any knowledge about the $$w(i)$$. The idea of the scheduler is to shift away the task of balancing the workload from the programmer. They have similar semantics, so we won't study them in detail.

### Parallel for-loop
To initialize arrays with a given value in parallel (writing `v` to every position in `xs`)

{% highlight scala linenos %}
def initializeArray(xs: Array[Int])(v: Int): Unit = {
    for (i <- (0 until xs.length).par) {
        xs(i) = v
    }
}
{% endhighlight %}

Here, the `.par` method converts the range to a parallel range; the for loop will be executed in parallel. Parallel for-loops are not functional, do not return a value, and can therefore only communicate with the rest of the program through some side effect, such as writing to an array. Therefore, the parallel for-loop must write to separate memory locations or be synchronized in order to work.


### Non-parallelizable operations
In general terms, most sequential collections can be converted to parallel collections by using `.par`; *some* collection operations subsequently applied become data-parallel, but not all. Let's look at an example:

{% highlight scala linenos %}
def sum(xs: Array[Int]): Int = {
    xs.par.foldLeft(0)(_ + _)
}
{% endhighlight %}

This **does not** execute in parallel, as `foldLeft` has no way of *not* processing elements sequentially; `foldRight`, `reduceLeft`, `reduceRight`, `scanLeft` and `scanRight` similarly must process the elements sequentially and operate sequentially on parallel collections.


### Parallelizable operations
However, `fold` (without any direction) can process elements in parallel (see LEGO bricks explanation in lecture videos). Our previous `sum` function, and `max` could be written like this instead:

{% highlight scala linenos %}
def sum(xs: Array[Int]): Int = {
    xs.par.fold(0)(_ + _)
}

def max(xs: Array[Int]): Int = {
    xs.par.fold(Int.MinValue)(math.max)
}
{% endhighlight %}

It is important to note that `fold` will with a function `f` if:

- `f` is an *associative* operation
- When applied to the neutral element `z`, it must act as an identity function.

In other words, the following relations must hold:

{% highlight scala linenos %}
f(a, f(b, c)) == f(f(a, b), c)
f(z, a) == f(a, z) == a
{% endhighlight %}

In more formal terms, the neutral element *z* and the binary operator `f` must form a *monoid*. Commutativity is not important for `fold`, but it is important that the neutral element `z` be of the same type as the collection items (unlike `foldLeft`); this is clear if we look at the signature of `fold`:

{% highlight scala linenos %}
def fold(z: A)(f: (A, A) => A): A
{% endhighlight %}

This seems like a lot of limitations, so we'll need a more powerful data parallel operation. Enter `aggregate`:

{% highlight scala linenos %}
def aggregate[B](z: B)(f: (B, A) => B, g: (B, B) => B): B
{% endhighlight %}

What it does is divide the collection into pieces, applying the sequential folding operator `f` and combine results using the parallel folding operator `g`. Using it, we can do what we couldn't do with `fold`: count the number of vowels in a character array:

{% highlight scala linenos %}
Array(‘E‘, ‘P‘, ‘F‘, ‘L‘).par.aggregate(0)(
    (count, c) => if (isVowel(c)) count + 1 else count,
    _ + _
)
{% endhighlight %}

Again, the parallel reduction operator `g` and the neutral element `z` should form a monoid. Note that this is *just* an "if", not "if and only if"; the only iff condition for `aggregate` to work is:

{% highlight scala linenos %}
(x1 ++ x2).foldLeft(z)(f) == g(x1.foldLeft(z)(f), x2.foldLeft(z)(f))
{% endhighlight %}

<!-- We did this in group exercises. Martin Odersky came by at that moment, and joked "if it type checks, it's correct" -->

Many other parallel collection operations can be expressed in terms of `aggregate`. So far we've only seen *accessor* combinators (`sum`, `max`, `fold`, `count`, `aggregate`, ...). *Transformer* combinators (such as `map`, `filter`, `flatMap`, `groupBy`) do not return a single value, but instead return new collections as a result.

### Parallel collections
In sequential collections, the hierarchy is as follows:

- `Traversable[T]`: collection of elements with type `T`, with operations
implemented using foreach
    - `Iterable[T]`: collection of elements with type `T`, with operations
implemented using iterator
        - `Seq[T]`: an ordered sequence of elements with type `T`
        - `Set[T]`: a set of elements with type `T` (no duplicates)
        - `Map[K, V]`: a map of keys with type K associated with values of type
V (no duplicate keys)

Traits `ParIterable[T]`, `ParSeq[T]`, `ParSet[T]` and `ParMap[K, V]` are the
parallel counterparts of different sequential traits. For code that is *agnostic* about parallelism, there exists a separate hierarchy of generic collection traits `GenIterable[T]`, `GenSeq[T]`, `GenSet[T]` and `GenMap[K, V]`.

![Hierarchy of generic collections](/images/parcon/generic_collections.png)

Using these generic collections, operations may or may not execute in parallel:

{% highlight scala linenos %}
def largestPalindrome(xs: GenSeq[Int]): Int = {
    xs.aggregate(Int.MinValue)((largest, n) =>
        if (n > largest && n.toString == n.toString.reverse) n else largest,
        math.max
    )
}
val array = (0 until 1000000).toArray

largestPalindrome(array) // sequential
largestPalindrome(array.par) // parallel
{% endhighlight %}

In practice, parallelizable collections are:

- `ParArray[T]`: parallel array of objects, counterpart of Array and
ArrayBuffer
- `ParRange`: parallel range of integers, counterpart of Range
- `ParVector[T]`: parallel vector, counterpart of Vector
- `immutable.ParHashSet[T]`: counterpart of immutable.HashSet
- `immutable.ParHashMap[K, V]`: counterpart of immutable.HashMap
- `mutable.ParHashSet[T]`: counterpart of mutable.HashSet
- `mutable.PasHashMap[K, V]`: counterpart of mutable.HashMap
- `ParTrieMap[K, V]`: thread-safe parallel map with atomic snapshots,
counterpart of `TrieMap`
- for other collections, par creates the closest parallel collection: e.g. a
List is converted to a ParVector

The last point stresses the importance of picking data structures carefully and making sure that they are parallelizable; otherwise, the conversion might take longer than the parallel instructions themselves.

#### Avoiding parallel errors
As we've said previously, one should either synchronize or write to separate memory locations. To synchronize, we can use the Java `ConcurrentSkipListSet[T]` instead of Scala mutable Set. To avoid side-effects, we can use the right combinators (for instead, use `filter` instead of making your own code).

A rule to avoid concurrent modifications during traversals is to *never* read or write to a parallel collection on which a data-parallel operation is in progress.

The `TrieMap` collection is an exception to this; it atomically takes snapshots whenever a parallel operation starts, so concurrent updates aren't observed during that time. It offers the `snapshot` method (efficient: constant time), which can be used to efficiently grab the current state.

## Data-parallel abstractions
Transformer operations are collection operations that create another collection instead of a single value. Methods such as `filter`, `map`, `flatMap`, `groupBy` are examples of transformer operations.

### Iterators
{% highlight scala linenos %}
trait Iterator[A] {
    def next(): A
    def hasNext: Boolean
}
def iterator: Iterator[A] // on every collection
{% endhighlight %}

The *iterator contract* states that:

- `next` can be called only if `hasNext` returns `true`. That means that when defining `next`, one should always call `hasNext`.
- After `hasNext` returns `false`, it will always return `false`

### Splitters
{% highlight scala linenos %}
trait Splitter[A] extends Iterator[A] {
    def split: Seq[Splitter[A]]
    def remaining: Int
}
def splitter: Splitter[A] // on every parallel collection
{% endhighlight %}

The *splitter contract* states that:

- After calling `split`, the original splitter is left in an undefined state
- The resulting splitters traverse disjoint subsets of the original splitter
- `remaining` is an estimate on the number of remaining elements
- `split` is an efficient method – $$O(\log{n})$$ or better (since we invoke it in parallel in hopes of obtaining a speedup)

### Builders
Builders are abstractions for creating new **sequential** collections. `T` denotes the type of the elements of the collection (e.g. `String`), and `Repr` is the type of the resulting collection of elements (e.g. `Seq[String]`).

{% highlight scala linenos %}
trait Builder[A, Repr] {
    def +=(elem: A): Builder[A, Repr] // add element to the builder
    def result: Repr // obtain collection after all elements are added 
}

def newBuilder: Builder[A, Repr] // on every collection
{% endhighlight %}

The *builder contract* states that:

- Calling `result` returns a collection of type `Repr`, containing the elements that were previously added with `+=`
- Calling `result` leaves the Builder in an undefined state (after this we cannot use it anymore)

### Combiners
A combiner is a **parallel** version of a builder. It has the same `+=` and `result` methods as it `extends Builder`, but adds a method `combine` to merge two combiners (invalidating the two old combiners in the process).

{% highlight scala linenos %}
trait Combiner[A, Repr] extends Builder[A, Repr] {
    def combine(that: Combiner[A, Repr]): Combiner[A, Repr]
}

def newCombiner: Combiner[T, Repr] // on every parallel collection
{% endhighlight %}

The *combiner contract* states that:

- Calling `combine` returns a new combiner that contains elements of input combiners
- Calling `combine` leaves both original `Combiners` in an undefined state
- `combine` is an efficient method – $$O(\log{n})$$ or better

#### Implementing combiners
How do we implement it efficiently ($$O(\log{n}+\log{m})$$)? We'll see that this depends on the underlying data structure; `Repr` could be a map, a set or a sequence, and that changes everything:

- When `Repr` is a set or a map, `combine` represents **union**
- When `Repr` is a sequence, `combine` represents **concatenation**

For **arrays**, there is no efficient `combine` operation. This has to do with how arrays are stored in memory; the two subarrays may be stored in different locations, which implies having to move one to the end of another; this cannot be done in logarithmic time, only in linear time.

For **sets**, we can use different data structures, that may have different runtimes for lookup, insertion and deletion:

- Hash tables: expected $$O(1)$$
- Balanced trees: $$O(\log{n})$$
- Linked lists: $$O(\log{n})$$

Unfortunately, most set implementations do not have an efficient union operation, so `combine` is tricky to implement. 

For **sequences**, there are also a few data structures, with which the operation complexity varies.

- Mutable linked lists: $$O(1)$$ prepend and append, $$O(n)$$ insertion
- Functional (cons) lists: $$O(1)$$ prepend, everything else $$O(n)$$
- Array lists: amortized $$O(1)$$ append, $$O(1)$$ random access, otherwise $$O(n)$$

*(Amortized means that we may need to copy the array to another location when we need more space, but overall we expect it to be constant time on average.)*

Mutable linked lists can have $$O(1)$$ concatenation, but for most sequences, concatenation is $$O(n)$$.

All of the above shows that providing a combiner for the corresponding collections is not straightforward, since most parallel data structures do not have the efficient union or concatenation operation we want &mdash; but it is still possible to implement.

#### Two-phase construction
Most data structures can be constructed in parallel using *two-phase* construction. Here, the combiner doesn't use the final data structure in its internal representation; it uses an intermediate data structure that:

- Has an efficient `combine` method: $$O(\log{n}+\log{m})$$ or better
- Has an efficient `+=` method (this ensures that individual processors can efficiently modify the data structure)
- Can be converted to the resulting data structure in $$O(n/P)$$ time (*n* size of data structure, *P* number of processors)

Let's implement it for arrays:

{% highlight scala linenos %}
class ArrayCombiner[T <: AnyRef: ClassTag](val parallelism: Int) { 
    // parallelism = parallelism level
    private var numElems = 0 // # of elements in the combiner
    private val buffers = new ArrayBuffer[ArrayBuffer[T]] // nested ArrayBuffer
    buffers += new ArrayBuffer[T]

    def +=(x: T) = { // Amortized O(1)
        // if the ArrayBuffer ever gets full,
        // it is expanded to accommodate more elements
        buffers.last += x
        numElems += 1
        this
    }

    def combine(that: ArrayCombiner[T]) = { // O(P) since there are P array combiners
        buffers ++= that.buffers // copies references
        numElems += that.numElems
        this
    }

    def result: Array[T] = {
        val array = new Array[T](numElems)
        val step = math.max(1, numElems / parallelism)
        val starts = (0 until numElems by step) :+ numElems
        val chunks = starts.zip(starts.tail)
        val tasks = for ((from, end) <- chunks) yield task {
           copyTo(array, from, end)
        }
        tasks.foreach(_.join())
        array
    }
}
{% endhighlight %}

Benchmarks show over 2x speedups with 4 (not linear because of the memory access bottleneck). So for arrays:

1. partition the indices into subintervals
2. initialize the array in parallel

For hash tables:

1. partition the hash codes into buckets (e.g. linked lists of arrays) according to their hashcode prefix
2. allocate the table, and map hash codes from different buckets into different regions

For search trees: 

1. partition the elements into non-overlapping intervals according to
their ordering
2. construct search trees in parallel, and link non-overlapping trees (which is efficient when they're non-overlapping)

Spatial data structures (see exercises):

1. spatially partition the elements
2. construct non-overlapping subsets and link them

So as a conclusion, how can we implement combiners?

1. **Two-phase construction**: the combiner uses an intermediate data
structure with an efficient combine method to partition the elements.
When result is called, the final data structure is constructed in
parallel from the intermediate data structure.
2. **An efficient concatenation or union operation**: a preferred way when
the resulting data structure allows this.
3. **Concurrent data structure**: different combiners share the same
underlying data structure, and rely on synchronization to correctly
update the data structure when `+=` is called.

We'll look more into the second method, which is more suited for parallel computations.

#### Conc-Trees
`Conc` is the parallel counterpart to parallel cons lists, and is used to manipulate data. Let's compare them to other data structures with an example: the implementation of `filter`:

Lists are built for sequential computations, and are traversed left to right:
{% highlight scala linenos %}
def filter[T](lst: List[T])(p: T => Boolean): List[T] = lst match {
    case x :: xs if p(x) => x :: filter(xs)(p)
    case x :: xs => filter(xs)(p)
    case Nil => Nil
}
{% endhighlight %}

Trees allow parallel computations – their subtrees can be traversed in
parallel (this is not a search tree, just a regular tree):
{% highlight scala linenos %}
def filter[T](t: Tree[T])(p: T => Boolean): Tree[T] = t match {
    case Node(left, right) => Node(parallel(filter(left)(p), filter(right)(p)))
    case Leaf(elem) => if (p(elem)) t else Empty
    case Empty => Empty
}
{% endhighlight %}

Trees are not good for parallelism unless they are balanced. Let’s devise a data type called Conc, which represents balanced trees:

{% highlight scala linenos %}
sealed trait Conc[+T] {
    def level: Int // level of subtree
    def size: Int // size of subtree
    def left: Conc[T]
    def right: Conc[T]
}

case object Empty extends Conc[Nothing] {
    def level = 0
    def size = 0
}
class Single[T](val x: T) extends Conc[T] {
    def level = 0
    def size = 1
}
// "Conc class": you can go left < or right >
case class <>[T](left: Conc[T], right: Conc[T]) extends Conc[T] {
    val level = 1 + math.max(left.level, right.level)
    val size = left.size + right.size
}
{% endhighlight %}

In addition, we will define the following *invariants* for Conc-trees:

1. A `<>` node can never contain `Empty` as its subtree. This guards us from sparse trees with too many empty subtrees
2. The level (read: height) difference between the left and the right subtree of a `<>` node is always 1 or less. This ensures that the height is bounded by $$\log{n}$$.

Concatenation is then:

{% highlight scala linenos %}
def <>(that: Conc[T]): Conc[T] = {
    if (this == Empty) that
    else if (that == Empty) this
    else concat(this, that) // delegate real work to concat
                            // which may reorganize the tree completely
}

def concat[T](xs: Conc[T], ys: Conc[T]): Conc[T] = {
    val diff = ys.level - xs.level
    if (diff >= -1 && diff <= 1) new <>(xs, ys) // link the trees
    else if (diff < -1) {
        if (xs.left.level >= xs.right.level) { // Left leaning (left deeper)
            val nr = concat(xs.right, ys)
            new <>(xs.left, nr)
        } else { // Right leaning
            val nrr = concat(xs.right.right, ys)
            if (nrr.level == xs.level - 3) {
                val nl = xs.left
                val nr = new <>(xs.right.left, nrr)
                new <>(nl, nr)
            } else {
                val nl = new <>(xs.left, xs.right.left) // new left
                val nr = nrr // new right
                new <>(nl, nr)
            }
        }
    }
}
{% endhighlight %}

Concatenation takes $$O(h_1 − h_2)$$ time, where $$h_1$$ and $$h_2$$ are the heights of the two trees.

#### Combiners using Conc-Trees
First we'll implement `+=`. To make it efficient ($$O(1)$$), We extend the Conc-Tree with a new node type:

{% highlight scala linenos %}
case class Append[T](left: Conc[T], right: Conc[T]) extends Conc[T] {
    val level = 1 + math.max(left.level, right.level)
    val size = left.size + right.size
}
{% endhighlight %}

The `Append` node has the same structure as a regular `<>` Conc node, and it has the same level and size; however, we will not impose the balance invariant on it. Trees of arbitrary size difference are allowed. To append a leaf:

{% highlight scala linenos %}
def appendLeaf[T](xs: Conc[T], y: T): Conc[T] = Append(xs, new Single(y))
{% endhighlight %}

This is indeed $$O(1)$$ but creates an unbalanced tree, which means concatenation will be $$O(n)$$ instead of $$O(\log{n})$$.

{% highlight scala linenos %}
def appendLeaf[T](xs: Conc[T], ys: Single[T]): Conc[T] = xs match {
    case Empty => ys
    case xs: Single[T] => new <>(xs, ys)
    case _ <> _ => new Append(xs, ys)
    case xs: Append[T] => append(xs, ys) // delegate work to append
}

@tailrec
private def append[T](xs: Append[T], ys: Conc[T]): Conc[T] = {
    if (xs.right.level > ys.level) new Append(xs, ys)
    else {
        val zs = new <>(xs.right, ys)
        xs.left match {
            case ws @ Append(_, _) => append(ws, zs)
            case ws if ws.level <= zs.level => ws <> zs
            case ws => new Append(ws, zs)
        }
    }
}
{% endhighlight %}

We have implemented an *immutable* data structure with:

- $$O(1)$$ appends
- $$O(\log{n})$$ concatenation

Transforming a Conc-Tree with `Append` nodes into a regular Conc-Tree should be fairly straightforward from this point on (concatenate trees from append list together). We almost have a functioning combiner; we just need to do a little more work.

The `ConcBuffer` appends elements into an array of size *k*. When the array gets full, it is stored into a `Chunk` node and added into the Conc-tree.

{% highlight scala linenos %}
class ConcBuffer[T: ClassTag](val k: Int, private var conc: Conc[T]) {
    private var chunk: Array[T] = new Array(k)
    private var chunkSize: Int = 0

    final def +=(elem: T): Unit = {
        if (chunkSize >= k) expand() // push array into conc-tree
        chunk(chunkSize) = elem
        chunkSize += 1
    }

    private def expand() {
        conc = appendLeaf(conc, new Chunk(chunk, chunkSize))
        chunk = new Array(k)
        chunkSize = 0
    }

    final def combine(that: ConcBuffer[T]): ConcBuffer[T] = {
        val combinedConc = this.result <> that.result // obtain conc-trees from buffers
        new ConcBuffer(k, combinedConc)
    }

    def result: Conc[T] = { // packs chuck array into the tree, returns resulting tree
        conc = appendLeaf(conc, new Chunk(chunk, chunkSize))
        conc
    }
}
class Chunk[T](val array: Array[T], val size: Int) extends Conc[T] {
    def level = 0
}
{% endhighlight %}

`Chunk` nodes are similar to `Single` nodes, but instead of a single element, they hold an array of elements.


Summary:

- $$O(\log{n})$$ `combine` concatenation
- Fast $$O(1)$$ `+=` operation
- $$O(1)$$ `result` operation



# Concurrent programming

## A surprising program
{% highlight scala linenos %}
var a, b = false
var x, y = -1
val t1 = thread {
    Thread.sleep(1) // pause for 1ms
    a = true
    y = if (b) 0 else 1    
}
val t2 = thread {
    Thread.sleep(1) // pause for 1ms
    b = true
    x = if (a) 0 else 1
}
t1.join()
t2.join()
assert(!(x == 1 && y == 1))
{% endhighlight %}

If we try to mentally simulate all runtime scenarios, where the threads execute in parallel, we can distrniguish three scenarios:

- `y = 1`, `x = 0`
- `y = 0`, `x = 1`
- `y = 0`, `x = 0`

In no scenario do we have `x = 1`, `y = 1`. Yet if we run this program, we do encounter this scenario! Let's rebuild our intuition of concurrent programming.

Every concurrent programming model must answer two questions:

1. How to express that two executions are concurrent?
2. Given a set of concurrent executions, how can they exchange
information (i.e. synchronize)?

In what follows, we will answer these two questions in the context of the
JVM concurrency model.

## Overview of threads
The thread notation starts a new thread – a concurrent execution.
{% highlight scala linenos %}
thread {
    a = true
    y = if (b) 0 else 1
}
{% endhighlight %}

The thread function is implemented as follows:

{% highlight scala linenos %}
def thread(body: =>Unit): Thread = {
    val t = new Thread {
        override def run() = body
    }
    t.start()
    t
}
{% endhighlight %}

We need threads, instead of working directly with the CPU for two reasons:

- **Portability**: We don't know which / how many CPUs to address, since this depends on the system
- The number of concurrent entities in a program can be much larger than the number of CPUs

Threads work as an abstraction. A thread image in memory contains:

- Copies of processor registers
- The call stack (~2MB per default)

The operating system eventually assigns threads to processes (the OS
guarantees liveness). Two approaches:

- Cooperative multitasking: a program has to explicitly give control (yield) back to the OS (think Windows 3.1)
- Preemptive multitasking: the OS has a hardware timer that periodically interrupts the running thread, and assigns different thread to the CPU (time slices usually ~10 ms)

### Some more definitions

**Non-deterministic program**: Given the same input, the program output is not unique between multiple runs. We want to write deterministic programs!

When `join` returns, the effects of the terminated thread are visible to the
thread that called `join`.

To avoid *race conditions*, we want to ensure that all operations of a function are performed *atomically*, without another thread reading or writing intermediate results. To do so, we used `synchronized` blocks, as seen previously (remember how it places a lock on the object).

## Monitors
How do we know if a monitor is released? We could do *polling* (also called *busy waiting*), but that consumes compute time while waiting. Instead, we can use a notification; indeed, all `Monitor` objects have the following methods:

- `wait()`: suspends the current thread
- `notify()`: wakes up one other thread waiting on the current object
- `notifyAll()`: wakes up all other thread waiting on the current object

{% highlight scala linenos %}
class OnePlaceBuffer[Elem] extends Monitor {
    var elem: Elem = _; var full = false
    def put(e: Elem): Unit = synchronized {
        while (full) wait()
        elem = e
        full = true
        notifyAll()
    }
    def get(): Elem = synchronized {
        while (!full) wait()
        full = false
        notifyAll()
        elem
    }
}
{% endhighlight %}

The fine print:

- `wait`, `notify` and `notifyAll` should **only** be called from within a
`synchronized` on `this`
- `wait` will release the lock, so other threads can enter the monitor
- `notify` and `notifyAll` schedule other threads for execution after the calling thread has released the lock (has left the monitor)
- On the JVM runtime, it is possible that a thread calling `wait`
sometimes wakes up even if nobody called `notify` or `notifyAll`

### Memory model
A *memory model* is a set of rules that defines how and when the writes to
memory by one thread become visible to other threads. Consider our introductory example:

{% highlight scala linenos %}
var a, b = false
var x, y = -1
val t1 = thread {
    Thread.sleep(1) // pause for 1ms
    a = true
    y = if (b) 0 else 1    
}
val t2 = thread {
    Thread.sleep(1) // pause for 1ms
    b = true
    x = if (a) 0 else 1
}
t1.join()
t2.join()
assert(!(x == 1 && y == 1))
{% endhighlight %}

When we initially analyzed the introductory example, we assumed that
every read and write happens in the program order, and that every read
and write goes to main memory. That specific memory model is called the sequential consistency model. More formally:

> Consider all the reads and writes to program variables. If the
result of the execution is the same as if the read and write
operations were executed in some sequential order, and the
operations of each individual processor appear in the program
order, then the model is sequentially consistent.

Unfortunately, as we saw in our experiment, multicore processors and
compilers do not implement the sequential consistency model.

The **Java Memory Model** (JMM) defines a “*happens-before*” relationship as follows.

- **Program order**: Each action in a thread *happens-before* every
subsequent action in the same thread.
- **Monitor locking**: Unlocking a monitor *happens-before* every
subsequent locking of that monitor.
- **Volatile fields**: A write to a volatile field *happens-before* every
subsequent read of that field.
- **Thread start**: A call to `start()` on a thread *happens-before* all
actions of that thread.
- **Thread termination**: An action in a thread *happens-before* another
thread completes a join on that thread.
- **Transitivity**: If A happens before B and B *happens-before* C, then A
*happens-before* C.

This means:

- A program point of a thread *t* is *guaranteed* to see all actions that *happen_before* it.
- It *may* (**may**) also see actions that can occur before it in the sequential consistency (interleaving) model.

Back to our surprising program:

{% highlight scala linenos %}
var a, b = false
var x, y = -1
val t1 = thread {
    Thread.sleep(1) // pause for 1ms
    a = true
    y = if (b) 0 else 1    
}
val t2 = thread {
    Thread.sleep(1) // pause for 1ms
    b = true
    x = if (a) 0 else 1
}
t1.join()
t2.join()
assert(!(x == 1 && y == 1))
{% endhighlight %}

This can fail since the two threads operate on their separate copy of memory, and can therefore have outdated information compared to each other.

{% highlight scala linenos %}
var a, b = false
var x, y = -1
val t1 = thread {
    synchronized { a = true }
    synchronized { y = if (b) 0 else 1 }
}
val t2 = thread {
    synchronized { b = true }
    synchronized { x = if (a) 0 else 1 }
}
t1.join()
t2.join()
assert(!(x == 1) && (y == 1))
{% endhighlight %}

This would work though! Because `synchronized` **synchronizes the processor caches with *main memory*** at the end of its execution.

In general, a memory model is an abstraction of the hardware capabilities of different computer systems. It essentially abstracts over the underlying system's *cache coherence protocol*.

#### Volatile fields
Making a variable `@volatile` has several effects:

- Reads and writes to volatile variables are never reordered by the compiler.
- Reads and writes are never cached in CPU registers &mdash; they go directly to the main memory
- Writes to normal variables, that in the program precede a volatile write *W*, cannot be moved by the compiler after *W*
- Reads from normal variables that in the program appear after a volatile read *R* cannot be moved by the compiler before R.
- Before a volatile write, values cached in registers must be written back to main memory.
- After a volatile read, values cached in registers must be re-read from the main memory.

Writes to `@volatile` are somewhat expensive (~50% as much as `synchronized`), but reads are very cheap. But `@volatile` offers fewer guarantees than `synchronized`.


### Executors
Threads have a lot of nice guarantees, but they are expensive to create. What people do to counteract that is use threads as workhorses that perform the tasks given to them. The number of available threads in a pool is typically some polynomial of the number of cores $$N$$ (e.g. $$N^2$$).

A task presented to an executor is encapsulated in a `Runnable` object:

{% highlight scala linenos %}
trait Runnable {
    def run(): Unit // actions to be performed by the task
}
{% endhighlight %}

Here's how a task gets passed to the `ForkJoinPool`:

{% highlight scala linenos %}
import java.util.concurrent.ForkJoinPool
object ExecutorsCreate extends App {
    val executor = new ForkJoinPool
    executor.execute(new Runnable {
        def run() = log("This task is run async")
    })
    Thread.sleep(1000)
}
{% endhighlight %}

Note that there is no way to await the end of a task like we did with `t.join()` for threads. Instead, we pause the main thread to give the executor threads time to finish.

The `scala.concurrent` package defines the `ExecutionContext` trait and object which is similar to `Executor` but more specific to Scala.

Execution contexts are passed as implicit parameters to many of Scala's concurrency abstractions. Here's how one runs a task using the default execution context:

{% highlight scala linenos %}
import scala.concurrent
object ExecutionContextCreate extends App {
    val ectx = ExecutionContext.global
    ectx.execute(new Runnable {
        def run() = log("This task is run async")
    })
    Thread.sleep(500)
}
{% endhighlight %}

To hide all of this boilerplate, we can put it all in an `execute` function.

### Atomic primitives
`synchronized`, `wait`, `notify`, `notifyAll` are complex and require support from the OS scheduler. We now look at the primitives in terms of which these higher-level operations are implemented.

An atomic variable is a memory location that supports **linearizable** operations (meaning that can be executed atomically). Here's how we can define `getUID` without `synchronized`:

{% highlight scala linenos %}
import java.util.concurrent.atomic._
object AtomicUid extends App {
    private val uid = new AtomicLong(0L)
    def getUID(): Long = uid.incrementAndGet()
    execute {
        log(s"Got a unique id asynchronously: ${getUID()}")
    }
    log(s"God a unique id: ${getUID()}")
}
{% endhighlight %}

`AtomicLong` offers the atomic operations `incrementAndGet()` `getAndSet(newValue: Long)`, and `compareAndSet(expect: Long, update: Long)`:

{% highlight scala linenos %}
class AtomicLong {
    ...
    // Functionally equivalent to the following (but in hardware):
    def compareAndSet(expect: Long, update: Long) = this.synchronized {
        if (this.get == expect) { this.set(update); true }
        else false
    }
}
{% endhighlight %}

CAS is a building block on which other linearizable operations are implemented with. It's often built-in into the hardware, and runs over a hundred cycles or so.

We can implement `getUID` using CAS directly:

{% highlight scala linenos %}
@tailrec def getUID(): Long = {
    val oldUID = uid.get  // read old value from atomic variable
    val newUID = oldUID + 1 // compute new value
    // Attempt to do a CAS
    if (uid.compareAndSet(oldUID, newUID)) newUID // Success!
    else getUID() // Some other thread has already done it. Try again
}
{% endhighlight %}

## Programming without locks
Locks as implemented by `synchronized` are a convenient concurrency mechanism, but are also problematic (possibility of deadlock, possibility to arbitrarily delay other threads if a thread executes a long-running operation in a `synchronized`).

With atomic variables and their **lock-free operation**, we can avoid these problems. We can even simulate locks with atomic variables!

{% highlight scala linenos %}
private val lock = new AtomicBoolean(false)
def mySynchronized(body: => Unit): Unit = {
    while (!lock.compareAndSet(false, true)) {}
    try body
    finally lock.set(false)
}
{% endhighlight %}

Here's how we define **lock-freedom**:

> An operation `op` is **lock-free** if, whenever there is a set of threads executing `op`, at least one thread completes the operation after a finite number of steps, regardless of the speed in which the different threads progress.

Essentially: *at least one* thread needs to complete the operation in a finite number of steps.

### Lazy values
Here's how `scalac` currently implements lazy values. It doesn't use `synchronized` around the whole block since it's quite costly, but still must set up some protections:


{% highlight scala linenos %}
@volatile private var x_defined = false
private var x_cached: T = _
def x: T = {
    if (!x_defined) this.synchronized {
        if (!x_defined) { // this pattern is called double-locking
            x_cached = E
            x_defined = true
        }
        x_cached
    }
}
{% endhighlight %}

The problems with this implementation are:

- It's not lock-free; `E` could take arbitrarily long time.
- It uses `this` as a lock, which might conflict with application-defined locking.
- It's prone to deadlocks.

The new Scala compiler, `dotty`, does this instead:

{% highlight scala linenos %}
def x: T = {
    if (!x_defined) {
        this.synchronized {
            if (x_evaluating) wait()
            else x_evaluating = true
        }
        if (!x_defined) {
            x_cached = E
            this.synchronized {
                x_evaluating = false
                x_defined = true
                notifyAll()
            }
        }
    }
}
{% endhighlight %}

- The evaluation of `E` happens outside a monitor, therefore no arbitrary slowdowns
- Two short `synchronized` blocks instead of one arbitrary long one
- No interference with user-defined locks
- Deadlocks are still possible but only in cases where sequential execution would give an infinite loop

### Collections
Operations on mutable collections are usually not thread-safe. The safe way to deal with this is of course to use `synchronized`, but that often leads to too much blocking. To gain speed, we can use or implement special *concurrent collection* implementations.

As an example, here's how concurrent queues could be implemented:

{% highlight scala linenos %}
import java.util.concurrent.atomic._
import scala.annotation.tailrec

object ConcQueue {
    private class Node[T](@volatile var next: Node[T]) {
        var elem: T = _
    }
}

class ConcQueue[T] {
    import ConcQueue._
    private var last = new AtomicReference(new Node[T](null))
    private var head = new AtomicReference(last.get)

    @tailrec final def append(elem: T): Unit = {
        // fiddle with last pointer
        val last1 = new Node[T](null)
        last1.elem = elem
        val prev = last.get
        // the following 2 lines differ from the sequential implementation
        // append needs to atomically update 2 variables, but CAS can only
        // work with 1 variable at a time. So we only use one CAS, and set
        // the other assignment when successful
        if (last.compareAndSet(prev, last1)) prev.next = last1
        else append(elem)
    }

    @tailrec final def remove(): Option[T] =
        if (head eq last) None
        else {
            val hd = head.get
            val first = hd.next
            // We just need to do a CAS in case first == null
            // This is to ensure that we don't have prev.next == null
            // instead of prev.next == last1 in append()
            if (first != null && head.compareAndSet(hd, first))
                Some(first.elem)
            else remove()
        }
}
{% endhighlight %}

This is not lock-free, but it guarantees that we actually remove. We could also just give up if `first == null` and return `None`, which would be lock-free.

## Futures

|              | One         | Many            |
| :----------- | :---------: | :-------------: |
| Synchronous  | `Try[T]`    | `Iterable[T]`   |
| Asynchronous | `Future[T]` | `Observable[T]` |


### Synchronous: `Try`
{% highlight scala linenos %}
def collectCoins(): List[Coin] = {
    if (eatenByMonster(this))
       throw new GameOverException("Ooops")
    List(Gold, Gold, Silver)
}
{% endhighlight %}

The return type here is dishonest, since actions may fail. So if we want to expose the possibility of failure in the types, then we should do `T => Try[S]` instead of `T => S`. Our game might now look like this:

{% highlight scala linenos %}
// Making failure evident in types:
abstract class Try[T]
case class Success[T](elem: T) extends Try[T]
case class Failure(t: Throwable) extends Try[Nothing]

trait Adventure {
    def collectCoins(): Try[List[Coin]]
    def buyTreasure(coins: List[Coin]): Try[Treasure]
}

// Dealing with failure explicitly
val adventure = Adventure()
val coins: Try[List[Coin]] = adventure.collectCoins()
val treasure: Try[Treasure] = coins match {
    case Success(cs) => adventure.buyTreasure(cs)
    case failure@Failure(e) => failure
}
{% endhighlight %}

There are some higher-order functions available that manipulate `Try[T]`:

{% highlight scala linenos %}
def flatMap[S](f: T=>Try[S]): Try[S]
def flatten[U <: Try[T]]: Try[U]
def map[S](f: T=>S): Try[T]
def filter(p: T=>Boolean): Try[T]
def recoverWith(f: PartialFunction[Throwable,Try[T]]): Try[T]
{% endhighlight %}

### Asynchronous: `Future`
`Future[T]` is a monad that handles exceptions and **latency**. Usually exceptions aren't really nice in a multi-threaded context, but `Future` exists to abstract away from all of our worries. They can asynchronously notify consumers:

{% highlight scala linenos %}
trait Future[T] {
    def onComplete(success: T => Unit, failed: Throwable => Unit): Unit
    def onComplete(callback: Observer[T]): Unit
}
{% endhighlight %}

Sending a packet across the Atlantic is a situation in which we could use futures; it takes quite a while and we'd like to do other things in the meantime, and still manage failures:

{% highlight scala linenos %}
val socket = Socket()
val packet: Future[Array[Byte]] =
socket.readFromMemory()
val confirmation: Future[Array[Byte]] = packet.onComplete {
    case Success(p) => socket.sendToEurope(p)
    case Failure(t) => …
}

val socket = Socket()
val packet: Future[Array[Byte]] = socket.readFromMemory()
packet.onComplete {
    case Success(p) => {
        val confirmation: Future[Array[Byte]] =
        socket.sendToEurope(p)
    }
    case Failure(t) => …
}
{% endhighlight %}

This is a bit awkward, we can do much better:

{% highlight scala linenos %}
// Starts an asynchronous computation
// and returns a future object to which you
// can subscribe to be notified when the
// future completes
object Future {
    def apply(body: =>T)(implicit context: ExecutionContext): Future[T]
}
{% endhighlight %}

We can then do this elegantly (?):

{% highlight scala linenos %}
import scala.concurrent.ExecutionContext.Implicits.global
import akka.serializer._

val memory = Queue[EMailMessage](
    EMailMessage(from = “Erik”, to = “Roland”),
    EMailMessage(from = “Martin”, to = “Erik”),
    EMailMessage(from = “Roland”, to = “Martin”))

def readFromMemory(): Future[Array[Byte]] = Future { // LOOK HERE!
    val email = queue.dequeue()
    val serializer = serialization.findSerializerFor(email)
    serializer.toBinary(email)
}
{% endhighlight %}

#### Recover and recoverWith
{% highlight scala linenos %}
def recover(f: PartialFunction[Throwable,T]): Future[T]
def recoverWith(f: PartialFunction[Throwable, Future[T]]): Future[T]
{% endhighlight %}

Here's how we would send packets using futures robustly:

{% highlight scala linenos %}
def sendTo(url: URL, packet: Array[Byte]): Future[Array[Byte]] =
    Http(url, Request(packet))
        .filter(response => response.isOK)
        .map(response => response.toByteArray)

def sendToSafe(packet: Array[Byte]): Future[Array[Byte]] =
    sendTo(mailServer.europe, packet) recoverWith {
        case europeError => // catches everything
            sendTo(mailServer.usa, packet) recover {
                case usaError => usaError.getMessage.toByteArray
            }
    }
{% endhighlight %}

A sometimes cleaner way of doing it is to provide a fallback:

{% highlight scala linenos %}
def sendToSafe(packet: Array[Byte]): Future[Array[Byte]] =
    sendTo(mailServer.europe, packet) recoverWith {
        case europeError => sendTo(mailServer.usa, packet) recover {
            case usaError => usaError.getMessage.toByteArray
        }
    }
    
def fallbackTo(that: =>Future[T]): Future[T] = {
    … if this future fails take the successful result
    of that future …
    … if that future fails too, take the error of
    this future …
}
{% endhighlight %}

### Implementation of FlatMap on Future
{% highlight scala linenos %}
trait Future[T] { self =>
    def flatMap[S](f: T => Future[S]): Future[S] = new Future[S] {
        def onComplete(callback: Try[S] => Unit): Unit =
            self onComplete {
                case Success(x) =>
                    f(x).onComplete(callback) // we apply f and if that succeeds, we do callback
                case Failure(e) => callback(Failure(e))
            }
    }
}
{% endhighlight %}

The actual implementation is a bit more evolved, as there's some scheduling involved, but this is the gist of it.

